<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>How do you recon a website?</title>
    <link>https://0x00sec.org/t/how-do-you-recon-a-website/37698</link>
    <description>This is how I would recon a website. I would gather subdomains, and directories of a website. Gathering subdomains is not a problem since there are tools or search engines to gather info about subdomains. A problem rises when I want to gather the directories of a website.

I have been thinking about using automated tools and user-directed tools to gather the dirs of the website. But automated-tools simply brute-force the dirs of a website which would send thousands of http requests and I think this is easily traceable and risky. But without automated tools, I wouldn&#39;t be able to gather enough number of directories.

I would like to hear how you usually carry out recon of a website to get some insights:D</description>
    
    <lastBuildDate>Fri, 03 Nov 2023 07:32:29 +0000</lastBuildDate>
    <category>Questions</category>
    <atom:link href="https://0x00sec.org/t/how-do-you-recon-a-website/37698.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>How do you recon a website?</title>
        <dc:creator><![CDATA[system]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed 3 days after the last reply. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/how-do-you-recon-a-website/37698/3">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/how-do-you-recon-a-website/37698/3</link>
        <pubDate>Mon, 06 Nov 2023 07:32:32 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-37698-3</guid>
        <source url="https://0x00sec.org/t/how-do-you-recon-a-website/37698.rss">How do you recon a website?</source>
      </item>
      <item>
        <title>How do you recon a website?</title>
        <dc:creator><![CDATA[hoek]]></dc:creator>
        <description><![CDATA[
            <p>Since harvesting a website’s directories is more like guessing, it is almost impossible to make it quiet. What came to my mind is that you can try directory brute-force with rate limiting: use a brute-force tool, set rate limits to reduce traffic and logs. Tools like Gobuster and Dirsearch allow you to control the request rate. You can also use a proxy to make it more like connecting from different locations. You may be able to use Google Dorks, which are specific search queries to find directories and files on a website. This doesn’t involve making direct requests to the server and is relatively discreet. Use tools such as Burp Suite’s Spider or wget to recursively crawl a website. This can be done more selectively and quietly than brute-force enumeration of directories.</p>
          <p><a href="https://0x00sec.org/t/how-do-you-recon-a-website/37698/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/how-do-you-recon-a-website/37698/2</link>
        <pubDate>Fri, 03 Nov 2023 07:32:29 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-37698-2</guid>
        <source url="https://0x00sec.org/t/how-do-you-recon-a-website/37698.rss">How do you recon a website?</source>
      </item>
      <item>
        <title>How do you recon a website?</title>
        <dc:creator><![CDATA[GuessWHO]]></dc:creator>
        <description><![CDATA[
            <p>This is how I would recon a website. I would gather subdomains, and directories of a website. Gathering subdomains is not a problem since there are tools or search engines to gather info about subdomains. A problem rises when I want to gather the directories of a website.</p>
<p>I have been thinking about using automated tools and user-directed tools to gather the dirs of the website. But automated-tools simply brute-force the dirs of a website which would send thousands of http requests and I think this is easily traceable and risky. But without automated tools, I wouldn’t be able to gather enough number of directories.</p>
<p>I would like to hear how you usually carry out recon of a website to get some insights:D</p>
          <p><a href="https://0x00sec.org/t/how-do-you-recon-a-website/37698/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/how-do-you-recon-a-website/37698/1</link>
        <pubDate>Thu, 02 Nov 2023 22:37:24 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-37698-1</guid>
        <source url="https://0x00sec.org/t/how-do-you-recon-a-website/37698.rss">How do you recon a website?</source>
      </item>
  </channel>
</rss>
