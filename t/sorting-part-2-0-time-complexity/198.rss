<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Sorting (Part 2.0): Time Complexity</title>
    <link>https://0x00sec.org/t/sorting-part-2-0-time-complexity/198</link>
    <description>Welcome back, 0x00sec community, to my series on sorting.

I introduced in my last article the concept of complexity. When I say complexity, I&#39;m talking about **time complexity**.
&lt;!--more--&gt;
## What Is Time Complexity?
You can view the Wikipedia article &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity&quot;&gt;here&lt;/a&gt;, but I&#39;ll be speaking from my heart and soul.

**Time complexity** is a mathematical representation of how long an algorithm could take in the worse-case scenario, and it is a function of the size of the input. Input size is usually represented by `n`.

## Example: Bubble Sort in C++
Alright, class. Take out that C++ code I handed out yesterday...

```c++
void bubblesort(vector&lt;char&gt;&amp; vec) {
    int n = vec.size();
 
    // nest loops because O(n^2)
    // changes_made is an optimization that can reduce runtime in certain conditions
    bool changes_made = false;
 
    for (int i = 0; i &lt; n - 1; i++) {
 
        for (int j = 0; j &lt; n - 1; j++) {
            if (vec[j] &gt; vec[j+1]) {
 
                // swap them
                char temp = vec[j];
                vec[j] = vec[j+1];
                vec[j+1] = temp;
 
                // tell program to run thru vec at least once more
                changes_made = true;
            }
        }
 
        // optimization to exit loop if no changes made
        if (!changes_made) break;
 
        changes_made = false;
    }
}
```

Now, let&#39;s go line-by-line in `bubblesort(...)` and add up all the things we have to do.

- On line 9, we declare a variable to hold the size of our vector, so let&#39;s add `1` to our complexity.
- Another variable is declared on line 13, so now our complexity is `2`.
- We have a `for` loop starting on line 15, so that adds `n-1` to our complexity, because we loop through the vector from the first element to the second-to-last. We have a `for` loop within the outer one in which we do the same, so we have `(n-1)x(n-1)`. Our grand total is `(n-1)x(n-1)+2`. _We do some things in the `for` loops, but we can ignore those and you&#39;ll see why in a second_.
- Okay, now we&#39;ll simplify down our total: `n^2 - 2n + 1 + 2 = n^2 -2n +3`
- Let&#39;s take the limit as n approaches infinity. This is a calculus thing, but it basically means that we want to see what happens as the size of our vector gets really, really big.
- Per the limit operation, we can simplify it down to n^2, but note that any coefficient of n^2 can also be stripped, since no coefficient could really do all that much to the square of infinity.

## Complexity Notation

Okay, so now we&#39;ve concluded that the time complexity of our algorithm, Bubble Sort, can be expressed as `n^2`. But what if the vector was already sorted? Due to our awesome optimization, per `changes_made`, we only need n time to sort (in this best-case situation).

That means that our algorithm has a lower bound time complexity of n and an upper bound time complexity of `n^2.`

How do we express upper and lower bounds?

**Upper bound notation** is expressed as `O(n^2)` for our algorithm. This is called &quot;Big-O Notation.&quot; To reiterate, this notation represents the worst-case-input scenario for our algorithm as n gets really, really huge.

**Lower bound notation** is expressed using the Greek letter omega, O. We say the lower-bound, the best-case, for our algorithm (Bubble Sort) is O(n).

## Conclusion

Well, that&#39;s time complexity right there. Bubble sort has O(n^2) and O(n) time complexity. From now on, I&#39;ll be noting the time complexity for each algorithm I discuss in the Sorting series.

Best,
oaktree</description>
    
    <lastBuildDate>Mon, 16 Jan 2017 23:34:51 +0000</lastBuildDate>
    <category>Algorithms</category>
    <atom:link href="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>Sorting (Part 2.0): Time Complexity</title>
        <dc:creator><![CDATA[oaktree]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed after 30 days. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/5">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/5</link>
        <pubDate>Sun, 21 Jan 2018 00:37:36 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-198-5</guid>
        <source url="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198.rss">Sorting (Part 2.0): Time Complexity</source>
      </item>
      <item>
        <title>Sorting (Part 2.0): Time Complexity</title>
        <dc:creator><![CDATA[pry0cc]]></dc:creator>
        <description><![CDATA[
            <p>Oh crap. How did I miss this!?</p>
          <p><a href="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/4">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/4</link>
        <pubDate>Mon, 16 Jan 2017 23:34:51 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-198-4</guid>
        <source url="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198.rss">Sorting (Part 2.0): Time Complexity</source>
      </item>
      <item>
        <title>Sorting (Part 2.0): Time Complexity</title>
        <dc:creator><![CDATA[oaktree]]></dc:creator>
        <description><![CDATA[
            <p>Of course. And it’s actually a very important concept in CS.</p>
          <p><a href="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/3">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/3</link>
        <pubDate>Fri, 06 May 2016 00:18:11 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-198-3</guid>
        <source url="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198.rss">Sorting (Part 2.0): Time Complexity</source>
      </item>
      <item>
        <title>Sorting (Part 2.0): Time Complexity</title>
        <dc:creator><![CDATA[Cromical]]></dc:creator>
        <description><![CDATA[
            <p>Never knew about it. Thanks for bringing it up!</p>
          <p><a href="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/2</link>
        <pubDate>Fri, 06 May 2016 00:17:41 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-198-2</guid>
        <source url="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198.rss">Sorting (Part 2.0): Time Complexity</source>
      </item>
      <item>
        <title>Sorting (Part 2.0): Time Complexity</title>
        <dc:creator><![CDATA[oaktree]]></dc:creator>
        <description><![CDATA[
            <p>Welcome back, 0x00sec community, to my series on sorting.</p>
<p>I introduced in my last article the concept of complexity. When I say complexity, I’m talking about <strong>time complexity</strong>.</p>

<h2>What Is Time Complexity?</h2>
<p>You can view the Wikipedia article <a href="https://en.wikipedia.org/wiki/Time_complexity" rel="nofollow noopener">here</a>, but I’ll be speaking from my heart and soul.</p>
<p><strong>Time complexity</strong> is a mathematical representation of how long an algorithm could take in the worse-case scenario, and it is a function of the size of the input. Input size is usually represented by <code>n</code>.</p>
<h2>Example: Bubble Sort in C++</h2>
<p>Alright, class. Take out that C++ code I handed out yesterday…</p>
<pre><code class="lang-auto">void bubblesort(vector&lt;char&gt;&amp; vec) {
    int n = vec.size();
 
    // nest loops because O(n^2)
    // changes_made is an optimization that can reduce runtime in certain conditions
    bool changes_made = false;
 
    for (int i = 0; i &lt; n - 1; i++) {
 
        for (int j = 0; j &lt; n - 1; j++) {
            if (vec[j] &gt; vec[j+1]) {
 
                // swap them
                char temp = vec[j];
                vec[j] = vec[j+1];
                vec[j+1] = temp;
 
                // tell program to run thru vec at least once more
                changes_made = true;
            }
        }
 
        // optimization to exit loop if no changes made
        if (!changes_made) break;
 
        changes_made = false;
    }
}
</code></pre>
<p>Now, let’s go line-by-line in <code>bubblesort(...)</code> and add up all the things we have to do.</p>
<ul>
<li>On line 9, we declare a variable to hold the size of our vector, so let’s add <code>1</code> to our complexity.</li>
<li>Another variable is declared on line 13, so now our complexity is <code>2</code>.</li>
<li>We have a <code>for</code> loop starting on line 15, so that adds <code>n-1</code> to our complexity, because we loop through the vector from the first element to the second-to-last. We have a <code>for</code> loop within the outer one in which we do the same, so we have <code>(n-1)x(n-1)</code>. Our grand total is <code>(n-1)x(n-1)+2</code>. <em>We do some things in the <code>for</code> loops, but we can ignore those and you’ll see why in a second</em>.</li>
<li>Okay, now we’ll simplify down our total: <code>n^2 - 2n + 1 + 2 = n^2 -2n +3</code>
</li>
<li>Let’s take the limit as n approaches infinity. This is a calculus thing, but it basically means that we want to see what happens as the size of our vector gets really, really big.</li>
<li>Per the limit operation, we can simplify it down to n^2, but note that any coefficient of n^2 can also be stripped, since no coefficient could really do all that much to the square of infinity.</li>
</ul>
<h2>Complexity Notation</h2>
<p>Okay, so now we’ve concluded that the time complexity of our algorithm, Bubble Sort, can be expressed as <code>n^2</code>. But what if the vector was already sorted? Due to our awesome optimization, per <code>changes_made</code>, we only need n time to sort (in this best-case situation).</p>
<p>That means that our algorithm has a lower bound time complexity of n and an upper bound time complexity of <code>n^2.</code></p>
<p>How do we express upper and lower bounds?</p>
<p><strong>Upper bound notation</strong> is expressed as <code>O(n^2)</code> for our algorithm. This is called “Big-O Notation.” To reiterate, this notation represents the worst-case-input scenario for our algorithm as n gets really, really huge.</p>
<p><strong>Lower bound notation</strong> is expressed using the Greek letter omega, O. We say the lower-bound, the best-case, for our algorithm (Bubble Sort) is O(n).</p>
<h2>Conclusion</h2>
<p>Well, that’s time complexity right there. Bubble sort has O(n^2) and O(n) time complexity. From now on, I’ll be noting the time complexity for each algorithm I discuss in the Sorting series.</p>
<p>Best,<br>
oaktree</p>
          <p><a href="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/sorting-part-2-0-time-complexity/198/1</link>
        <pubDate>Thu, 05 May 2016 23:14:07 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-198-1</guid>
        <source url="https://0x00sec.org/t/sorting-part-2-0-time-complexity/198.rss">Sorting (Part 2.0): Time Complexity</source>
      </item>
  </channel>
</rss>
