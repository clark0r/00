<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Python Scripting skill - Scraping</title>
    <link>https://0x00sec.org/t/python-scripting-skill-scraping/186</link>
    <description>Hello Hackers !
 I&#39;m back for a new tutorial on scripting with python, here I will teach you how to do web scraping.
I needed to grab some proxy on internet to test proxychain so I decided to do a python script who will scrape the proxy for me.
## ProxyScraping
First of all you&#39;ll need python and some libraries 
- *BeautifulSoup* for scraping web content
- *requests* for all the web request to the site

For this script I choose [www.samair.ru/proxy/](www.samair.ru/proxy/) because it&#39;s an easy website to scrape.
Firs we import what we need
```python
import requests
from bs4 import BeautifulSoup
```
When I looked to the source code of [www.samair.ru](www.samair.ru/proxy/), I saw that the link *You can do it there* redirected me to the page with all proxy and port that can be easily copied and pasted 
Here on the screenshot below you can see that *You can do it here* is a link to another page
![Source-code](upload://vAeb2y3cWlIBOKDde9bq9DTymvg.png)

Here is the source code of the page with all proxies
![pre](upload://oXPntUUlguGj6U68hzjbEHy2WGH.png)

We can see the ip and port is in the 
```html
&lt;pre&gt;&lt;/pre&gt;
```
So we need to tell beautiful soup *follow the link you can do it there* and  *When you find a pre get the content of it*
So here the code : 
```python
#We define a global variable to be access later by other function
global url
#Here the url of the site we want to scrape
url = &quot;http://www.samair.ru&quot;
#Here we go to http://samair.ru/proxy/
r = requests.get(url + &quot;/proxy&quot;)
#We dump dump all the page into the variable content
content = BeautifulSoup(r.text, &quot;lxml&quot;)
#We dump all th links in the variale links
links = soup.find_all(&#39;a&#39;, href=True)
```

Now we define a function to follow the link *You can do it there*, the function will take the variale links as an argument
```python
def followLink(links):
    #We loop through the links
    for i in Link:
                if i.text == &quot;You can do it there&quot;:
                    &quot;&quot;&quot;return the url + the link in href 
                    ex :http://www.samair.ru/proxy/ip-port/668758911.html
                    &quot;&quot;&quot;
                    return url + i[&#39;href&#39;]
```

Now we do the function who will get all the ip and port 
```python
# The function take a url as an argument
def scrap(url):
    #Here we go to the page with all the proxies
    r = requests.get(url)
    #We get the content of the page
    soup = BeautifulSoup(r.text, &quot;lxml&quot;)
    #Here we dump the text who is in &lt;pre&gt;&lt;/pre&gt; in the variable pre
    pre = soup.find(&quot;pre&quot;).text
    #We return a list of proxies
    return pre.split(&quot;\n&quot;)
```
Now We want to parse all the proxy to a file
```python
# parse ip:port to file &quot;http ip:port&quot;
def parseToFile(proxies):
    for i in proxies:
        #We check if i is not empty, if i is not empty write proxies to a file
        if i:
            #We open the file proxy.txt and append the proxies
            with open(&quot;proxy.txt&quot;,&quot;a&quot;) as file:
                        #Write the proxies : http ip:port
                        file.write(&quot;http &quot; + i +&quot;\n&quot;)
                        #and we close the file
                        file.close()
```
So we have all the function needed to scape the content of the page
we can define a main function and run it
```python
def main():
    url = followLink(links)
    proxy=scrap(url)
    parseToFile(proxy)

if __name__ == &quot;__main__&quot;:
    main()
```

The whole script will look like that 
```python
import requests
from bs4 import BeautifulSoup

#We define a global variable to be access later by other function
global url
#Here the url of the site we want to scrape
url = &quot;http://www.samair.ru&quot;
#Here we go to http://samair.ru/proxy/
r = requests.get(url + &quot;/proxy&quot;)
#We dump dump all the page into the variable content
content = BeautifulSoup(r.text, &quot;lxml&quot;)
#We dump all th links in the variale links
links = soup.find_all(&#39;a&#39;, href=True)

def followLink(links):
    #We loop in the content    
    for i in Link:
                if i.text == &quot;You can do it there&quot;:
                    &quot;&quot;&quot;return the url + the link in href 
                    ex :http://www.samair.ru/proxy/ip-port/668758911.html
                    &quot;&quot;&quot;
                    return url + i[&#39;href&#39;]

# The function take a url as an argument
def scrap(url):
    #Here we go to the page with all the proxies
    r = requests.get(url)
    #We get the content of the page
    soup = BeautifulSoup(r.text, &quot;lxml&quot;)
    #Here we dump the text who is in &lt;pre&gt;&lt;/pre&gt; in the variable pre
    pre = soup.find(&quot;pre&quot;).text
    #We return a list of proxies
    return pre.split(&quot;\n&quot;)

# parse ip:port to file &quot;http ip:port&quot;
def parseToFile(proxies):
    for i in proxies:
        #We check if i is not empty, if i is not empty write proxies to a file
        if i:
            #We open the file proxy.txt and append the proxies
            with open(&quot;proxy.txt&quot;,&quot;a&quot;) as file:
                        #Write the proxies : http ip:port
                        file.write(&quot;http &quot; + i +&quot;\n&quot;)
                        #and we close the file
                        file.close()

def main():
    url = followLink(links)
    proxy=scrap(url)
    parseToFile(proxy)

if __name__ == &quot;__main__&quot;:
    main()
```

You can run this script you&#39;ll have a *proxy.txt* file in your directory whith all the proxies scraped

Edit:Tutorial Updated

@L3akM3-0day</description>
    
    <lastBuildDate>Wed, 01 Nov 2017 08:05:45 +0000</lastBuildDate>
    <category>Programming</category>
    <atom:link href="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[system]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed after 30 days. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/13">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/13</link>
        <pubDate>Sun, 21 Jan 2018 00:30:04 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-13</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[SoLux]]></dc:creator>
        <description><![CDATA[
            <p>if you’re on Linux:</p>
<blockquote>
<p>sudo pip install utils</p>
</blockquote>
<p>If that doesn’t seem to work,  try:</p>
<blockquote>
<p>python pip install utils</p>
</blockquote>
<p>^ Will also work on windows.</p>
<p>As python should have the right permissions, even though sudo should work.</p>
<p>Perhaps you have python2 and python3 installed?<br>
try installing with:</p>
<blockquote>
<p>python3 pip install utils</p>
</blockquote>
<p>or</p>
<blockquote>
<p>python2 pip install utils</p>
</blockquote>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/12">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/12</link>
        <pubDate>Wed, 01 Nov 2017 08:05:45 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-12</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[root_haxor]]></dc:creator>
        <description><![CDATA[
            <p>brother i tried this also</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/11">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/11</link>
        <pubDate>Wed, 03 Aug 2016 18:21:11 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-11</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[root_haxor]]></dc:creator>
        <description><![CDATA[
            <p>i copied your code bro exact</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/10">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/10</link>
        <pubDate>Wed, 03 Aug 2016 18:20:48 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-10</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[pry0cc]]></dc:creator>
        <description><![CDATA[
            <p>Try running the command prompt as Administrator, and then run <code>pip install email</code></p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/9">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/9</link>
        <pubDate>Wed, 03 Aug 2016 17:01:55 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-9</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[L3akM3-0day]]></dc:creator>
        <description><![CDATA[
            <p>Can you paste your script here ? <img src="https://0x00sec.org/images/emoji/twitter/slight_smile.png?v=9" title=":slight_smile:" class="emoji" alt=":slight_smile:"></p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/8">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/8</link>
        <pubDate>Wed, 03 Aug 2016 16:32:40 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-8</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[root_haxor]]></dc:creator>
        <description><![CDATA[
            <p>anybody can solve this issue<br>
<code>python proxyscrape.py</code></p>
<pre><code class="lang-auto">Traceback (most recent call last):
  File "proxyscrape.py", line 1, in &lt;module&gt;
    import requests
  File "C:\pentestbox\base\python\Lib\site-packages\requests\__init__.py", line 61, in &lt;module&gt;
    from .packages.urllib3.exceptions import DependencyWarning
  File "C:\pentestbox\base\python\Lib\site-packages\requests\packages\__init__.py", line 29, in &lt;module&gt;
    import urllib3
  File "C:\pentestbox\base\python\Lib\site-packages\urllib3\__init__.py", line 8, in &lt;module&gt;
    from .connectionpool import (
  File "C:\pentestbox\base\python\Lib\site-packages\urllib3\connectionpool.py", line 41, in &lt;module&gt;
    from .request import RequestMethods
  File "C:\pentestbox\base\python\Lib\site-packages\urllib3\request.py", line 7, in &lt;module&gt;
    from .filepost import encode_multipart_formdata
  File "C:\pentestbox\base\python\Lib\site-packages\urllib3\filepost.py", line 9, in &lt;module&gt;
    from .fields import RequestField
  File "C:\pentestbox\base\python\Lib\site-packages\urllib3\fields.py", line 2, in &lt;module&gt;
    import email.utils
ImportError: No module named utils
</code></pre>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/7">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/7</link>
        <pubDate>Wed, 03 Aug 2016 15:47:08 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-7</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[root_haxor]]></dc:creator>
        <description><![CDATA[
            <p>Bro! very tuff to understand what you trying to say but i loved reading and trying this</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/6">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/6</link>
        <pubDate>Mon, 01 Aug 2016 14:44:48 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-6</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[Cromical]]></dc:creator>
        <description><![CDATA[
            <p>I like it mate! I’m glad to see more and more Python tut’s popping up, Python is not only my favorite scripting language, but also my first one (that I learned).</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/5">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/5</link>
        <pubDate>Fri, 06 May 2016 00:23:32 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-5</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[L3akM3-0day]]></dc:creator>
        <description><![CDATA[
            <p>That’s what I’ve done haha <img src="https://0x00sec.org/images/emoji/twitter/stuck_out_tongue.png?v=9" title=":stuck_out_tongue:" class="emoji" alt=":stuck_out_tongue:"></p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/4">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/4</link>
        <pubDate>Thu, 05 May 2016 23:15:48 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-4</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[Sea]]></dc:creator>
        <description><![CDATA[
            <p>While I personally understand the code, I think you should add way more comments and elaborate more on each step, instead of throwing out a huge chunk of code.</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/3">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/3</link>
        <pubDate>Thu, 05 May 2016 23:12:23 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-3</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[oaktree]]></dc:creator>
        <description><![CDATA[
            <p>Hi, <a class="mention" href="https://0x00sec.org/u/l3akm3-0day">@L3akM3-0day</a>:</p>
<p>Would you mind more carefully explaining your code for everyone, please?</p>
<p>And this doesn’t seem like the best way to “learn python”. Take a look at a post like <a href="http://0x00sec.org/t/re-building-a-vsftpd-backdoor-exploit-in-python/159">this one</a> by <a class="mention" href="https://0x00sec.org/u/defalt">@Defalt</a>. He does a great job integrating Python skills into hacking.</p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/2</link>
        <pubDate>Thu, 05 May 2016 21:36:44 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-2</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
      <item>
        <title>Python Scripting skill - Scraping</title>
        <dc:creator><![CDATA[L3akM3-0day]]></dc:creator>
        <description><![CDATA[
            <p>Hello Hackers !<br>
I’m back for a new tutorial on scripting with python, here I will teach you how to do web scraping.<br>
I needed to grab some proxy on internet to test proxychain so I decided to do a python script who will scrape the proxy for me.</p>
<h2>ProxyScraping</h2>
<p>First of all you’ll need python and some libraries</p>
<ul>
<li>
<em>BeautifulSoup</em> for scraping web content</li>
<li>
<em>requests</em> for all the web request to the site</li>
</ul>
<p>For this script I choose <a href="https://0x00sec.org">www.samair.ru/proxy/</a> because it’s an easy website to scrape.<br>
Firs we import what we need</p>
<pre><code class="lang-python">import requests
from bs4 import BeautifulSoup
</code></pre>
<p>When I looked to the source code of <a href="https://0x00sec.org">www.samair.ru</a>, I saw that the link <em>You can do it there</em> redirected me to the page with all proxy and port that can be easily copied and pasted<br>
Here on the screenshot below you can see that <em>You can do it here</em> is a link to another page<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/d/dd5bc9fe969cf693b5d26ca0e91169b4398d27aa.png" alt="Source-code" data-base62-sha1="vAeb2y3cWlIBOKDde9bq9DTymvg" width="" height=""></p>
<p>Here is the source code of the page with all proxies<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/a/aef783e4d484f77c3e9fd9ceb1b9aa949106456f.png" alt="pre" data-base62-sha1="oXPntUUlguGj6U68hzjbEHy2WGH" width="" height=""></p>
<p>We can see the ip and port is in the</p>
<pre><code class="lang-auto">&lt;pre&gt;&lt;/pre&gt;
</code></pre>
<p>So we need to tell beautiful soup <em>follow the link you can do it there</em> and  <em>When you find a pre get the content of it</em><br>
So here the code :</p>
<pre><code class="lang-python">#We define a global variable to be access later by other function
global url
#Here the url of the site we want to scrape
url = "http://www.samair.ru"
#Here we go to http://samair.ru/proxy/
r = requests.get(url + "/proxy")
#We dump dump all the page into the variable content
content = BeautifulSoup(r.text, "lxml")
#We dump all th links in the variale links
links = soup.find_all('a', href=True)
</code></pre>
<p>Now we define a function to follow the link <em>You can do it there</em>, the function will take the variale links as an argument</p>
<pre><code class="lang-python">def followLink(links):
    #We loop through the links
    for i in Link:
                if i.text == "You can do it there":
                    """return the url + the link in href 
                    ex :http://www.samair.ru/proxy/ip-port/668758911.html
                    """
                    return url + i['href']
</code></pre>
<p>Now we do the function who will get all the ip and port</p>
<pre><code class="lang-python"># The function take a url as an argument
def scrap(url):
    #Here we go to the page with all the proxies
    r = requests.get(url)
    #We get the content of the page
    soup = BeautifulSoup(r.text, "lxml")
    #Here we dump the text who is in &lt;pre&gt;&lt;/pre&gt; in the variable pre
    pre = soup.find("pre").text
    #We return a list of proxies
    return pre.split("\n")
</code></pre>
<p>Now We want to parse all the proxy to a file</p>
<pre><code class="lang-python"># parse ip:port to file "http ip:port"
def parseToFile(proxies):
    for i in proxies:
        #We check if i is not empty, if i is not empty write proxies to a file
        if i:
            #We open the file proxy.txt and append the proxies
            with open("proxy.txt","a") as file:
                        #Write the proxies : http ip:port
                        file.write("http " + i +"\n")
                        #and we close the file
                        file.close()
</code></pre>
<p>So we have all the function needed to scape the content of the page<br>
we can define a main function and run it</p>
<pre><code class="lang-python">def main():
    url = followLink(links)
    proxy=scrap(url)
    parseToFile(proxy)

if __name__ == "__main__":
    main()
</code></pre>
<p>The whole script will look like that</p>
<pre><code class="lang-python">import requests
from bs4 import BeautifulSoup

#We define a global variable to be access later by other function
global url
#Here the url of the site we want to scrape
url = "http://www.samair.ru"
#Here we go to http://samair.ru/proxy/
r = requests.get(url + "/proxy")
#We dump dump all the page into the variable content
content = BeautifulSoup(r.text, "lxml")
#We dump all th links in the variale links
links = soup.find_all('a', href=True)

def followLink(links):
    #We loop in the content    
    for i in Link:
                if i.text == "You can do it there":
                    """return the url + the link in href 
                    ex :http://www.samair.ru/proxy/ip-port/668758911.html
                    """
                    return url + i['href']

# The function take a url as an argument
def scrap(url):
    #Here we go to the page with all the proxies
    r = requests.get(url)
    #We get the content of the page
    soup = BeautifulSoup(r.text, "lxml")
    #Here we dump the text who is in &lt;pre&gt;&lt;/pre&gt; in the variable pre
    pre = soup.find("pre").text
    #We return a list of proxies
    return pre.split("\n")

# parse ip:port to file "http ip:port"
def parseToFile(proxies):
    for i in proxies:
        #We check if i is not empty, if i is not empty write proxies to a file
        if i:
            #We open the file proxy.txt and append the proxies
            with open("proxy.txt","a") as file:
                        #Write the proxies : http ip:port
                        file.write("http " + i +"\n")
                        #and we close the file
                        file.close()

def main():
    url = followLink(links)
    proxy=scrap(url)
    parseToFile(proxy)

if __name__ == "__main__":
    main()
</code></pre>
<p>You can run this script you’ll have a <em>proxy.txt</em> file in your directory whith all the proxies scraped</p>
<p>Edit:Tutorial Updated</p>
<p><a class="mention" href="https://0x00sec.org/u/l3akm3-0day">@L3akM3-0day</a></p>
          <p><a href="https://0x00sec.org/t/python-scripting-skill-scraping/186/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/python-scripting-skill-scraping/186/1</link>
        <pubDate>Thu, 05 May 2016 21:28:00 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-186-1</guid>
        <source url="https://0x00sec.org/t/python-scripting-skill-scraping/186.rss">Python Scripting skill - Scraping</source>
      </item>
  </channel>
</rss>
