<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Point of no C3 | Linux Kernel Exploitation - Part 0</title>
    <link>https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585</link>
    <description>In the name of Allah, the most beneficent, the most merciful.

---
**HAH**IRRITATED**AHAHAHAHAHAHAHA**
*&quot;Appreciate the art, master the craft.&quot;*
**AHAHAHAH**OUTDATED**AHAHAHAHAH**

It&#39;s been more than a year, huh? but I&#39;m back, with *&quot;Point of no C3&quot;*. It&#39;s main focus will be *Kernel Exploitation*, but that won&#39;t stop it from looking at other things.


Summary
* Chapter I: Environment setup:
  * Preparing the VM
  * Using KGDB to debug the kernel
  * Compiling a simple module
  * What?
  * Few structs
  * Debug a module
* Chapter II: Overview on security and General understanding:
  * Control Registers
  * SMAP
  * SMEP
  * Write-Protect
  * Paging(a bit of segmentation too)
  * Processes
  * Syscalls
  * IDT(Interrupt Descriptor Table)
  * KSPP
  * KASLR
  * kptr_restrict
  * mmap_min_addr
  * addr_limit

#### Chapter I: *Environment setup*
&quot;*No QEMU for you.*&quot;
##### Preparing the VM:
To begin with, we would set up the environment and the VM&#39;s in order to experiment on them.
For this, [*Debian*](https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/) was choosen(*core only*).
Other choices include *SUSE* or *Centos*, etc.
```perl
debian-9.4.0-amd64-netinst.iso			2018-03-10 12:56 291M [X]
debian-9.4.0-amd64-xfce-CD-1.iso		2018-03-10 12:57 646M
debian-mac-9.4.0-amd64-netinst.iso		2018-03-10 12:56 294M
```
A VM is then created with atleast **35GB** space.(*Hey, It&#39;s for compiling the kernel!*)
```perl
Installer disc image file (iso):
[C:\vm\debian-9.4.0-amd64-netinst.iso	[▼]]
⚠ Could not detect which operating system is in this disc image.
  You will need to specify which operating system will be installed.
```
Once you boot it, you can proceed with *Graphical Install*, and since we only want the core, stop at *Software selection* and have only **SSH server** and **standard system utilities** selected.
And when it&#39;s done, you&#39;ll have your first VM ready.
```bash

Debian GNU/Linux 9 Nwwz tty1
Hint: Num Lock on

Nwwz login: root
Password:
Linux Nwwz 4.9.0-6-amd64 #1 SMP Debian 4.9.88-1+deb9u1 (2018-05-07) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@Nwwz:~#
```
In order to get the latest stable Linux kernel release(*4.17.2 at the time of writing*) and run it.
We would start by installing necessary packages:
```bash
apt-get install git build-essential fakeroot ncurses* libssl-dev libelf-dev ccache gcc-multilib bison flex bc
```
Downloading the kernel tarball and the patch:
```bash
root@Nwwz:~# cd /usr/src
root@Nwwz:/usr/src# wget &quot;https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/linux-4.17.2.tar.gz&quot;
root@Nwwz:/usr/src# wget &quot;https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/patch-4.17.2.gz&quot;
```
Extracting them:
```bash
root@Nwwz:/usr/src# ls
linux-4.17.2.tar.gz patch-4.17.2.gz
root@Nwwz:/usr/src# gunzip patch-4.17.2.gz
root@Nwwz:/usr/src# gunzip linux-4.17.2.tar.gz
root@Nwwz:/usr/src# tar -xvf linux-4.17.2.tar
```
Moving and applying the patch:
```bash
root@Nwwz:/usr/src# ls
linux-4.17.2 linux-4.17.2.tar patch-4.17.2
root@Nwwz:/usr/src# mv patch-4.17.2 linux-4.17.2/
root@Nwwz:/usr/src# cd linux-4*2
root@Nwwz:/usr/src/linux-4.17.2# patch -p1 &lt; patch-4.17.2
```
Cleaning the directory and copying the original bootfile to the current working directory and changing the config with an *ncurses* menu:
```bash
root@Nwwz:/usr/src/linux-4.17.2# make mrproper
root@Nwwz:/usr/src/linux-4.17.2# make clean
root@Nwwz:/usr/src/linux-4.17.2# cp /boot/config-$(uname -r) .config
root@Nwwz:/usr/src/linux-4.17.2# make menuconfig
```
One must then set up the following fields:
```
[*] Networking support 	---&gt;
    Device Drivers		---&gt;
	Firmware Drivers	---&gt;
	File systems		---&gt;
[X] Kernel hacking		---&gt;
		printk and dmesg options					---&gt;
	[X] Compile-time checks and compiler options	---&gt;
		...
		[*] Compile the kernel with debug info
		...
	...
	-*- Kernel debugging
	...
	[*] KGDB: kernel debugger
```
```
	Do you wish to save your new configuration?
	Press &lt;ESC&gt;&lt;ESC&gt; to continue kernel configuration.
			[&lt; Yes &gt;]			&lt; No &gt;
```
Make sure you do have similiar lines on .config:
```bash
CONFIG_STRICT_KERNEL_RWX=n
CONFIG_DEBUG_INFO=y
CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR=n
CONFIG_HARDENED_USERCOPY=n
CONFIG_HARDENED_USERCOPY_FALLBACK=n
```
Before starting the compiling process, to faster the process, you can split the work to multiple jobs(*on different processors*). **nproc** would hand you the number of processing units available.
```bash
root@Nwwz:/usr/src/linux-4.17.2# nproc
4
root@Nwwz:/usr/src/linux-4.17.2# make -j4
```
It will then automatically go through *stage 1 &amp; 2*:
```bash
Setup is 17116 bytes (padded to 17408 bytes).
System is 4897 kB
CRC 2f571cf0
Kernel: arch/x86/boot/bzImage is ready	(#1)
	Building modules, stage 2.
	MODPOST	3330 modules
(SNIP)
	CC		virt/lib/irqbypass.mod.o
	LD [M]	virt/lib/irqbypass.ko
root@Nwwz:/usr/src/linux-4.17.2#
```
If somehow, there&#39;s no *stage two*, a single command should be executed before moving on:
(This normally isn&#39;t required.)
```
make modules
```
Installing the modules:
```bash
root@Nwwz:/usr/src/linux-4.17.2# make modules_install
(SNIP)
	INSTALL	sound/usb/usx2y/snd-usb-usx2y.ko
	INSTALL	virt/lib/irqbypass.ko
	DEPMOD	4.17.0
root@Nwwz:/usr/src/linux-4.17.2#
```
Installing and preparing the kernel for boot:
```bash
root@Nwwz:/usr/src/linux-4.17.2# make install
(SNIP)
Found linux image: /boot/vmlinuz-4.17.0
Found initrd image: /boot/initrd.img-4.17.0
Found linux image: /boot/vmlinuz-4.9.0-6-amd64
Found initrd image: /boot/initrd.img-4.9.0-6-amd64
done
root@Nwwz:/usr/src/linux-4.17.2# cd /boot
root@Nwwz:/boot# mkinitramfs -o /boot/initrd.img-4.17.0 4.17.0
root@Nwwz:/boot# reboot
```
You can then choose the new kernel from the boot screen:
```
*Debian GNU/Linux, with Linux 4.17.0
 Debian GNU/Linux, with Linux 4.17.0 (recovery mode)
 Debian GNU/Linux, with Linux 4.9.0-6-amd64
 Debian GNU/Linux, with Linux 4.9.0-6-amd64 (recovery mode)
```
If it fails however, saying that it&#39;s an out-of-memory problem, you can reduce the size of the boot image.
```bash
root@Nwwz:/boot# cd /lib/modules/4.17.0/
root@Nwwz:/lib/modules/4.17.0# find . -name *.ko -exec strip --strip-unneeded {} +
root@Nwwz:/lib/modules/4.17.0# cd /boot
root@Nwwz:/boot# mkinitramfs -o initrd.img-4.17.0 4.17.0
```
It&#39;ll then boot successfully.
```
root@Nwwz:~# uname -r
4.17.0
```
##### Using KGDB to debug the kernel:
Installing **ifconfig** and running it would be the first thing to do:
```bash
root@Nwwz:~# apt-get install net-tools
(SNIP)
root@Nwwz:~# ifconfig
ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.150.145  netmask 255.255.255.0  broadcast 192.168.150.255
(SNIP)
```
Back to *Debian* machine, transfering **vmlinux** to the host is done with SCP or WinSCP in my case.
```
root@Nwwz:~# service ssh start
```
```
..							Répertoire parent
vmlinux			461 761 KB	Fichier
```
With this, you&#39;ll have debug symbols ready, but you still need to enable KGDB for the target kernel.
```bash
root@Nwwz:~# cd /boot/grub
root@Nwwz:/boot/grub# nano grub.cfg
```
Editing a single line, adding *__setup* arguments, we would then be able to manipulate the kernel for our needs, such as *disabling KASLR* and *enabling KGDB*.
Search for the first &#39;*Debian GNU*&#39; occurence and make sure it&#39;s the wanted kernel, and add the following to the line starting with [X]: *kgdboc=ttyS1,115200 kgdbwait nokaslr*.
```
menuentry &#39;Debian GNU/Linux&#39; --class debian --class gnu-linux --class gnu --class os $menuentry_id_option &#39;gnulinux-simple-b1a66d11-d729-4f23-99b0-4ddfea0af6c5&#39; {
	...
	echo	&#39;Loading Linux 4.17.0 ...&#39;
	[X] linux	/boot/vmlinuz-4.17.0 root=UUID=b1a66d11-d729-4f23-99b0-4ddfea0af6c5 ro quiet kgdboc=ttyS1,115200 kgdbwait nokaslr
	echo	&#39;Loading initial ramdisk ...&#39;
	initrd	/boot/initrd.img-4.17.0
}
```
In order to debug the running kernel, another VM similer to the one made previously(*Debian*) will be created(*Debian HOST*).
Now shutdown both VMs in order to set the pipe:
* *Debian*:
  ```
  ⦿ Use named pipe:
  	*---------------------------------------*
  	| \\.\pipe\com_2                        |
  	*---------------------------------------*
  	[This end is the server.             [▼]]
  	[The other end is a virtual machine. [▼]]
  ---------------------------------------------7
  I/O mode
  ⧆ Yield CPU on poll
  
  	Allow the guest operating system to use this serial
  	port in polled mode (as opposed to interrupt mode).
  ```
* *DebianHOST*:
  ```
  ⦿ Use named pipe:
  	*---------------------------------------*
  	| \\.\pipe\com_2                        |
  	*---------------------------------------*
  	[This end is the client.             [▼]]
  	[The other end is a virtual machine. [▼]]
  ---------------------------------------------7
  I/O mode
  ⧆ Yield CPU on poll
  
  	Allow the guest operating system to use this serial
  	port in polled mode (as opposed to interrupt mode).
  ```

Getting the **vmlinux** image to *DebianHOST* after installing necessary packages:
```bash
root@Nwwz:~# apt-get install gcc gdb git net-tools
root@Nwwz:~# cd /home/user
root@Nwwz:/home/user# ls
vmlinux
root@Nwwz:/home/user# gdb vmlinux
GNU gdb (Debian 7.12-6) 7.12.0.20161007-git
(SNIP)
```
Turning the Debian back on would result in a similiar message:
```
KASLR disabled: &#39;nokaslr&#39; on cmdline.
[	1.571915] KGDB: Waiting for connection from remote gdb...
```
Attaching to DebianHOST&#39;s GDB is then possible:
```
(gdb) set serial baud 115200
(gdb) target remote /dev/ttyS1
Remote debugging using /dev/ttyS1
kgdb_breakpoint () at kernel/debug/debug_core.c:1073
1073		wmb(); /* Sync point after breakpoint */
(gdb) list
1068	noinline void kgdb_breakpoint(void)
1069	{
1070		atomic_inc(&amp;kgdb_setting_breakpoint);
1071		wmb(); /* Sync point before breakpoint */
1072		arch_kgdb_breakpoint();
1073		wmb(); /* Sync point after breakpoint */
1074		atomic_dec(&amp;kgdb_setting_breakpoint);
1075	}
1076	EXPORT_SYMBOL_GPL(kgdb_breakpoint);
1077
(gdb)
```
Know that by writing *&#39;continue&#39;* on *GDB*, you wouldn&#39;t be able to control it again unless you use the *magic SysRq key* to force a *SIGTRAP* to happen:
```bash
root@Nwwz:~# echo &quot;g&quot; &gt; /proc/sysrq-trigger
```
And you can see in *DebianHOST* that it works.
```bash
(SNIP)
[New Thread 459]
[New Thread 462]
[New Thread 463]
[New Thread 476]
[New Thread 485]
[New Thread 487]

Thread 56 received signal SIGTRAP, Trace/breakpoint trap.
[Switching to Thread 489]
kgdb_breakpoint () at kernel/debug/debug_core.c:1073
1073	wmb(); /* Sync point after breakpoint */
(gdb)
```

##### Compiling a simple module:

A simple Hello 0x00sec module would be created.
We need to make a directory in root folder, and prepare two files:
```bash
root@Nwwz:~# mkdir mod
root@Nwwz:~# cd mod
root@Nwwz:~/mod/# nano hello.c
```
```c
#include &lt;linux/init.h&gt;
#include &lt;linux/module.h&gt;

static void hello_exit(void){
	printk(KERN_INFO &quot;Goodbye!\n&quot;);
}

static int hello_init(void){
	printk(KERN_INFO &quot;Hello 0x00sec!\n&quot;);
	return 0;
}
MODULE_LICENSE(&quot;GPU&quot;);

module_init(hello_init);
module_exit(hello_exit);
```
```bash
root@Nwwz:~/mod/# nano Makefile
```
```
obj-m += hello.o
KDIR   = /lib/modules/$(shell uname -r)/build

all:
	make -C $(KDIR) M=$(PWD) modules
clean:
	rm -rf *.ko *.o *.mod.* *.symvers *.order
```
Then, one can start compiling using *&#39;make&#39;* and insert/remove the module in kernel to trigger both init and exit handlers.
```
root@Nwwz:~/mod# make
make -c /lib/modules/4.17.0/build M=/root/mod modules
make[1]: Entering directory &#39;/usr/src/linux-4.17.2&#39;
	CC [M]	/root/mod/hello.o
	Building modules, stage 2.
	MODPOST 1 modules
	CC	/root/mod/hello.mod.o
	LD [M] /root/mod/hello.ko
make[1]: Leaving directory &#39;/usr/src/linux-4.17.2&#39;
root@Nwwz:~/mod# insmod hello.ko
root@Nwwz:~/mod# rmmod hello.ko
```
The messages would be by then saved in the *dmesg* circular buffer.
```bash
root@Nwwz:~/mod# dmesg | grep Hello
[ 6545.039487] Hello 0x00sec!
root@Nwwz:~/mod# dmesg | grep Good
[ 6574.452282] Goodbye!
```
To clean the current directory:
```bash
root@Nwwz:~/mod# make clean
```
##### What?:
The kernel doesn&#39;t count on the *C library* we&#39;ve been used to, because it&#39;s judged useless for it.
So instead, after the module is linked and loaded in kernel-space(*requires root privileges, duh*).
It can use header files available in the **kernel source tree**, which offers a huge number of functions such as *printk()* which logs the message and sets it&#39;s priority, *module_init()* and *module_exit()* to declare initialization and clean-up functions.
And while application usually run with no chance of changing their variables by another thread. This
certainly isn&#39;t the case for *LKM*s, since what they offer could be used by multiple processes at a single time, which could lead(*if the data dealt with is sensible, aka in critical region*) to a *panic*, or worse(*better?*), a **compromise**.
##### Few structs:
The kernel implements multiple locks, only semaphores and spinlocks will likely be used here.
When the *semaphore* is previously held, the thread will *sleep*, *waiting for the lock to be released* so he can *claim it*.
That&#39;s why it&#39;s a *sleeping lock*, therefore, it&#39;s only used in *process context*.
```c
/* Please don&#39;t access any members of this structure directly */
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count;
	struct list_head	wait_list;
};
```
It can then be initialized with sema_init() or DEFINE_SEMAPHORE():
```c
#define __SEMAPHORE_INITIALIZER(name, n)				\
{									\
	.lock		= __RAW_SPIN_LOCK_UNLOCKED((name).lock),	\
	.count		= n,						\
	.wait_list	= LIST_HEAD_INIT((name).wait_list),		\
}

static inline void sema_init(struct semaphore *sem, int val)
{
	static struct lock_class_key __key;
	*sem = (struct semaphore) __SEMAPHORE_INITIALIZER(*sem, val);
	lockdep_init_map(&amp;sem-&gt;lock.dep_map, &quot;semaphore-&gt;lock&quot;, &amp;__key, 0);
}
```
With *val* being the *much processes* that can *hold the lock* at once.
It&#39;s normally set to *1*, and a semaphore with a count of *1* is called a *mutex*.
Another type of locks would be *spinlocks*, it keeps the *thread spinning instead of sleeping*, for that reason, it can be used in the *interrupt context*.
```c
typedef struct spinlock {
	union {
		struct raw_spinlock rlock;

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
		struct {
			u8 __padding[LOCK_PADSIZE];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;


#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
	{					\
	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
	SPIN_DEBUG_INIT(lockname)		\
	SPIN_DEP_MAP_INIT(lockname) }

#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)

# define raw_spin_lock_init(lock)				\
	do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)
#endif

static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &amp;lock-&gt;rlock;
}

#define spin_lock_init(_lock)				\
do {							\
	spinlock_check(_lock);				\
	raw_spin_lock_init(&amp;(_lock)-&gt;rlock);		\
} while (0)
```
Enough with locks, what about *file_operations*?
This struct holds the *possible operations* that can be called on a *device/file/entry*.
When creating a *character device* by directly calling *cdev_alloc()* or *misc_register()*, it has to be provided along with the *major*(*on first function only*) and *minor*.
It is defined as follows:
```c
struct file_operations {
	struct module *owner;
	loff_t (*llseek) (struct file *, loff_t, int);
	ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
	ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
	...
} __randomize_layout;
```
There are similiar structs too, such as *inode_operations*, *block_device_operations* and *tty_operations*..
But they all provide *handlers* to *userspace function* if the file/inode/blockdev/tty is the target.
These are sometimes used by the attacker in order to *redirect execution* such as *perf_fops* or *ptmx_fops*.

The kernel provides some structs for lists with different *search times*.
The first being *double linked-list*, *list_head*, it&#39;s definition is simple, pointing to the *next* and *previous* *list_head*.
```c
struct list_head {
	struct list_head *next, *prev;
};
```
While the second is redblack tree, *rb_node*, provides better *search time*.
```c
struct rb_node {
	unsigned long  __rb_parent_color;
	struct rb_node *rb_right;
	struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));
```
It can be used to find the *target value* faster, if it&#39;s *bigger* than the *first node*(*head*), then go *right*, else, go *left*.
Function *container_of()* can then be used to extract the *container struct*.
Note: Each device, can have multiple *minors*, but it&#39;ll necessarily have a single *major*.
```bash
root@Nwwz:/# cd /dev
root@Nwwz:/dev# ls -l
total 0
crw------- 1 root root    [10], 175 Feb  9 09:24 agpgart
                            |
                            *-&gt; Same major, different minors.
                            |							
crw-r--r-- 1 root root    [10], 235 Feb  9 09:24 autofs
drwxr-xr-x 2 root root         160 Feb  9 09:24 block
drwxr-xr-x 2 root root          80 Feb  9 09:24 bsg
(SNIP)
[c]rw-rw-rw- 1 root tty      [5], [2] Feb  9 12:06 ptmx
|                             |    |
|                             |    *--&gt; Minor
*---&gt; Character Device        *---&gt; Major
(SNIP)
[b]rw-rw---- 1 root cdrom    [11], [0] Feb  9 09:24 sr0
|                             |    |
|                             |    *--&gt; Minor
*---&gt; Block Device            *---&gt; Major
(SNIP)
```
##### Debug a module:
When we started *gdb*, the only image it was aware of, is the *vmlinux* one.
It doesn&#39;t know about the *loaded module*, and doesn&#39;t know about the *load location*.
In order to provide these things and make debugging the module possible, one has to first transfer
the target module to *DebianHOST*.
```bash
root@Nwwz:~/mod# service ssh start
```
Once that&#39;s done, one should find different sections and addresses of the LKM in memory:
```bash
root@Nwwz:~/mod# insmod simple.ko
root@Nwwz:~/mod# cd /sys/module/simple/sections
root@Nwwz:/sys/module/simple/sections# ls -la
total 0
drwxr-xr-x 2 root root	   0 Aug 11 06:30 .
drwxr-xr-x 5 root root	   0 Aug  2 17:55 ..
-r-------- 1 root root	4096 Aug 11 06:31 .bss
-r-------- 1 root root	4096 Aug 11 06:31 .data
-r-------- 1 root root	4096 Aug 11 06:31 .gnu.linkonce.this_module
-r-------- 1 root root	4096 Aug 11 06:31 __mcount_loc
-r-------- 1 root root	4096 Aug 11 06:31 .note.gnu.build-id
-r-------- 1 root root	4096 Aug 11 06:31 .orc_unwind
-r-------- 1 root root	4096 Aug 11 06:31 .orc_unwind_ip
-r-------- 1 root root	4096 Aug 11 06:31 .rodata.str1.1
-r-------- 1 root root	4096 Aug 11 06:31 .rodata.str1.8
-r-------- 1 root root	4096 Aug 11 06:31 .strtab
-r-------- 1 root root	4096 Aug 11 06:31 .symtab
-r-------- 1 root root	4096 Aug 11 06:31 .text
root@Nwwz:/sys/module/simple/sections# cat .text
0xffffffffc054c000
root@Nwwz:/sys/module/simple/sections# cat .data
0xffffffffc054e000
root@Nwwz:/sys/module/simple/sections# cat .bss
0xffffffffc054e4c0
```
Back to *DebianHOST* and in gdb:
```python
(gdb) add-symbol-file simple.ko 0xffffffffc054c000 -s .data 0xffffffffc054e000 -s .bss 0xffffffffc054e4c0
```
And that&#39;s it.

#### Chapter II: *Overview on security and General understanding*
&quot;*Uuuuh, it&#39;s simple?*&quot;
##### Control Registers:
CRs are *special registers*, being *invisible* to the *user*, they hold *important* information on the current *CPU* and the *process* running on it.
**x86_32 and x86_64**:
Keep in mind that their *sizes* are *different*(*64bit for x86_64, 32bit for x86_32*).
*CR0*:
```bash
x32 and x64:
#0:     PE(Protected Mode Enable)
#1:     MP(Monitor co-processor)
#2:     EM(Emulation)
#3:     TS(Task Switched)
#4:     ET(Extension Type)
#5:     NE(Numeric Error)
#6-15:  Reserved
#16:    WP(Write Protect)
#17:    Reserved
#18:    AM(Alignment Mask)
#19-28: Reserved
#29:    NW(Not-Write Through)
#30:    CD(Cache Disable)
#31:    PG(Paging)
x64 only:
#32-61: Reserved
```
*CR2*:
Solely containing the *PFLA*(*Page Fault Linear Address*) address, which would later be extracted using *do_page_fault* function and passed to *__do_page_fault* to handle it.
```c
dotraplinkage void notrace
do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
	unsigned long address = read_cr2(); /* Get the faulting address */
	enum ctx_state prev_state;

	prev_state = exception_enter();
	if (trace_pagefault_enabled())
		trace_page_fault_entries(address, regs, error_code);

	__do_page_fault(regs, error_code, address);
	exception_exit(prev_state);
}
NOKPROBE_SYMBOL(do_page_fault);
```
*CR3*:
This register contains the physical address of the *current process* PGD(*Page Global Directory*), which(*once converted back to virtual address*) would link to the next *level*(*P4D on five-level page tables* or *PUD on four-level page tables*), but in the end, it&#39;s all to find the same struct, that is, *struct page*.
```c
static inline unsigned long read_cr3_pa(void)
{
	return __read_cr3() &amp; CR3_ADDR_MASK;
}

static inline unsigned long native_read_cr3_pa(void)
{
	return __native_read_cr3() &amp; CR3_ADDR_MASK;
}

static inline void load_cr3(pgd_t *pgdir)
{
	write_cr3(__sme_pa(pgdir));
}
```
This is called as an example when an Oops happens, and the kernel calls *dump_pagetable()*.
*CR4*:
```bash
x32 and x64:
#0:     VME(Virtual-8086 Mode Extensions)
#1:     PVI(Protected Mode Virtual Interrupts)
#2:     TSD(Time Stamp Disable)
#3:     DE(Debugging Extensions)
#4:     PSE(Page Size Extensions)
#5:     PAE(Physical Address Extensions)
#6:     MCE(Machine Check Enable)
#7:     PGE(Page Global Enable)
#8:     PCE(Performance-Monitoring Counter Enable)
#9:     OSFXSR(OS Support for FXSAVE and FXRSTOR Instructions)
#10:    OSXMMEXCPT(OS Support for Unmasked SIMD Floating Point Exceptions)
#11:    UMIP(User-Mode Instruction Prevention)
#12:    Reserved
#13:    VMXE(Virtual Machine Extensions Enable)
#14:    SMXE(Safer Mode Extensions Enable)
#15-16: Reserved
#17:    PCIDE(PCID Enable)
#18:    OSXSAVE(XSAVE and Processor Extended States Enable)
#19:    Reserved
#20:    SMEP(Supervisor Mode Execution Prevention)
#21:    SMAP(Supervisor Mode Access Prevention)
#22-31: Reserved
x64 only:
#31-63: Reserved
```
*CR1* and *CR5* to *CR7*:
Marked as *reserved*, accessing them would result in raising the *Undefined Behavior*(**#UD**) exception.
**x86_64 only**:
*CR8*:
Only the first 4 bits are used in this one, while the other 60 bits are *reserved(0)*.
Also called *TPR*(*Task Priority Register*). Those 4 bits are used when servicing interrupts, checking if the task should really be interrupted. It may or may not, depending on the interrupt&#39;s priority: (*IP &lt;= TP* ? *PASS*:**SERVICE**).

They differ from architecture to another, while the previous example reviewed two CISC(*x86_32*, *x86_64*). *Windows* itself does have much similiarities at this level:
![image|690x402](upload://7WkkQivFFlX8xG0EJLmv4WqrKjS.png) 
The thing is a little bit more different in RISC(*ARM* for this example):
Instead of *Control Registers*, they are named *Coprocessors*(*P0* to *P15*), each *Coprocessor* holds *16* registers(*C0* to *C15*). Note however, that only *CP14* and *CP15* are very important to the *system*.
*MCR* and *MRC* Instructions are available to deal with data transfer(*read/write*).
An example for the *TTBR*(*Translation Table Base Register*) is as follows:
![image|690x29](upload://jC14gDx9ZZjKpaK1oDzZksdxIrj.png)

##### SMAP:
Stands for *Supervisor Mode Access Prevention*, as it&#39;s name suggests, prevents access to user-space from a more *privileged context*, that is, *ring zero*. However, since access may still be necessary in certain occasions, a flag is dedicated(*AC in EFLAGS*) to this purpose, along with two instructions to *set* or *clear* it:
*CLAC*:
![image|690x83](upload://vQlcpknsX4fWsGJk9ji6eNFA21b.png)
*STAC*:
![image|690x86](upload://i5MnpYkZnj3JOkcaRNCAL5HCOFk.png) 
```c
static __init int setup_disable_smap(char *arg)
{
	setup_clear_cpu_cap(X86_FEATURE_SMAP);
	return 1;
}
__setup(&quot;nosmap&quot;, setup_disable_smap);
```
It can be disabled with *nosmap* boot flag, which would clear the CPU&#39;s *SMAP* capability, or by unsetting the *SMAP bit*(*#21*) on *CR4*.
##### SMEP:
An abbreviation for *Supervisor Mode Execution Prevention*, when running on *ring zero*, execution would not be allowed to be transmitted to *user-space*. So both *SMEP* and *SMAP* put a form of limitation on the *attacker&#39;s* surface.
```c
static __init int setup_disable_smep(char *arg)
{
	setup_clear_cpu_cap(X86_FEATURE_SMEP);
	
	check_mpx_erratum(&amp;boot_cpu_data);
	return 1;
}
__setup(&quot;nosmep&quot;, setup_disable_smep);
```
Knowing if it&#39;s on is as simple as checking */proc/cpuinfo*, and it&#39;s the same for *SMAP*.
This protection can be disabled with *nosmep* boot flag, it can also be disabled during runtime by unsetting *SMEP bit*(*#20*) on *CR4*.
##### Write-Protect:
Since code executing at the highest level of privilege should normally be capable of writting to all pages even those marked as *RO*(*Read Only*). However, a bit in *CR0*(*WP bit(16th)*) is supposed to stop that from happening, by providing additional checks.
##### Paging(a bit of segmentation too):
Linux does separate privileges. the processor can handle up to 4 different rings, starting from **0** which obviously is the *most privileged* and ending with **3** being the *least privileged* with limited access to *system resources*. However, most operating systems do work with only two rings, zero(also called *kernel-space*) and three(or *user-space*).
![image|353x215](upload://vFwqtwYnSRFoW3nUoUrVt2NT1UV.png)  
Each running process does have a **struct mm_struct** which fully describes it&#39;s virtual memory space.
But when it comes to *segmentation* and *paging*, we&#39;re only interested in few objects in this struct: *context*, the single-linked list *mmap* and *pgd*.
```c
typedef struct {

	u64 ctx_id;

	atomic64_t tlb_gen;

#ifdef CONFIG_MODIFY_LDT_SYSCALL
	struct rw_semaphore	ldt_usr_sem;
	struct ldt_struct	*ldt;
#endif

#ifdef CONFIG_X86_64
	unsigned short ia32_compat;
#endif

	struct mutex lock;
	void __user *vdso;
	const struct vdso_image *vdso_image;

	atomic_t perf_rdpmc_allowed;
#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS

	u16 pkey_allocation_map;
	s16 execute_only_pkey;
#endif
#ifdef CONFIG_X86_INTEL_MPX
	void __user *bd_addr;
#endif
} mm_context_t;
```
This struct holds many information on the context, including the *Local descriptor table(LDT)*, the VDSO image and base address(*residing in user-space **__user***), a *read/write semaphore* and a *mutual exclusion lock*(*it&#39;s a semaphore too, remember?*).
```c
struct ldt_struct {

	struct desc_struct	*entries;
	unsigned int		nr_entries;

	int			slot;
};
```
The first element in the LDT is a *desc_struct* pointer, referencing an array of entries, *nr_entries* of them.
However, know that *LDT* isn&#39;t usually set up, it would only use the *Global Descriptor Table*, it&#39;s enough for *most* processes.
```c
DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
#ifdef CONFIG_X86_64
	[GDT_ENTRY_KERNEL32_CS]		= GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc093, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER32_CS]	= GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),
#else
	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xc09a, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff),
	[GDT_ENTRY_PNPBIOS_CS32]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
	[GDT_ENTRY_PNPBIOS_CS16]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
	[GDT_ENTRY_PNPBIOS_DS]		= GDT_ENTRY_INIT(0x0092, 0, 0xffff),
	[GDT_ENTRY_PNPBIOS_TS1]		= GDT_ENTRY_INIT(0x0092, 0, 0),
	[GDT_ENTRY_PNPBIOS_TS2]		= GDT_ENTRY_INIT(0x0092, 0, 0),
	[GDT_ENTRY_APMBIOS_BASE]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
	[GDT_ENTRY_APMBIOS_BASE+1]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
	[GDT_ENTRY_APMBIOS_BASE+2]	= GDT_ENTRY_INIT(0x4092, 0, 0xffff),

	[GDT_ENTRY_ESPFIX_SS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
	[GDT_ENTRY_PERCPU]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
	GDT_STACK_CANARY_INIT
#endif
} };
EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
```
A per-cpu variable *gdt_page* is initialized using the *GDT_ENTRY_INIT* macro.
```c
#define GDT_ENTRY_INIT(flags, base, limit)			\
	{							\
		.limit0		= (u16) (limit),		\
		.limit1		= ((limit) &gt;&gt; 16) &amp; 0x0F,	\
		.base0		= (u16) (base),			\
		.base1		= ((base) &gt;&gt; 16) &amp; 0xFF,	\
		.base2		= ((base) &gt;&gt; 24) &amp; 0xFF,	\
		.type		= (flags &amp; 0x0f),		\
		.s		= (flags &gt;&gt; 4) &amp; 0x01,		\
		.dpl		= (flags &gt;&gt; 5) &amp; 0x03,		\
		.p		= (flags &gt;&gt; 7) &amp; 0x01,		\
		.avl		= (flags &gt;&gt; 12) &amp; 0x01,		\
		.l		= (flags &gt;&gt; 13) &amp; 0x01,		\
		.d		= (flags &gt;&gt; 14) &amp; 0x01,		\
		.g		= (flags &gt;&gt; 15) &amp; 0x01,		\
	}
```
This macro simply takes *three arguments*, and *splits them* in order to *store* at each *field* a *valid value*.
The *GDT* holds more entries on *32bit* than on *64bit*.
```c
struct gdt_page {
	struct desc_struct gdt[GDT_ENTRIES];
} __attribute__((aligned(PAGE_SIZE)));
```
Says that *gdt_page* is an array of *GDT_ENTRIES*(*32* on *x86_32*, *16* on *x86_64*) much of *desc_struct* aligned to *PAGE_SIZE*(*usually 4KB(4096)*).
```c
struct desc_struct {
	u16	limit0;
	u16	base0;
	u16	base1: 8, type: 4, s: 1, dpl: 2, p: 1;
	u16	limit1: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8;
} __attribute__((packed));
```
When an ELF is about to run, and is being loaded with *load_elf_binary()*, it does call *setup_new_exec()*, *install_exec_creds()* on *bprm* before it calls *setup_arg_pages()* which would pick a *random stack pointer*.
Before returning *successfully*, it would call *finalize_exec()* and *start_thread()* which would *update the stack&#39;s rlimit* and *begin execution* respectively:
```c
void
start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
{
	start_thread_common(regs, new_ip, new_sp,
			    __USER_CS, __USER_DS, 0);
}
EXPORT_SYMBOL_GPL(start_thread);
```
As you are able to see, this function is just a wrapper around *start_thread_common()*:
```c
static void
start_thread_common(struct pt_regs *regs, unsigned long new_ip,
		    unsigned long new_sp,
		    unsigned int _cs, unsigned int _ss, unsigned int _ds)
{
	WARN_ON_ONCE(regs != current_pt_regs());

	if (static_cpu_has(X86_BUG_NULL_SEG)) {

		loadsegment(fs, __USER_DS);
		load_gs_index(__USER_DS);
	}

	loadsegment(fs, 0);
	loadsegment(es, _ds);
	loadsegment(ds, _ds);
	load_gs_index(0);

	regs-&gt;ip		= new_ip;
	regs-&gt;sp		= new_sp;
	regs-&gt;cs		= _cs;
	regs-&gt;ss		= _ss;
	regs-&gt;flags		= X86_EFLAGS_IF;
	force_iret();
}
```
As a conclusion, every process starts with default segment registers, but different GPRs, stack and instruction pointer, and by looking at **__USER_DS** and **__USER_CS**:
```c
#define GDT_ENTRY_DEFAULT_USER_DS	5
#define GDT_ENTRY_DEFAULT_USER_CS	6

#define __USER_DS			(GDT_ENTRY_DEFAULT_USER_DS*8 + 3)
#define __USER_CS			(GDT_ENTRY_DEFAULT_USER_CS*8 + 3)
```
We would find the segment registers and their values on *user-space*:
```
Initial state:
CS = 6*8+3 = 0x33
SS = 5*8+3 = 0x2b
DS = FS = ES = 0
```
These values can be checked using *GDB* and a *dummy binary*.
```bash
(gdb) b* main
Breakpoint 1 at 0x6b0
(gdb) r
Starting program: /root/mod/cs

Breakpoint 1, 0x00005555555546b0 in main ()
(gdb) info reg cs ss
cs		0x33	51
ss		0x2b	43
```
Also, you should know that, CS holds in it&#39;s least 2 significant bits, the *Current Privilege Level(CPL)*, other segment selectors hold the *Requested Privilege Level(RPL)* instead of *CPL*.
```bash
(gdb) p/t $cs
$1 = 110011
(gdb) p/x $cs &amp; 0b11
$2 = 0x3
# (Privilege Level: User(3) SuperUser(0))
(gdb) p/d $cs &amp; ~0b1111
$3 = 48
# (Table Offset: 48)
(gdb) p/d $cs &amp; 0b100
$4 = 0
# (Table Indicator: GDT(0) LDT(1))
```
**3** stands for the *third ring*, *least privileged*, that is, *user-space*. 
It doesn&#39;t change, unless the execution is in *kernel-space*, so it&#39;s similiar for both *root* and *any normal user*. So both *RPL* and *CPL* could be considered a form of limitation when accessing segments with lower(*more privileged*) DPL(*Descriptor Privilege Level*).

When it comes to *paging*, it&#39;s *equivalent bit* in *CR0*(*#31*) is only set when the system is running in *protected mode*(*PE bit* in *CR0* is set), because in *real mode*, **virtual address are equal to physical ones**.
Linux moved from four-level page tables to support five-level page tables by adding an additional layer(*P4D*), so the levels now are: *PGD* *P4D* *PUD* *PMD* *PTE*.
*PGD* is the first level *Page Global Directory*, it is a pointer of type *pgd_t*, and it&#39;s definition is:
```c
typedef struct { pgdval_t pgd; } pgd_t;
```
It holds a *pgdval_t* inside, which is an unsigned long(*8 bytes on x86_64, 4 on x86_32*):
```c
typedef unsigned long	pgdval_t;
```
To get to the next level, *pagetable_l5_enabled()* is called to check if the CPU has *X86_FEATURE_LA57* enabled.
```c
#define pgtable_l5_enabled() cpu_feature_enabled(X86_FEATURE_LA57)
```
This can be seen in *p4d_offset()*:
```c
static inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)
{
	if (!pgtable_l5_enabled())
		return (p4d_t *)pgd;
	return (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);
}
```
If it isn&#39;t enabled, it simply casts the *pgd_t \** as *p4d_t \** and returns it, otherwise it returns the *P4D entry* within the *PGD* that links to the *specific address*.
Then *P4D* itself can be used to find the next level, which is *PUD* of type *pud_t \**, *PUD* links to *PMD*(*Page Middle Directory*) and *PMD* to the *PTE*(*Page Table Entry*) which is the last level, and contains the *physical address* of the *page* with some *protection flags* and is of type *pte_t \**.

Each *process* has it&#39;s own *virtual space*(*mm_struct*, *vm_area_struct* and *pgd_t*).
```c
struct vm_area_struct {

	unsigned long vm_start;
	unsigned long vm_end;

	struct vm_area_struct *vm_next, *vm_prev;

	struct rb_node vm_rb;


	unsigned long rb_subtree_gap;


	struct mm_struct *vm_mm;
	pgprot_t vm_page_prot;
	unsigned long vm_flags;	

	struct {
		struct rb_node rb;
		unsigned long rb_subtree_last;
	} shared;

	struct list_head anon_vma_chain;
	struct anon_vma *anon_vma;

	const struct vm_operations_struct *vm_ops;

	unsigned long vm_pgoff;
	struct file * vm_file;
	void * vm_private_data;

	atomic_long_t swap_readahead_info;
#ifndef CONFIG_MMU
	struct vm_region *vm_region;
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *vm_policy;
#endif
	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
} __randomize_layout;
```
```c
typedef struct { pgdval_t pgd; } pgd_t;
```
So creating a new process would be very expensive on performance. *Copy-on-Write*(**COW**) comes in helpful here, by making a clone out of the parent process and only copying when a write happens to the previously marked *read-only* pages.
This happens on fork and more specifically in *copy_process()*, which duplicates the *task_struct* and does specific operations depending on flags passed to *clone()*, before copying all parent information which includes *credentials, filesystem, files, namespaces, IO, Thread Local Storage, signal, address space*.
As an example, this walks *VMAs* in search of a user specified address, once found, it gets its *Physical address* and *Flags* by walking *page tables*.
```c
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/proc_fs.h&gt;
#include &lt;linux/sched.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;asm/pgtable.h&gt;
#include &lt;linux/highmem.h&gt;
#include &lt;linux/slab.h&gt;

#define device_name &quot;useless&quot;
#define SET_ADDRESS 0x00112233

char *us_buf;
unsigned long address = 0;

long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	switch(cmd){
		case SET_ADDRESS:
			address = arg;
			return 0;
		default:
			return -EINVAL;
	}
}

ssize_t do_read(struct file *filp, char *buf, size_t count, loff_t *offp){
	int res, phys, flags;
	struct vm_area_struct *cmap;
	pgd_t *pgd;
	p4d_t *p4d;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep;

	/* Find corresponding VMA */
	cmap    = current-&gt;mm-&gt;mmap;

	while(1){
		if(cmap-&gt;vm_start &lt;= address &amp;&amp; address &lt; cmap-&gt;vm_end){
			break;
		}
		
		cmap     = cmap-&gt;vm_next;
		if(cmap  == NULL){
			return -1;
		}
	};
	
	/* Walking Page-tables for fun */
	pgd     = pgd_offset(current-&gt;mm, address);
	p4d     = p4d_offset(pgd,         address);
	pud     = pud_offset(p4d,         address);
	pmd     = pmd_offset(pud,         address);
	ptep    = pte_offset_kernel(pmd,  address);
	phys    = *((int *) ptep);
	flags   = phys &amp; 0xfff;
	phys   &amp;= ~0xfff;
	
	snprintf(us_buf, 64, &quot;PhysAddr(%x) VMAStart(%lx) Flags(%x)&quot;, phys, cmap-&gt;vm_start, flags);

	if(count &gt; 64)
		count = 64;
	res = copy_to_user(buf, us_buf, count);
	return res;
}

struct file_operations fileops = {
					.owner = THIS_MODULE,
					.read  = do_read,
					.unlocked_ioctl = do_ioctl,
				 };

static int us_init(void){
	struct proc_dir_entry *res;

	us_buf = kmalloc(64, GFP_KERNEL);
	if(us_buf == NULL){
		printk(KERN_ERR &quot;Couldn&#39;t reserve memory.&quot;);
		return -ENOMEM;
	}
	
	res = proc_create(device_name, 0, NULL, &amp;fileops);
	if(res == NULL){
		printk(KERN_ERR &quot;Failed allocating a proc entry.&quot;);
		return -ENOMEM;
	}
	
	return 0;
}

static void us_exit(void){
	remove_proc_entry(device_name, NULL);
	kfree(us_buf);
}
MODULE_LICENSE(&quot;GPU&quot;);

module_init(us_init);
module_exit(us_exit);
```
To communicate with this *proc entry*, the following was written:
```c
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/ioctl.h&gt;

#define device_path &quot;/proc/useless&quot;
#define SET_ADDRESS 0x00112233


void main(void){
	int fd;
	char *ok;
	char c[64];

	fd = open(device_path, O_RDONLY);
	
	ok = malloc(512);
	memcpy(ok, &quot;Welp&quot;, sizeof(int ));
	
	ioctl(fd, SET_ADDRESS, ok);

	read(fd, c, sizeof( c));
	printf(&quot;%s\n&quot;, &amp;c);
}
```
This gives:
![image|419x98](upload://qwsO47pCKiLfvekFOpRV0YnwdjR.png)
*0x867 in binary is: 100001100111.
Present: 1 (The page is present)
R/W: 1 (The page have both read and write permissions)
U/S: 1 (The page can be accessed by the user and supervisor)
00
Accessed: 1 (Set if the page had been accessed)
Dirty: 1 (Set if the page was written to since last writeback)
0000*

Note that necessary checks on validity of return values was ignored in this example, these could be performed with *p??_none()* and *p??_present()*, and multiple other things could have been done, such as playing with the *PFN* or *page* or reading from the *Physical Address* with *void __iomem \**, *ioremap()* and *memcpy_fromio()* or *struct page \** and *kmap()*.

Translating address from virtual to physical takes time, so caching is implemented using the *TLB*(*Translation Lookaside Buffer*) to improve the performance, hopefully that the next access is going to land a cache-hit and that&#39;ll hand the *PTE* faster than a miss where a *memory access* is forced to happen to get it. The *TLB* flushes from time to another, an example would be after a *page fault* is raised and completed.
##### Processes:
The kernel sees each process as a **struct task_struct** which is a huge struct that contains many fields which we can&#39;t cover entirely, some are used to guarantee the (almost) fair scheduling and some show the task&#39;s state(if it&#39;s either unrunnable, runnable or stopped), priority, the parent process, a linked list of children processes, the address space it holds, and many others.
We are mainly interested in the **const struct cred __rcu \*cred;** which holds the task&#39;s credentials.
```c
struct cred {
	atomic_t	usage;
#ifdef CONFIG_DEBUG_CREDENTIALS
	atomic_t	subscribers;
	void		*put_addr;
	unsigned	magic;
#define CRED_MAGIC	0x43736564
#define CRED_MAGIC_DEAD	0x44656144
#endif
	kuid_t		uid;
	kgid_t		gid;
	kuid_t		suid;
	kgid_t		sgid;
	kuid_t		euid;
	kgid_t		egid;
	kuid_t		fsuid;
	kgid_t		fsgid;
	unsigned	securebits;
	kernel_cap_t	cap_inheritable;
	kernel_cap_t	cap_permitted;
	kernel_cap_t	cap_effective;
	kernel_cap_t	cap_bset;
	kernel_cap_t	cap_ambient;
#ifdef CONFIG_KEYS
	unsigned char	jit_keyring;
	struct key __rcu *session_keyring;
	struct key	*process_keyring;
	struct key	*thread_keyring;
	struct key	*request_key_auth;
#endif
#ifdef CONFIG_SECURITY
	void		*security;
#endif
	struct user_struct *user;
	struct user_namespace *user_ns;
	struct group_info *group_info;
	struct rcu_head	rcu;
} __randomize_layout;
```
This struct holds Capabilities, ((effective) *user* and *group*) ID, keyrings, (for synchronization, *Read-Copy-Update*) RCU, (tracks the user&#39;s *usage* of the system by keeping **counts**) user and (holds *U/G ID* and the **privileges** for them) user_ns.
In order to better understand this structure, a simple *proc entry* was created which extracts the *task_struct* of the process that uses it(**current**) and reads the effective *UID* and *GID*.
```c
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/proc_fs.h&gt;
#include &lt;linux/sched.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;linux/cred.h&gt;
#include &lt;linux/uidgid.h&gt;

#define device_name &quot;useless&quot;
#define SD_PRIV     0x10071007

struct{
	kuid_t ceuid;
	kgid_t cegid;
	spinlock_t clock;
}us_cd;

long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	int res;

	switch(cmd){
		case SD_PRIV:
			spin_lock(&amp;us_cd.clock);
			current_euid_egid(&amp;us_cd.ceuid, &amp;us_cd.cegid);
			spin_unlock(&amp;us_cd.clock);
			res = copy_to_user((void *)arg, &amp;us_cd, 8);
			return res;
		default:
			return -EINVAL;
	}
}

struct file_operations fileops = {
					.owner = THIS_MODULE,
					.unlocked_ioctl = do_ioctl,
				 };

static int us_init(void){
	struct proc_dir_entry *res;

	spin_lock_init(&amp;us_cd.clock);
	res = proc_create(device_name, 0, NULL, &amp;fileops);
	if(res == NULL){
		printk(KERN_ERR &quot;Failed allocating a proc entry.&quot;);
		return -ENOMEM;
	}

	return 0;
}

static void us_exit(void){
	remove_proc_entry(device_name, NULL);
}
MODULE_LICENSE(&quot;GPU&quot;);

module_init(us_init);
module_exit(us_exit);
```
The initialization process starts by preparing the spinlock and creating a proc entry with a specified name *&quot;useless&quot;* and a file_operations struct containing only necessary **owner** and **unlocked_ioctl** entries.
While the ioctl handler simply checks if the command passed was *SD_PRIV* to extract the *UID* and *GID* with a call to the **current_euid_egid()** macro which in turn calls **current_cred()** to extract the *current-&gt;cred*:
```c
#define current_euid_egid(_euid, _egid)		\
do {						\
	const struct cred *__cred;		\
	__cred = current_cred();		\
	*(_euid) = __cred-&gt;euid;		\
	*(_egid) = __cred-&gt;egid;		\
} while(0)
```
```c
#define current_cred() \
	rcu_dereference_protected(current-&gt;cred, 1)
```
Then, we create a **tasktry.c** to interract with the */proc/useless*.
```c
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/ioctl.h&gt;

#define device_path &quot;/proc/useless&quot;
#define SD_PRIV     0x10071007

struct{
	unsigned int uid;
	unsigned int gid;
}data;

void main(void){
	int fd;

	fd = open(device_path, O_RDONLY);
	ioctl(fd, SD_PRIV, &amp;data);

	printf(&quot;UID: %d GID: %d\n&quot;, data.uid, data.gid);
}
```
Two binaries are then created in **/tmp** directory, one which is compiled by root(*setuid* bit set) tasktry_root and the other by a normal user called tasktry_user.
```
root@Nwwz:~# cd /tmp
root@Nwwz:/tmp# gcc tasktry.c -o tasktry_root; chmod u+s tasktry_root
root@Nwwz:/tmp# cd /root/mod
root@Nwwz:~/mod# make
make -c /lib/modules/4.17.0/build M=/root/mod modules
make[1]: Entering directory &#39;/usr/src/linux-4.17.2&#39;
	CC [M]	/root/mod/task.o
	Building modules, stage 2.
	MODPOST 1 modules
	CC	/root/mod/task.mod.o
	LD [M] /root/mod/task.ko
make[1]: Leaving directory &#39;/usr/src/linux-4.17.2&#39;
root@Nwwz:~/mod# insmod task.ko
root@Nwwz:~/mod# su - user
user@Nwwz:~$ cd /tmp
user@Nwwz:/tmp$ gcc tasktry.c -o tasktry_user
user@Nwwz:/tmp$ ls
tasktry_user tasktry_root tasktry.c
user@Nwwz:/tmp$ ./tasktry_root
UID: 0 GID: 1000
user@Nwwz:/tmp$ ./tasktry_user
UID: 1000 GID: 1000
```
As you can see, the effective UID of *tasktry_root* is *0* making it own high privileges, so overwritting *effective creds* is one way to *privilege escalation*(*prepare_kernel_creds()* and *commit_creds()* are used for this *purpose in most exploits*, instead of getting the *stack base* and *overwritting it directly*.), another is to *change capabilities*.
On *Windows*, one way to *escalate privileges* would be to steal the token of *System process*(*ID 4*) and assign it to the newly spawned *cmd.exe* after changing the *reference count*:
![image|690x269](upload://g4ZHnS7f1r9sa9TBv5Hted3xpmR.png) 
##### Syscalls:
Processes running in userspace can still communicate with the kernel, thanks to *syscalls*.
Each syscall is defined as follows:
```c
SYSCALL_DEFINE0(getpid)
{
	return task_tgid_vnr(current);
}
```
With multiple arguments:
```c
SYSCALL_DEFINE3(lseek, unsigned int, fd, off_t, offset, unsigned int, whence)
{
	return ksys_lseek(fd, offset, whence);
}
```
So, in general:
```c
SYSCALL_DEFINE[ARG_COUNT]([SYSCALL_NAME], [ARG_TYPE], [ARG_NAME]){
	/* Passing the argument to another function, for processing. */
	return call_me([ARG_NAME]);
}
```
Few tries aaand :slight_smile::
```c
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;

int main(void){
	printf(&quot;ID: %d\n&quot;, getuid());
	
	return 0;
}
```
Running this sample with GDB and putting breakpoint on the x64 libc, we can see that it does set *EAX* register to *0x66*(**syscall number on x64**) before the *syscall* instruction.
```
(gdb) x/i $rip
=&gt; 0x555555554704 &lt;main+4&gt;:		callq 0x5555555545a0 &lt;getuid@plt&gt;
(gdb) x/x getuid
0x7ffff7af2f30 &lt;getuid&gt;: 		0x000066b8
(gdb) b* getuid
Breakpoint 2 at 0x7ffff7af2f30: file ../sysdeps/unix/syscall-template.S, line 65.
(gdb) c
Continuing.

Breakpoint 2, getuid () at ../sysdeps/unix/syscall-template.S:65
65		../sysdeps/unix/syscall-template.S: No such file or directory.
(gdb) disas $rip
Dump of assembler code for function getuid:
=&gt; 0x00007ffff7af2f30 &lt;+0&gt;:		mov		$0x66,%eax
   0x00007ffff7af2f35 &lt;+5&gt;:		syscall
   0x00007ffff7af2f37 &lt;+7&gt;:		retq
 End of assembler dump.
(gdb) shell
root@Nwwz:~# echo &quot;g&quot; &gt; /proc/sysrq-trigger
```
We can invoke a shell from *GDB* to force *SysRQ*, and see what this *offset* in the *kernel* links for:
```bash
[New Thread 756]
[New Thread 883]
[New Thread 885]

Thread 103 received signal SIGTRAP, Trace/breakpoint trap.
[Switching to Thread 889]
kgdb_breakpoint () at kernel/debug/debug_core.c:1073
10733			wmb(); /* Sync point after breakpoint */
(gdb) p &amp;sys_call_table
$1 = (const sys_call_ptr_t (*)[]) 0xffffffff81c00160 &lt;sys_call_table&gt;
(gdb) x/gx (void *)$1 + 0x66*8
0xffffffff81c00490 &lt;sys_call_table+816&gt;:	0xffffffff8108ec60
(gdb) x/i 0xffffffff8108ec60
0xffffffff8108ec60 &lt;__x64_sys_getuid&gt;:		nopl	0x0(%rax,%rax,1)
```
So, it&#39;s the global **sys_call_table**, indexing the *__x64_sys_getuid* there.
```
&quot;The __x64_sys_*() stubs are created on-the-fly for sys_*() system calls&quot;
is written in syscall_64.tbl that contains all the syscalls
available to the kernel.
```
This is similiar to the *nt!KiServiceTable* on Windows.
```
kd&gt; dps nt!KeServiceDescriptorTable
82b759c0  82a89d9c nt!KiServiceTable
82b759c4  00000000
82b759c8  00000191
82b759cc  82a8a3e4 nt!KiArgumentTable
82b759d0  00000000
82b759d4  00000000
kd&gt; dd nt!KiServiceTable
82a89d9c  82c85c28 82acc40d 82c15b68 82a3088a
82a89dac  82c874ff 82b093fa 82cf7b05 82cf7b4e
82a89dbc  82c0a3bd 82d11368 82d125c1 82c00b95
kd&gt; ln 82c85c28
(82c85c28)   nt!NtAcceptConnectPort   |  (82c85ca5)   nt!EtwpRundownNotifications
Exact matches:
    nt!NtAcceptConnectPort = &lt;no type information&gt;
kd&gt; ln 82acc40d 
(82acc40d)   nt!NtAccessCheck   |  (82acc43e)   nt!PsGetThreadId
Exact matches:
    nt!NtAccessCheck = &lt;no type information&gt;
kd&gt; ln 82d125c1
(82d125c1)   nt!NtAddDriverEntry   |  (82d125f3)   nt!NtDeleteDriverEntry
Exact matches:
    nt!NtAddDriverEntry = &lt;no type information&gt;
```
Dissasembling it gives us:
```asm
(gdb) disas __x64_sys_getuid
Dump of assembler code for function __x64_sys_getuid:
	0xffffffff8108ec60 &lt;+0&gt;:	nopl	0x0(%rax,%rax,1)
	0xffffffff8108ec65 &lt;+5&gt;:	mov		%gs:0x15c00,%rax
	0xffffffff8108ec6e &lt;+14&gt;:	mov		0x668(%rax),%rax
	0xffffffff8108ec75 &lt;+21&gt;:	mov		0x4(%rax),%esi
	0xffffffff8108ec78 &lt;+24&gt;:	mov		0x88(%rax),%rdi
	0xffffffff8108ec7f &lt;+31&gt;:	callq	0xffffffff8112d4a0 &lt;from_kuid_munged&gt;
	0xffffffff8108ec84 &lt;+36&gt;:	mov		%eax,%eax
	0xffffffff8108ec86 &lt;+38&gt;:	retq
```
With a basic understanding of ASM and a very limited knowledge of the kernel *(AT&amp;T haha, too lazy to switch the syntax :wink:.)*, one can know that it does first search for the *current* task, store some pointer it holds at offset *0x668* at RAX before dereferencing it again and using content at +0x88(*RDI*) and +0x4(*RSI*) as arguments to the **from_kuid_munged** call before it nops and returns(*q there stands for qword*).
We can verify this either by *looking at the source*:
```c
SYSCALL_DEFINE0(getuid)
{
	return from_kuid_munged(current_user_ns(), current_uid());
}
```
```c
uid_t from_kuid_munged(struct user_namespace *targ, kuid_t kuid)
{
	uid_t uid;
	uid = from_kuid(targ, kuid);

	if (uid == (uid_t) -1)
		uid = overflowuid;
	return uid;
}
EXPORT_SYMBOL(from_kuid_munged);
```
Or checking in *GDB*(*maybe both?*):

```
(gdb) b* __x64_sys_getuid
Breakpoint 1 at 0xffffffff8108ec60: file kernel/sys.c, line 920.
(gdb) c
[New Thread 938]
[Switching to Thread 938]

Thread 122 hit Breakpoint 1, __x64_sys_getuid () at kernel/sys.c:920
920		{
(gdb) ni
get_current () at ./arch/x86/include/asm/current.h:15
15		return this_cpu_read_stable(current_task);
(gdb) x/i $rip
=&gt; 0xffffffff8108ec65 &lt;__x64_sys_getuid+5&gt;:		mov		%gs:0x15c00,%rax
(gdb) p ((struct task_struct *)0)-&gt;cred
Cannot access memory at address 0x668
(gdb) p ((struct cred *)0)-&gt;uid
Cannot access memory at address 0x4
(gdb) p ((struct cred *)0)-&gt;user_ns
Cannot access memory at address 0x88
```
The sys_call_table is residing in a RO(*read only*) memory space:
```bash
(gdb) x/x sys_call_table
0xffffffff81c00160 &lt;sys_call_table&gt;:	0xffffffff81247310
(gdb) maintenance info sections
...
 [3]	0xffffffff81c00000-&gt;0xffffffff81ec1a42 at 0x00e00000: .rodata ALLOC LOAD RELOC DATA HAS_CONTENTS
...
(gdb) 
```
But a kernel module can overcome this protection and place a hook at any *systemcall*.
For that, two example modules will be given:
**=] Disabling the previously discussed WP(*write-protect*) bit in the CR0(*control register #0*), using *read_cr0* and *write_cr0* to acheive that.**
```c
#include &lt;linux/fs.h&gt;
#include &lt;asm/pgtable.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;linux/kallsyms.h&gt;
#include &lt;linux/miscdevice.h&gt;
#include &lt;asm/special_insns.h&gt;

#define device_name &quot;hookcontrol&quot;
#define ioctl_base    0x005ec
#define ioctl_enable  ioctl_base+1
#define ioctl_disable ioctl_base+2

int    res;
int  (*real_getuid)(void);
void **sys_call_table;
unsigned long const *address;

static int hooked_getuid(void){
	printk(KERN_INFO &quot;Received getuid call from %s!&quot;, current-&gt;comm);
	if(real_getuid != NULL){
		return real_getuid();
	}
	
	return 0;
}

long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	unsigned long cr0 = read_cr0();

	switch(cmd){
		case ioctl_enable:
		printk(KERN_INFO &quot;Enabling hook!&quot;);
		write_cr0(cr0 &amp; ~0x10000);
		sys_call_table[__NR_getuid] = hooked_getuid;
		write_cr0(cr0 |  0x10000);
		printk(KERN_INFO &quot;Successfully changed!&quot;);

		return 0;
		case ioctl_disable:
		printk(KERN_INFO &quot;Disabling hook!&quot;);
		write_cr0(cr0 &amp; ~0x10000);
		sys_call_table[__NR_getuid] = real_getuid;
		write_cr0(cr0 |  0x10000);
		printk(KERN_INFO &quot;Successfully restored!&quot;);
		
		return 0;
		default:
		return -EINVAL;
	}
}

struct file_operations file_ops = {
									.owner          = THIS_MODULE,
									.unlocked_ioctl = do_ioctl
								  };

struct miscdevice hk_dev = {
							MISC_DYNAMIC_MINOR,
							device_name,
							&amp;file_ops
						   };

static int us_init(void){
	res = misc_register(&amp;hk_dev);
	if(res){
		printk(KERN_ERR &quot;Couldn&#39;t load module!&quot;);
		return -1;
	}
	
	sys_call_table = (void *) kallsyms_lookup_name(&quot;sys_call_table&quot;);
	real_getuid    = sys_call_table[__NR_getuid];
	address        = (unsigned long *) &amp;sys_call_table;
	printk(KERN_INFO &quot;Module successfully loaded with minor: %d!&quot;, hk_dev.minor);
	return 0;
}

static void us_exit(void){
	misc_deregister(&amp;hk_dev);
}
MODULE_LICENSE(&quot;GPL&quot;);

module_init(us_init);
module_exit(us_exit);
```
**=] *Orr*&#39;ing the protection mask of the page at which it resides(*__pgprot(_PAGE_RW)*)( *set_memory_rw()* &amp; *set_memory_rw()*), or directly modifying the *PTE*.**
```c
static inline pte_t pte_mkwrite(pte_t pte)
{
	return pte_set_flags(pte, _PAGE_RW);
}

static inline pte_t pte_wrprotect(pte_t pte)
{
	return pte_clear_flags(pte, _PAGE_RW);
}
```
Looking at these functions, one can safely assume that manipulation can be acheived with simple OR and AND(*_PAGE_RW*) operations on the *pte_t*.
```c
pte_t *lookup_address(unsigned long address, unsigned int *level)
{
	return lookup_address_in_pgd(pgd_offset_k(address), address, level);
}
```
Since it&#39;s a kernel address, *pgd_offset_k()* is called, which makes use of **&amp;init_mm**, instead of a mm_struct belonging to some process of *one&#39;s choice*.
```c
pte_t *lookup_address_in_pgd(pgd_t *pgd, unsigned long address,
			     unsigned int *level)
{
	p4d_t *p4d;
	pud_t *pud;
	pmd_t *pmd;

	*level = PG_LEVEL_NONE;

	if (pgd_none(*pgd))
		return NULL;

	p4d = p4d_offset(pgd, address);
	if (p4d_none(*p4d))
		return NULL;

	*level = PG_LEVEL_512G;
	if (p4d_large(*p4d) || !p4d_present(*p4d))
		return (pte_t *)p4d;

	pud = pud_offset(p4d, address);
	if (pud_none(*pud))
		return NULL;

	*level = PG_LEVEL_1G;
	if (pud_large(*pud) || !pud_present(*pud))
		return (pte_t *)pud;

	pmd = pmd_offset(pud, address);
	if (pmd_none(*pmd))
		return NULL;

	*level = PG_LEVEL_2M;
	if (pmd_large(*pmd) || !pmd_present(*pmd))
		return (pte_t *)pmd;

	*level = PG_LEVEL_4K;

	return pte_offset_kernel(pmd, address);
}
```
so, the *ioctl* handler looks like this:
```c
long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	unsigned int level;
	pte_t *pte = lookup_address(*address, &amp;level);;

	switch(cmd){
		case ioctl_enable:
		printk(KERN_INFO &quot;Enabling hook!&quot;);
		pte-&gt;pte |= _PAGE_RW;
		sys_call_table[__NR_getuid] = hooked_getuid;
		pte-&gt;pte &amp;= ~_PAGE_RW;
		printk(KERN_INFO &quot;Successfully changed!&quot;);

		return 0;
		case ioctl_disable:
		printk(KERN_INFO &quot;Disabling hook!&quot;);
		pte-&gt;pte |= _PAGE_RW;
		sys_call_table[__NR_getuid] = real_getuid;
		pte-&gt;pte &amp;= ~_PAGE_RW;
		printk(KERN_INFO &quot;Successfully restored!&quot;);
		
		return 0;
		default:
		return -EINVAL;
	}
}
```
(Know that these are only *examples*, **usually**, *replacing* should take place at *init* and *restoring the original* at *exit*, plus the definition of both the *hook* and *original* handlers, should hold *asmlinkage*(passing *arguments* in *stack*, unlike *fastcall*(*default*) in *registers*), however, since the syscall here holds no *arguments*, this was *ignored*.)
By running an application from user-space to interact with **/dev/hookcontrol**: (*enabling and disabling after a while*) and taking a look at *dmesg*:
![image|377x307](upload://3w48GWeMaYiIqdgRYJRrBDKcYCw.png) 
This can be used to provide a layer on the syscall, prevent or manipulate the return value, like *kill* to prevent a process from being *killed*, *getdents* to *hide* some files, *unlink* to prevent a file from being *deleted*, et cetera..
And it doesn&#39;t stop here, even without *syscall hooking*, one can play with processes(*hide them as an example..*) with *task_struct elements* and *per-task flags*, or change the *file_operations* in some *specific struct*, and **many** other possibilities.

##### IDT(Interrupt Descriptor Table):
In order to handle *exceptions*, this *table* exists, by linking a *specific handler* to each exception, it helps deal with those raised from *userspace*(*a translation to ring zero is required first*) and *kernelspace*.
It first is initialized during early setup, and this can be seen in *setup_arch()* which calls multiple functions, some to setup the *IDT*, most important to us is *idt_setup_traps()*:
```c
void __init idt_setup_traps(void)
{
	idt_setup_from_table(idt_table, def_idts, ARRAY_SIZE(def_idts), true);
}
```
It makes use of the *default IDT*s array(*def_idts*).
```c
static const __initconst struct idt_data def_idts[] = {
	INTG(X86_TRAP_DE,		divide_error),
	INTG(X86_TRAP_NMI,		nmi),
	INTG(X86_TRAP_BR,		bounds),
	INTG(X86_TRAP_UD,		invalid_op),
	INTG(X86_TRAP_NM,		device_not_available),
	INTG(X86_TRAP_OLD_MF,		coprocessor_segment_overrun),
	INTG(X86_TRAP_TS,		invalid_TSS),
	INTG(X86_TRAP_NP,		segment_not_present),
	INTG(X86_TRAP_SS,		stack_segment),
	INTG(X86_TRAP_GP,		general_protection),
	INTG(X86_TRAP_SPURIOUS,		spurious_interrupt_bug),
	INTG(X86_TRAP_MF,		coprocessor_error),
	INTG(X86_TRAP_AC,		alignment_check),
	INTG(X86_TRAP_XF,		simd_coprocessor_error),

#ifdef CONFIG_X86_32
	TSKG(X86_TRAP_DF,		GDT_ENTRY_DOUBLEFAULT_TSS),
#else
	INTG(X86_TRAP_DF,		double_fault),
#endif
	INTG(X86_TRAP_DB,		debug),

#ifdef CONFIG_X86_MCE
	INTG(X86_TRAP_MC,		&amp;machine_check),
#endif

	SYSG(X86_TRAP_OF,		overflow),
#if defined(CONFIG_IA32_EMULATION)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_compat),
#elif defined(CONFIG_X86_32)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
#endif
};
```
On *x86_32* as an example, when an *int 0x80* is raised. the following happens:
```c
static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
{
	struct thread_info *ti = current_thread_info();
	unsigned int nr = (unsigned int)regs-&gt;orig_ax;

#ifdef CONFIG_IA32_EMULATION
	ti-&gt;status |= TS_COMPAT;
#endif

	if (READ_ONCE(ti-&gt;flags) &amp; _TIF_WORK_SYSCALL_ENTRY) {
	
		nr = syscall_trace_enter(regs);
	}

	if (likely(nr &lt; IA32_NR_syscalls)) {
		nr = array_index_nospec(nr, IA32_NR_syscalls);
#ifdef CONFIG_IA32_EMULATION
		regs-&gt;ax = ia32_sys_call_table[nr](regs);
#else

		regs-&gt;ax = ia32_sys_call_table[nr](
			(unsigned int)regs-&gt;bx, (unsigned int)regs-&gt;cx,
			(unsigned int)regs-&gt;dx, (unsigned int)regs-&gt;si,
			(unsigned int)regs-&gt;di, (unsigned int)regs-&gt;bp);
#endif
	}

	syscall_return_slowpath(regs);
}

__visible void do_int80_syscall_32(struct pt_regs *regs)
{
	enter_from_user_mode();
	local_irq_enable();
	do_syscall_32_irqs_on(regs);
}
```
It would call *enter_from_user_mod()* to , then enable Interrupt Requests(*IRQs*) on the *current CPU*.
Push the saved registers to find the syscall number(*EAX*), use it as an index in the *ia32_sys_call_table* array.
Arguments are passed to the handler in *registers* with the *following order*: *EBX*, *ECX*, *EDX*, *ESI*, *EDI*, *EBP*.
However, the first object as seen in the *idt_table* is the **X86_TRAP_DE**(*divide error*).
This can be seen from *GDB*, that the first gate within *idt_table* holds the *offset_high*, *offset_middle* and *offset_low* referencing *divide_error*. Which would deal with *division by 0* exceptions.
```
(gdb) p idt_table
$1 = 0xffffffff82598000 &lt;idt_table&gt;
(gdb) p/x *(idt_table + 0x10*0)
$2 = {offset_low = 0xb90, segment = 0x10,
      bits = {ist = 0x0, zero = 0, type = 14, dpl = 0, p = 1},
	  offset_middle = 0x8180, offset_high = 0xffffffff, reserved = 0x0}
(gdb) x/8i 0xffffffff81800b90
	0xffffffff81800b90 &lt;divide_error&gt;:		nopl	(%rax)
	0xffffffff81800b93 &lt;divide_error+3&gt;:	pushq	$0xffffffffffffffff
	0xffffffff81800b95 &lt;divide_error+5&gt;:	callq	0xffffffff81801210 &lt;error_entry&gt;
	0xffffffff81800b9a &lt;divide_error+10&gt;:	mov		%rsp,%rdi
	0xffffffff81800b9d &lt;divide_error+13&gt;:	xor		%esi,%esi
	0xffffffff81800b9f &lt;divide_error+15&gt;:	callq	0xffffffff81025d60 &lt;do_devide_error&gt;
	0xffffffff81800ba4 &lt;divide_error+20&gt;:	jmpq	0xffffffff81801310 &lt;error_exit&gt;
```
You can see that it&#39;s *DPL* is zero, that is, an *int $0x00* from a userland process wouldn&#39;t help reaching it(unlike *int $0x03*, *int $0x04* or *int $0x80*). *Gate descriptors* are initialized in *idt_setup_from_table* which calls *idt_init_desc*:
```c
idt_setup_from_table(gate_desc *idt, const struct idt_data *t, int size, bool sys)
{
	gate_desc desc;

	for (; size &gt; 0; t++, size--) {
		idt_init_desc(&amp;desc, t);
		write_idt_entry(idt, t-&gt;vector, &amp;desc);
		if (sys)
			set_bit(t-&gt;vector, system_vectors);
	}
}
```
And here it is.
```c
static inline void idt_init_desc(gate_desc *gate, const struct idt_data *d)
{
	unsigned long addr = (unsigned long) d-&gt;addr;

	gate-&gt;offset_low	= (u16) addr;
	gate-&gt;segment		= (u16) d-&gt;segment;
	gate-&gt;bits		= d-&gt;bits;
	gate-&gt;offset_middle	= (u16) (addr &gt;&gt; 16);
#ifdef CONFIG_X86_64
	gate-&gt;offset_high	= (u32) (addr &gt;&gt; 32);
	gate-&gt;reserved		= 0;
#endif
}
```
This could be used by the attacker, such as by getting the IDT address using the SIDT instruction, and looking for a specific handler in the list, incrementing *offset_high* would set it to *0*.

```bash
As we said above, we&#39;re going to use the IDT and overwrite one of its
entries (more precisely a Trap Gate, so that we&#39;re able to hijack an
exception handler and redirect the code-flow towards userspace).
Each IDT entry is 64-bit (8-bytes) long and we want to overflow the
&#39;base_offset&#39; value of it, to be able to modify the MSB of the exception
handler routine address and thus redirect it below PAGE_OFFSET
(0xc0000000) value.
```
~ [Phrack](http://phrack.org/issues/64/6.html)
##### KSPP:
This is a protection that appeared starting from *4.8*, it&#39;s name is a short for: *&quot;Kernel self-protection project&quot;*, It does provide additional checks on **copy_to_user()** and **copy_from_user()** to prevent classic buffer-overflows bugs from happening, by checking the saved compile-time buffer size and making sure it fits. if not, abort and prevent any possible exploitation from happening.
```bash
root@Nwwz:~/mod# cd /usr/src
root@Nwwz:/usr/src# cd linux-4.17.2
root@Nwwz:/usr/src/linux-4.17.2# cd include
root@Nwwz:/usr/src/linux-4.17.2/include# nano uaccess.h
```
We can directly see a check that&#39;s *likely to be 1*, before proceeding to the *copy operation*:
```c
static __always_inline unsigned long __must_check
copy_from_user(void *to, const void __user *from, unsigned long n)
{
	if (likely(check_copy_size(to, n, false)))
		n = _copy_from_user(to, from, n);
	return n;
}

static __always_inline unsigned long __must_check
copy_to_user(void __user *to, const void *from, unsigned long n)
{
	if (likely(check_copy_size(from, n, true)))
		n = _copy_to_user(to, from, n);
	return n;
}
```
The check function is as follows, it does first check the compile-time size against the requested size, and calls *__bad_copy_from()* or *__bad_copy_to()* depending on the boolean *is_source* if it seems like an overflow is possible, which is *unlikely* of course(*or not?*), it then returns false.
If not, it does call *check_object_size()* and returns true.
```c
extern void __compiletime_error(&quot;copy source size is too small&quot;)
__bad_copy_from(void);
extern void __compiletime_error(&quot;copy destination size is too small&quot;)
__bad_copy_to(void);

static inline void copy_overflow(int size, unsigned long count)
{
	WARN(1, &quot;Buffer overflow detected (%d &lt; %lu)!\n&quot;, size, count);
}

static __always_inline bool
check_copy_size(const void *addr, size_t bytes, bool is_source)
{
	int sz = __compiletime_object_size(addr);
	if (unlikely(sz &gt;= 0 &amp;&amp; sz &lt; bytes)) {
		if (!__builtin_constant_p(bytes))
			copy_overflow(sz, bytes);
		else if (is_source)
			__bad_copy_from();
		else
			__bad_copy_to();
		return false;
	}
	check_object_size(addr, bytes, is_source);
	return true;
}
```
This function is simply just a wrapper around *__check_object_size()*.
```c
#ifdef CONFIG_HARDENED_USERCOPY
extern void __check_object_size(const void *ptr, unsigned long n,
					bool to_user);

static __always_inline void check_object_size(const void *ptr, unsigned long n,
					      bool to_user)
{
	if (!__builtin_constant_p(n))
		__check_object_size(ptr, n, to_user);
}
#else
static inline void check_object_size(const void *ptr, unsigned long n,
				     bool to_user)
{ }
#endif
```
Additional checks are provided here in *__check_object_size()*, and as the comment says, not a *kernel .text* address, not a *bogus* address and is a safe *heap or stack object*.
```c
void __check_object_size(const void *ptr, unsigned long n, bool to_user)
{
	if (static_branch_unlikely(&amp;bypass_usercopy_checks))
		return;

	if (!n)
		return;

	check_bogus_address((const unsigned long)ptr, n, to_user);

	check_heap_object(ptr, n, to_user);

	switch (check_stack_object(ptr, n)) {
	case NOT_STACK:
		break;
	case GOOD_FRAME:
	case GOOD_STACK:
		return;
	default:
		usercopy_abort(&quot;process stack&quot;, NULL, to_user, 0, n);
	}

	check_kernel_text_object((const unsigned long)ptr, n, to_user);
}
EXPORT_SYMBOL(__check_object_size);
```
With this, it does provide enough to block and kill classic buffer-overflow bugs, this can be disabled by *commenting the check* and *recompiling a module*.
##### KASLR:
Stands for *Kernel Address Space Layout Randomization*.
It&#39;s similiar to the ASLR on *userspace* which protects the *stack* and *heap addresses* from being at the *same location* in two different runs(*unless the attacker gets lucky :stuck_out_tongue:*). PIE too since it does target the *main binary segments* which are *text*, *data* and *bss*.

This protection randomizes the kernel segments(*Exception table*, *text*, *data*..) at each *restart*(*boot*), we&#39;ve previously disabled it by using the *nokaslr* at the *kernel command line*.
In order to experiment on it, this was removed and specific symbols in */proc/kallsyms* were then fetched on two different runs.
First run:
![image|575x81](upload://hmY6rBts39KAyi3lwdELezIKy8w.png) 
Second run:
![image|574x82](upload://lA0OeuD5CE6XEqPtGRHGlLhto8u.png) 
This shows that addresses are randomly assigned on boottime to *_stext* and *_sdata*, whereas their end is just the *start address* plus a *size* which doesn&#39;t change in this case(*0x21dc0* for *.data*, *0x6184d1* for *.text*), note that *.data* is on a constant distance from *.text*.
So if the attacker gets the *.text base address*(*which is the result of a **leak***), he can know the location of all the *kernel symbols* even with *no access* to *kallsyms* using *RVAs*(*or offsets*), but he&#39;ll have to compile the *target kernel* in his box to get them.
This is for example used when *SMEP* is on and one has to go for *ROP* to disable it first, and then redirect execution to a shellcode placed in *userspace*(*&lt; TASK_SIZE*).

##### kptr_restrict:
This protection prevents *kernel addresses* from being exposed to the attacker. It does stop *%pK* format from dumping an address, and it&#39;s work depends on the *kptr_restrict* value(0, 1 or 2).
```
Kernel Pointers:

	%pK	0x01234567 or 0x0123456789abcdef

	For printing kernel pointers which should be hidden from unprivileged
	users. The behaviour of %pK depends on the kptr_restrict sysctl - see
	Documentation/sysctl/kernel.txt for more details.
```
This can be seen in *kprobe_blacklist_seq_show()* which performs a *check* with a call to *kallsyms_show_value()*, depending on it, it *would* or *would not* print the *start* and *end* addresses.
```c
static int kprobe_blacklist_seq_show(struct seq_file *m, void *v)
{
	struct kprobe_blacklist_entry *ent =
		list_entry(v, struct kprobe_blacklist_entry, list);

	if (!kallsyms_show_value())
		seq_printf(m, &quot;0x%px-0x%px\t%ps\n&quot;, NULL, NULL,
			   (void *)ent-&gt;start_addr);
	else
		seq_printf(m, &quot;0x%px-0x%px\t%ps\n&quot;, (void *)ent-&gt;start_addr,
			   (void *)ent-&gt;end_addr, (void *)ent-&gt;start_addr);
	return 0;
}
```
What *kallsyms_show_value()* does is shown here:
```c
int kallsyms_show_value(void)
{
	switch (kptr_restrict) {
	case 0:
		if (kallsyms_for_perf())
			return 1;
	case 1:
		if (has_capability_noaudit(current, CAP_SYSLOG))
			return 1;
	default:
		return 0;
	}
}
```
If *kptr_restrict* value is 0, it does call *kallsyms_for_perf()* to check if *sysctl_perf_event_paranoid* value is smaller or equal to 1, returns 1 if true.
If it&#39;s 1, it checks if *CAP_SYSLOG* is within the user&#39;s capabilities, if true, it returns 1.
Otherwise, it returns 0.

Disabling this protection can be done by setting */proc/sys/kernel/kptr_restrict* content to **0**.
Or using *sysctl* to do that:
```
sysctl -w kernel.kptr_restrict=0
```
But watchout for *perf_event_paranoid* too, if it&#39;s *&gt; 1*, then it needs to be adjusted.
This is an example on the default kernel run by my *Debian VM*:
```bash
user@Nwwz:~$ cd /proc/self
user@Nwwz:/proc/self$ cat stack
[&lt;ffffffff81e7c869&gt;] do_wait+0x1c9/0x240
[&lt;ffffffff81e7d9ab&gt;] SyS_wait4+0x7b/0xf0
[&lt;ffffffff81e7b550&gt;] task_stopped_code+0x50/0x50
[&lt;ffffffff81e03b7d&gt;] do_syscall_64+0x8d/0xf0
[&lt;ffffffff8241244e&gt;] entry_SYSCALL_64_after_swapgs+0x58/0xc6
[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff
```
However, in the *4.17* kernel, we get this, because of *perf_event_paranoid*:
```bash
root@Nwwz:~# cd /proc/self
root@Nwwz:/proc/self# cat stack
[&lt;0&gt;] do_wait+0x1c9/0x240
[&lt;0&gt;] kernel_wait4+0x8d/0x140
[&lt;0&gt;] __do_sys_wait4+0x95/0xa0
[&lt;0&gt;] do_syscall_64+0x55/0x100
[&lt;0&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9
[&lt;0&gt;] 0xffffffffffffffff
root@Nwwz:/proc/self# cat /proc/sys/kernel/kptr_restrict
0
root@Nwwz:/proc/self# cat /proc/sys/kernel/perf_event_paranoid
2
```
##### mmap_min_addr:
The *mm_struct* within *task_struct* holds an operation function called *get_unmapped_area*.
```c
struct mm_struct {
...
#ifdef CONFIG_MMU
		unsigned long (*get_unmapped_area) (struct file *filp,
				unsigned long addr, unsigned long len,
				unsigned long pgoff, unsigned long flags);
#endif
...
}
```
It is then extracted in *get_unmapped_area()*, which tries to get it from the *mm*(*mm_struct*), before checking it&#39;s *file* and it&#39;s *file_operations* or if it has the *MAP_SHARED* flag and assign *shmem_get_unmapped_area()* to it.
However, within the *mm_struct*, the default value of *get_unmapped_area* is the *arch specific* function.
This function does search for a *large enough memory block* to satisfy the *request*, but before returning the *addr*, it does check if it&#39;s **bigger or equal** to *mmap_min_addr*, which means that any address *below it* will **not be given**, this prevents *NULL pointer dereference* attack from happening(no mmaping *NULL address*, *nothing* will be stored there(*shellcode, pointers..*)).

Disabling this protection can be done by setting */proc/sys/vm/mmap_min_addr* content to **0**, or using *sysctl* like before.
```
sysctl -w vm.mmap_min_addr=0
```
##### addr_limit:
The **thread**(*thread_struct*) within the *task_struct* contains some important fields, amongst them, is the *addr_limit*.
```c
typedef struct {
	unsigned long		seg;
} mm_segment_t;

struct thread_struct {
	...
	mm_segment_t		addr_limit;

	unsigned int		sig_on_uaccess_err:1;
	unsigned int		uaccess_err:1;
	...
};
```
This can be read with a call to *get_fs()*, changed with *set_fs()*:
```c
#define MAKE_MM_SEG(s)	((mm_segment_t) { (s) })

#define KERNEL_DS	MAKE_MM_SEG(-1UL)
#define USER_DS 	MAKE_MM_SEG(TASK_SIZE_MAX)

#define get_ds()	(KERNEL_DS)
#define get_fs()	(current-&gt;thread.addr_limit)
static inline void set_fs(mm_segment_t fs)
{
	current-&gt;thread.addr_limit = fs;
	
	set_thread_flag(TIF_FSCHECK);
}
```
When *userspace* likes to reach an address, it is checked against this first, so overwritting it with -1UL(*KERNEL_DS*) would let you access(*read or write*) to *kernelspace*.

This was the introduction, I&#39;ve noticed that it has grown bigger than I expected, so I stopped, and removed parts about [protections](https://en.wikipedia.org/wiki/Kernel_page-table_isolation), [side-channel](https://meltdownattack.com/meltdown.pdf) [attacks](https://arxiv.org/pdf/1901.01161.pdf) and others.
 

Starting this was possible, thanks to: @_py(DA BEST), @pry0cc, @anon79434934, @4w1il, @ricksanchez and @Leeky.
See y&#39;all in *part 1*, peace.

&quot;*nothing is enough*, *search more to learn more*&quot;.
~ exploit</description>
    
    <lastBuildDate>Thu, 14 Feb 2019 17:44:30 +0000</lastBuildDate>
    <category>Exploit Development</category>
    <atom:link href="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>Point of no C3 | Linux Kernel Exploitation - Part 0</title>
        <dc:creator><![CDATA[system]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed after 30 days. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585/3">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585/3</link>
        <pubDate>Sat, 16 Mar 2019 17:42:39 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-11585-3</guid>
        <source url="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585.rss">Point of no C3 | Linux Kernel Exploitation - Part 0</source>
      </item>
      <item>
        <title>Point of no C3 | Linux Kernel Exploitation - Part 0</title>
        <dc:creator><![CDATA[f0ur]]></dc:creator>
        <description><![CDATA[
            <aside class="quote group-VIP" data-username="exploit" data-post="1" data-topic="11585">
<div class="title">
<div class="quote-controls"></div>
<img alt="" width="20" height="20" src="https://0x00sec.org/user_avatar/0x00sec.org/exploit/40/8365_2.png" class="avatar"> exploit:</div>
<blockquote>
<p><strong>HAH</strong> IRRITATED <strong>AHAHAHAHAHAHAHA</strong><br>
<em>“Appreciate the art, master the craft.”</em><br>
<strong>AHAHAHAH</strong> OUTDATED <strong>AHAHAHAHAH</strong></p>
</blockquote>
</aside>
<p>First four lines are hilarious^^^</p>
          <p><a href="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585/2</link>
        <pubDate>Thu, 14 Feb 2019 17:44:30 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-11585-2</guid>
        <source url="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585.rss">Point of no C3 | Linux Kernel Exploitation - Part 0</source>
      </item>
      <item>
        <title>Point of no C3 | Linux Kernel Exploitation - Part 0</title>
        <dc:creator><![CDATA[exploit]]></dc:creator>
        <description><![CDATA[
            <p>In the name of Allah, the most beneficent, the most merciful.</p>
<hr>
<p><strong>HAH</strong>IRRITATED<strong>AHAHAHAHAHAHAHA</strong><br>
<em>“Appreciate the art, master the craft.”</em><br>
<strong>AHAHAHAH</strong>OUTDATED<strong>AHAHAHAHAH</strong></p>
<p>It’s been more than a year, huh? but I’m back, with <em>“Point of no C3”</em>. It’s main focus will be <em>Kernel Exploitation</em>, but that won’t stop it from looking at other things.</p>
<p>Summary</p>
<ul>
<li>Chapter I: Environment setup:
<ul>
<li>Preparing the VM</li>
<li>Using KGDB to debug the kernel</li>
<li>Compiling a simple module</li>
<li>What?</li>
<li>Few structs</li>
<li>Debug a module</li>
</ul>
</li>
<li>Chapter II: Overview on security and General understanding:
<ul>
<li>Control Registers</li>
<li>SMAP</li>
<li>SMEP</li>
<li>Write-Protect</li>
<li>Paging(a bit of segmentation too)</li>
<li>Processes</li>
<li>Syscalls</li>
<li>IDT(Interrupt Descriptor Table)</li>
<li>KSPP</li>
<li>KASLR</li>
<li>kptr_restrict</li>
<li>mmap_min_addr</li>
<li>addr_limit</li>
</ul>
</li>
</ul>
<h4>Chapter I: <em>Environment setup</em>
</h4>
<p>“<em>No QEMU for you.</em>”</p>
<h5>Preparing the VM:</h5>
<p>To begin with, we would set up the environment and the VM’s in order to experiment on them.<br>
For this, <a href="https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/" rel="noopener nofollow ugc"><em>Debian</em></a> was choosen(<em>core only</em>).<br>
Other choices include <em>SUSE</em> or <em>Centos</em>, etc.</p>
<pre><code class="lang-perl">debian-9.4.0-amd64-netinst.iso			2018-03-10 12:56 291M [X]
debian-9.4.0-amd64-xfce-CD-1.iso		2018-03-10 12:57 646M
debian-mac-9.4.0-amd64-netinst.iso		2018-03-10 12:56 294M
</code></pre>
<p>A VM is then created with atleast <strong>35GB</strong> space.(<em>Hey, It’s for compiling the kernel!</em>)</p>
<pre><code class="lang-perl">Installer disc image file (iso):
[C:\vm\debian-9.4.0-amd64-netinst.iso	[▼]]
⚠ Could not detect which operating system is in this disc image.
  You will need to specify which operating system will be installed.
</code></pre>
<p>Once you boot it, you can proceed with <em>Graphical Install</em>, and since we only want the core, stop at <em>Software selection</em> and have only <strong>SSH server</strong> and <strong>standard system utilities</strong> selected.<br>
And when it’s done, you’ll have your first VM ready.</p>
<pre><code class="lang-bash">
Debian GNU/Linux 9 Nwwz tty1
Hint: Num Lock on

Nwwz login: root
Password:
Linux Nwwz 4.9.0-6-amd64 #1 SMP Debian 4.9.88-1+deb9u1 (2018-05-07) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@Nwwz:~#
</code></pre>
<p>In order to get the latest stable Linux kernel release(<em>4.17.2 at the time of writing</em>) and run it.<br>
We would start by installing necessary packages:</p>
<pre><code class="lang-bash">apt-get install git build-essential fakeroot ncurses* libssl-dev libelf-dev ccache gcc-multilib bison flex bc
</code></pre>
<p>Downloading the kernel tarball and the patch:</p>
<pre><code class="lang-bash">root@Nwwz:~# cd /usr/src
root@Nwwz:/usr/src# wget "https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/linux-4.17.2.tar.gz"
root@Nwwz:/usr/src# wget "https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/patch-4.17.2.gz"
</code></pre>
<p>Extracting them:</p>
<pre><code class="lang-bash">root@Nwwz:/usr/src# ls
linux-4.17.2.tar.gz patch-4.17.2.gz
root@Nwwz:/usr/src# gunzip patch-4.17.2.gz
root@Nwwz:/usr/src# gunzip linux-4.17.2.tar.gz
root@Nwwz:/usr/src# tar -xvf linux-4.17.2.tar
</code></pre>
<p>Moving and applying the patch:</p>
<pre><code class="lang-bash">root@Nwwz:/usr/src# ls
linux-4.17.2 linux-4.17.2.tar patch-4.17.2
root@Nwwz:/usr/src# mv patch-4.17.2 linux-4.17.2/
root@Nwwz:/usr/src# cd linux-4*2
root@Nwwz:/usr/src/linux-4.17.2# patch -p1 &lt; patch-4.17.2
</code></pre>
<p>Cleaning the directory and copying the original bootfile to the current working directory and changing the config with an <em>ncurses</em> menu:</p>
<pre><code class="lang-bash">root@Nwwz:/usr/src/linux-4.17.2# make mrproper
root@Nwwz:/usr/src/linux-4.17.2# make clean
root@Nwwz:/usr/src/linux-4.17.2# cp /boot/config-$(uname -r) .config
root@Nwwz:/usr/src/linux-4.17.2# make menuconfig
</code></pre>
<p>One must then set up the following fields:</p>
<pre><code class="lang-auto">[*] Networking support 	---&gt;
    Device Drivers		---&gt;
	Firmware Drivers	---&gt;
	File systems		---&gt;
[X] Kernel hacking		---&gt;
		printk and dmesg options					---&gt;
	[X] Compile-time checks and compiler options	---&gt;
		...
		[*] Compile the kernel with debug info
		...
	...
	-*- Kernel debugging
	...
	[*] KGDB: kernel debugger
</code></pre>
<pre><code class="lang-auto">	Do you wish to save your new configuration?
	Press &lt;ESC&gt;&lt;ESC&gt; to continue kernel configuration.
			[&lt; Yes &gt;]			&lt; No &gt;
</code></pre>
<p>Make sure you do have similiar lines on .config:</p>
<pre><code class="lang-bash">CONFIG_STRICT_KERNEL_RWX=n
CONFIG_DEBUG_INFO=y
CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR=n
CONFIG_HARDENED_USERCOPY=n
CONFIG_HARDENED_USERCOPY_FALLBACK=n
</code></pre>
<p>Before starting the compiling process, to faster the process, you can split the work to multiple jobs(<em>on different processors</em>). <strong>nproc</strong> would hand you the number of processing units available.</p>
<pre><code class="lang-bash">root@Nwwz:/usr/src/linux-4.17.2# nproc
4
root@Nwwz:/usr/src/linux-4.17.2# make -j4
</code></pre>
<p>It will then automatically go through <em>stage 1 &amp; 2</em>:</p>
<pre><code class="lang-bash">Setup is 17116 bytes (padded to 17408 bytes).
System is 4897 kB
CRC 2f571cf0
Kernel: arch/x86/boot/bzImage is ready	(#1)
	Building modules, stage 2.
	MODPOST	3330 modules
(SNIP)
	CC		virt/lib/irqbypass.mod.o
	LD [M]	virt/lib/irqbypass.ko
root@Nwwz:/usr/src/linux-4.17.2#
</code></pre>
<p>If somehow, there’s no <em>stage two</em>, a single command should be executed before moving on:<br>
(This normally isn’t required.)</p>
<pre><code class="lang-auto">make modules
</code></pre>
<p>Installing the modules:</p>
<pre><code class="lang-bash">root@Nwwz:/usr/src/linux-4.17.2# make modules_install
(SNIP)
	INSTALL	sound/usb/usx2y/snd-usb-usx2y.ko
	INSTALL	virt/lib/irqbypass.ko
	DEPMOD	4.17.0
root@Nwwz:/usr/src/linux-4.17.2#
</code></pre>
<p>Installing and preparing the kernel for boot:</p>
<pre><code class="lang-bash">root@Nwwz:/usr/src/linux-4.17.2# make install
(SNIP)
Found linux image: /boot/vmlinuz-4.17.0
Found initrd image: /boot/initrd.img-4.17.0
Found linux image: /boot/vmlinuz-4.9.0-6-amd64
Found initrd image: /boot/initrd.img-4.9.0-6-amd64
done
root@Nwwz:/usr/src/linux-4.17.2# cd /boot
root@Nwwz:/boot# mkinitramfs -o /boot/initrd.img-4.17.0 4.17.0
root@Nwwz:/boot# reboot
</code></pre>
<p>You can then choose the new kernel from the boot screen:</p>
<pre><code class="lang-auto">*Debian GNU/Linux, with Linux 4.17.0
 Debian GNU/Linux, with Linux 4.17.0 (recovery mode)
 Debian GNU/Linux, with Linux 4.9.0-6-amd64
 Debian GNU/Linux, with Linux 4.9.0-6-amd64 (recovery mode)
</code></pre>
<p>If it fails however, saying that it’s an out-of-memory problem, you can reduce the size of the boot image.</p>
<pre><code class="lang-bash">root@Nwwz:/boot# cd /lib/modules/4.17.0/
root@Nwwz:/lib/modules/4.17.0# find . -name *.ko -exec strip --strip-unneeded {} +
root@Nwwz:/lib/modules/4.17.0# cd /boot
root@Nwwz:/boot# mkinitramfs -o initrd.img-4.17.0 4.17.0
</code></pre>
<p>It’ll then boot successfully.</p>
<pre><code class="lang-auto">root@Nwwz:~# uname -r
4.17.0
</code></pre>
<h5>Using KGDB to debug the kernel:</h5>
<p>Installing <strong>ifconfig</strong> and running it would be the first thing to do:</p>
<pre><code class="lang-bash">root@Nwwz:~# apt-get install net-tools
(SNIP)
root@Nwwz:~# ifconfig
ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.150.145  netmask 255.255.255.0  broadcast 192.168.150.255
(SNIP)
</code></pre>
<p>Back to <em>Debian</em> machine, transfering <strong>vmlinux</strong> to the host is done with SCP or WinSCP in my case.</p>
<pre><code class="lang-auto">root@Nwwz:~# service ssh start
</code></pre>
<pre><code class="lang-auto">..							Répertoire parent
vmlinux			461 761 KB	Fichier
</code></pre>
<p>With this, you’ll have debug symbols ready, but you still need to enable KGDB for the target kernel.</p>
<pre><code class="lang-bash">root@Nwwz:~# cd /boot/grub
root@Nwwz:/boot/grub# nano grub.cfg
</code></pre>
<p>Editing a single line, adding <em>__setup</em> arguments, we would then be able to manipulate the kernel for our needs, such as <em>disabling KASLR</em> and <em>enabling KGDB</em>.<br>
Search for the first ‘<em>Debian GNU</em>’ occurence and make sure it’s the wanted kernel, and add the following to the line starting with [X]: <em>kgdboc=ttyS1,115200 kgdbwait nokaslr</em>.</p>
<pre><code class="lang-auto">menuentry 'Debian GNU/Linux' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-b1a66d11-d729-4f23-99b0-4ddfea0af6c5' {
	...
	echo	'Loading Linux 4.17.0 ...'
	[X] linux	/boot/vmlinuz-4.17.0 root=UUID=b1a66d11-d729-4f23-99b0-4ddfea0af6c5 ro quiet kgdboc=ttyS1,115200 kgdbwait nokaslr
	echo	'Loading initial ramdisk ...'
	initrd	/boot/initrd.img-4.17.0
}
</code></pre>
<p>In order to debug the running kernel, another VM similer to the one made previously(<em>Debian</em>) will be created(<em>Debian HOST</em>).<br>
Now shutdown both VMs in order to set the pipe:</p>
<ul>
<li>
<em>Debian</em>:<pre><code class="lang-auto">⦿ Use named pipe:
	*---------------------------------------*
	| \\.\pipe\com_2                        |
	*---------------------------------------*
	[This end is the server.             [▼]]
	[The other end is a virtual machine. [▼]]
---------------------------------------------7
I/O mode
⧆ Yield CPU on poll

	Allow the guest operating system to use this serial
	port in polled mode (as opposed to interrupt mode).
</code></pre>
</li>
<li>
<em>DebianHOST</em>:<pre><code class="lang-auto">⦿ Use named pipe:
	*---------------------------------------*
	| \\.\pipe\com_2                        |
	*---------------------------------------*
	[This end is the client.             [▼]]
	[The other end is a virtual machine. [▼]]
---------------------------------------------7
I/O mode
⧆ Yield CPU on poll

	Allow the guest operating system to use this serial
	port in polled mode (as opposed to interrupt mode).
</code></pre>
</li>
</ul>
<p>Getting the <strong>vmlinux</strong> image to <em>DebianHOST</em> after installing necessary packages:</p>
<pre><code class="lang-bash">root@Nwwz:~# apt-get install gcc gdb git net-tools
root@Nwwz:~# cd /home/user
root@Nwwz:/home/user# ls
vmlinux
root@Nwwz:/home/user# gdb vmlinux
GNU gdb (Debian 7.12-6) 7.12.0.20161007-git
(SNIP)
</code></pre>
<p>Turning the Debian back on would result in a similiar message:</p>
<pre><code class="lang-auto">KASLR disabled: 'nokaslr' on cmdline.
[	1.571915] KGDB: Waiting for connection from remote gdb...
</code></pre>
<p>Attaching to DebianHOST’s GDB is then possible:</p>
<pre><code class="lang-auto">(gdb) set serial baud 115200
(gdb) target remote /dev/ttyS1
Remote debugging using /dev/ttyS1
kgdb_breakpoint () at kernel/debug/debug_core.c:1073
1073		wmb(); /* Sync point after breakpoint */
(gdb) list
1068	noinline void kgdb_breakpoint(void)
1069	{
1070		atomic_inc(&amp;kgdb_setting_breakpoint);
1071		wmb(); /* Sync point before breakpoint */
1072		arch_kgdb_breakpoint();
1073		wmb(); /* Sync point after breakpoint */
1074		atomic_dec(&amp;kgdb_setting_breakpoint);
1075	}
1076	EXPORT_SYMBOL_GPL(kgdb_breakpoint);
1077
(gdb)
</code></pre>
<p>Know that by writing <em>‘continue’</em> on <em>GDB</em>, you wouldn’t be able to control it again unless you use the <em>magic SysRq key</em> to force a <em>SIGTRAP</em> to happen:</p>
<pre><code class="lang-bash">root@Nwwz:~# echo "g" &gt; /proc/sysrq-trigger
</code></pre>
<p>And you can see in <em>DebianHOST</em> that it works.</p>
<pre><code class="lang-bash">(SNIP)
[New Thread 459]
[New Thread 462]
[New Thread 463]
[New Thread 476]
[New Thread 485]
[New Thread 487]

Thread 56 received signal SIGTRAP, Trace/breakpoint trap.
[Switching to Thread 489]
kgdb_breakpoint () at kernel/debug/debug_core.c:1073
1073	wmb(); /* Sync point after breakpoint */
(gdb)
</code></pre>
<h5>Compiling a simple module:</h5>
<p>A simple Hello 0x00sec module would be created.<br>
We need to make a directory in root folder, and prepare two files:</p>
<pre><code class="lang-bash">root@Nwwz:~# mkdir mod
root@Nwwz:~# cd mod
root@Nwwz:~/mod/# nano hello.c
</code></pre>
<pre><code class="lang-auto">#include &lt;linux/init.h&gt;
#include &lt;linux/module.h&gt;

static void hello_exit(void){
	printk(KERN_INFO "Goodbye!\n");
}

static int hello_init(void){
	printk(KERN_INFO "Hello 0x00sec!\n");
	return 0;
}
MODULE_LICENSE("GPU");

module_init(hello_init);
module_exit(hello_exit);
</code></pre>
<pre><code class="lang-bash">root@Nwwz:~/mod/# nano Makefile
</code></pre>
<pre><code class="lang-auto">obj-m += hello.o
KDIR   = /lib/modules/$(shell uname -r)/build

all:
	make -C $(KDIR) M=$(PWD) modules
clean:
	rm -rf *.ko *.o *.mod.* *.symvers *.order
</code></pre>
<p>Then, one can start compiling using <em>‘make’</em> and insert/remove the module in kernel to trigger both init and exit handlers.</p>
<pre><code class="lang-auto">root@Nwwz:~/mod# make
make -c /lib/modules/4.17.0/build M=/root/mod modules
make[1]: Entering directory '/usr/src/linux-4.17.2'
	CC [M]	/root/mod/hello.o
	Building modules, stage 2.
	MODPOST 1 modules
	CC	/root/mod/hello.mod.o
	LD [M] /root/mod/hello.ko
make[1]: Leaving directory '/usr/src/linux-4.17.2'
root@Nwwz:~/mod# insmod hello.ko
root@Nwwz:~/mod# rmmod hello.ko
</code></pre>
<p>The messages would be by then saved in the <em>dmesg</em> circular buffer.</p>
<pre><code class="lang-bash">root@Nwwz:~/mod# dmesg | grep Hello
[ 6545.039487] Hello 0x00sec!
root@Nwwz:~/mod# dmesg | grep Good
[ 6574.452282] Goodbye!
</code></pre>
<p>To clean the current directory:</p>
<pre><code class="lang-bash">root@Nwwz:~/mod# make clean
</code></pre>
<h5>What?:</h5>
<p>The kernel doesn’t count on the <em>C library</em> we’ve been used to, because it’s judged useless for it.<br>
So instead, after the module is linked and loaded in kernel-space(<em>requires root privileges, duh</em>).<br>
It can use header files available in the <strong>kernel source tree</strong>, which offers a huge number of functions such as <em>printk()</em> which logs the message and sets it’s priority, <em>module_init()</em> and <em>module_exit()</em> to declare initialization and clean-up functions.<br>
And while application usually run with no chance of changing their variables by another thread. This<br>
certainly isn’t the case for <em>LKM</em>s, since what they offer could be used by multiple processes at a single time, which could lead(<em>if the data dealt with is sensible, aka in critical region</em>) to a <em>panic</em>, or worse(<em>better?</em>), a <strong>compromise</strong>.</p>
<h5>Few structs:</h5>
<p>The kernel implements multiple locks, only semaphores and spinlocks will likely be used here.<br>
When the <em>semaphore</em> is previously held, the thread will <em>sleep</em>, <em>waiting for the lock to be released</em> so he can <em>claim it</em>.<br>
That’s why it’s a <em>sleeping lock</em>, therefore, it’s only used in <em>process context</em>.</p>
<pre><code class="lang-auto">/* Please don't access any members of this structure directly */
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count;
	struct list_head	wait_list;
};
</code></pre>
<p>It can then be initialized with sema_init() or DEFINE_SEMAPHORE():</p>
<pre><code class="lang-auto">#define __SEMAPHORE_INITIALIZER(name, n)				\
{									\
	.lock		= __RAW_SPIN_LOCK_UNLOCKED((name).lock),	\
	.count		= n,						\
	.wait_list	= LIST_HEAD_INIT((name).wait_list),		\
}

static inline void sema_init(struct semaphore *sem, int val)
{
	static struct lock_class_key __key;
	*sem = (struct semaphore) __SEMAPHORE_INITIALIZER(*sem, val);
	lockdep_init_map(&amp;sem-&gt;lock.dep_map, "semaphore-&gt;lock", &amp;__key, 0);
}
</code></pre>
<p>With <em>val</em> being the <em>much processes</em> that can <em>hold the lock</em> at once.<br>
It’s normally set to <em>1</em>, and a semaphore with a count of <em>1</em> is called a <em>mutex</em>.<br>
Another type of locks would be <em>spinlocks</em>, it keeps the <em>thread spinning instead of sleeping</em>, for that reason, it can be used in the <em>interrupt context</em>.</p>
<pre><code class="lang-auto">typedef struct spinlock {
	union {
		struct raw_spinlock rlock;

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
		struct {
			u8 __padding[LOCK_PADSIZE];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;


#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
	{					\
	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
	SPIN_DEBUG_INIT(lockname)		\
	SPIN_DEP_MAP_INIT(lockname) }

#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)

# define raw_spin_lock_init(lock)				\
	do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)
#endif

static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &amp;lock-&gt;rlock;
}

#define spin_lock_init(_lock)				\
do {							\
	spinlock_check(_lock);				\
	raw_spin_lock_init(&amp;(_lock)-&gt;rlock);		\
} while (0)
</code></pre>
<p>Enough with locks, what about <em>file_operations</em>?<br>
This struct holds the <em>possible operations</em> that can be called on a <em>device/file/entry</em>.<br>
When creating a <em>character device</em> by directly calling <em>cdev_alloc()</em> or <em>misc_register()</em>, it has to be provided along with the <em>major</em>(<em>on first function only</em>) and <em>minor</em>.<br>
It is defined as follows:</p>
<pre><code class="lang-auto">struct file_operations {
	struct module *owner;
	loff_t (*llseek) (struct file *, loff_t, int);
	ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
	ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
	...
} __randomize_layout;
</code></pre>
<p>There are similiar structs too, such as <em>inode_operations</em>, <em>block_device_operations</em> and <em>tty_operations</em>…<br>
But they all provide <em>handlers</em> to <em>userspace function</em> if the file/inode/blockdev/tty is the target.<br>
These are sometimes used by the attacker in order to <em>redirect execution</em> such as <em>perf_fops</em> or <em>ptmx_fops</em>.</p>
<p>The kernel provides some structs for lists with different <em>search times</em>.<br>
The first being <em>double linked-list</em>, <em>list_head</em>, it’s definition is simple, pointing to the <em>next</em> and <em>previous</em> <em>list_head</em>.</p>
<pre><code class="lang-auto">struct list_head {
	struct list_head *next, *prev;
};
</code></pre>
<p>While the second is redblack tree, <em>rb_node</em>, provides better <em>search time</em>.</p>
<pre><code class="lang-auto">struct rb_node {
	unsigned long  __rb_parent_color;
	struct rb_node *rb_right;
	struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));
</code></pre>
<p>It can be used to find the <em>target value</em> faster, if it’s <em>bigger</em> than the <em>first node</em>(<em>head</em>), then go <em>right</em>, else, go <em>left</em>.<br>
Function <em>container_of()</em> can then be used to extract the <em>container struct</em>.<br>
Note: Each device, can have multiple <em>minors</em>, but it’ll necessarily have a single <em>major</em>.</p>
<pre><code class="lang-bash">root@Nwwz:/# cd /dev
root@Nwwz:/dev# ls -l
total 0
crw------- 1 root root    [10], 175 Feb  9 09:24 agpgart
                            |
                            *-&gt; Same major, different minors.
                            |							
crw-r--r-- 1 root root    [10], 235 Feb  9 09:24 autofs
drwxr-xr-x 2 root root         160 Feb  9 09:24 block
drwxr-xr-x 2 root root          80 Feb  9 09:24 bsg
(SNIP)
[c]rw-rw-rw- 1 root tty      [5], [2] Feb  9 12:06 ptmx
|                             |    |
|                             |    *--&gt; Minor
*---&gt; Character Device        *---&gt; Major
(SNIP)
[b]rw-rw---- 1 root cdrom    [11], [0] Feb  9 09:24 sr0
|                             |    |
|                             |    *--&gt; Minor
*---&gt; Block Device            *---&gt; Major
(SNIP)
</code></pre>
<h5>Debug a module:</h5>
<p>When we started <em>gdb</em>, the only image it was aware of, is the <em>vmlinux</em> one.<br>
It doesn’t know about the <em>loaded module</em>, and doesn’t know about the <em>load location</em>.<br>
In order to provide these things and make debugging the module possible, one has to first transfer<br>
the target module to <em>DebianHOST</em>.</p>
<pre><code class="lang-bash">root@Nwwz:~/mod# service ssh start
</code></pre>
<p>Once that’s done, one should find different sections and addresses of the LKM in memory:</p>
<pre><code class="lang-bash">root@Nwwz:~/mod# insmod simple.ko
root@Nwwz:~/mod# cd /sys/module/simple/sections
root@Nwwz:/sys/module/simple/sections# ls -la
total 0
drwxr-xr-x 2 root root	   0 Aug 11 06:30 .
drwxr-xr-x 5 root root	   0 Aug  2 17:55 ..
-r-------- 1 root root	4096 Aug 11 06:31 .bss
-r-------- 1 root root	4096 Aug 11 06:31 .data
-r-------- 1 root root	4096 Aug 11 06:31 .gnu.linkonce.this_module
-r-------- 1 root root	4096 Aug 11 06:31 __mcount_loc
-r-------- 1 root root	4096 Aug 11 06:31 .note.gnu.build-id
-r-------- 1 root root	4096 Aug 11 06:31 .orc_unwind
-r-------- 1 root root	4096 Aug 11 06:31 .orc_unwind_ip
-r-------- 1 root root	4096 Aug 11 06:31 .rodata.str1.1
-r-------- 1 root root	4096 Aug 11 06:31 .rodata.str1.8
-r-------- 1 root root	4096 Aug 11 06:31 .strtab
-r-------- 1 root root	4096 Aug 11 06:31 .symtab
-r-------- 1 root root	4096 Aug 11 06:31 .text
root@Nwwz:/sys/module/simple/sections# cat .text
0xffffffffc054c000
root@Nwwz:/sys/module/simple/sections# cat .data
0xffffffffc054e000
root@Nwwz:/sys/module/simple/sections# cat .bss
0xffffffffc054e4c0
</code></pre>
<p>Back to <em>DebianHOST</em> and in gdb:</p>
<pre><code class="lang-python">(gdb) add-symbol-file simple.ko 0xffffffffc054c000 -s .data 0xffffffffc054e000 -s .bss 0xffffffffc054e4c0
</code></pre>
<p>And that’s it.</p>
<h4>Chapter II: <em>Overview on security and General understanding</em>
</h4>
<p>“<em>Uuuuh, it’s simple?</em>”</p>
<h5>Control Registers:</h5>
<p>CRs are <em>special registers</em>, being <em>invisible</em> to the <em>user</em>, they hold <em>important</em> information on the current <em>CPU</em> and the <em>process</em> running on it.<br>
<strong>x86_32 and x86_64</strong>:<br>
Keep in mind that their <em>sizes</em> are <em>different</em>(<em>64bit for x86_64, 32bit for x86_32</em>).<br>
<em>CR0</em>:</p>
<pre><code class="lang-bash">x32 and x64:
#0:     PE(Protected Mode Enable)
#1:     MP(Monitor co-processor)
#2:     EM(Emulation)
#3:     TS(Task Switched)
#4:     ET(Extension Type)
#5:     NE(Numeric Error)
#6-15:  Reserved
#16:    WP(Write Protect)
#17:    Reserved
#18:    AM(Alignment Mask)
#19-28: Reserved
#29:    NW(Not-Write Through)
#30:    CD(Cache Disable)
#31:    PG(Paging)
x64 only:
#32-61: Reserved
</code></pre>
<p><em>CR2</em>:<br>
Solely containing the <em>PFLA</em>(<em>Page Fault Linear Address</em>) address, which would later be extracted using <em>do_page_fault</em> function and passed to <em>__do_page_fault</em> to handle it.</p>
<pre><code class="lang-auto">dotraplinkage void notrace
do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
	unsigned long address = read_cr2(); /* Get the faulting address */
	enum ctx_state prev_state;

	prev_state = exception_enter();
	if (trace_pagefault_enabled())
		trace_page_fault_entries(address, regs, error_code);

	__do_page_fault(regs, error_code, address);
	exception_exit(prev_state);
}
NOKPROBE_SYMBOL(do_page_fault);
</code></pre>
<p><em>CR3</em>:<br>
This register contains the physical address of the <em>current process</em> PGD(<em>Page Global Directory</em>), which(<em>once converted back to virtual address</em>) would link to the next <em>level</em>(<em>P4D on five-level page tables</em> or <em>PUD on four-level page tables</em>), but in the end, it’s all to find the same struct, that is, <em>struct page</em>.</p>
<pre><code class="lang-auto">static inline unsigned long read_cr3_pa(void)
{
	return __read_cr3() &amp; CR3_ADDR_MASK;
}

static inline unsigned long native_read_cr3_pa(void)
{
	return __native_read_cr3() &amp; CR3_ADDR_MASK;
}

static inline void load_cr3(pgd_t *pgdir)
{
	write_cr3(__sme_pa(pgdir));
}
</code></pre>
<p>This is called as an example when an Oops happens, and the kernel calls <em>dump_pagetable()</em>.<br>
<em>CR4</em>:</p>
<pre><code class="lang-bash">x32 and x64:
#0:     VME(Virtual-8086 Mode Extensions)
#1:     PVI(Protected Mode Virtual Interrupts)
#2:     TSD(Time Stamp Disable)
#3:     DE(Debugging Extensions)
#4:     PSE(Page Size Extensions)
#5:     PAE(Physical Address Extensions)
#6:     MCE(Machine Check Enable)
#7:     PGE(Page Global Enable)
#8:     PCE(Performance-Monitoring Counter Enable)
#9:     OSFXSR(OS Support for FXSAVE and FXRSTOR Instructions)
#10:    OSXMMEXCPT(OS Support for Unmasked SIMD Floating Point Exceptions)
#11:    UMIP(User-Mode Instruction Prevention)
#12:    Reserved
#13:    VMXE(Virtual Machine Extensions Enable)
#14:    SMXE(Safer Mode Extensions Enable)
#15-16: Reserved
#17:    PCIDE(PCID Enable)
#18:    OSXSAVE(XSAVE and Processor Extended States Enable)
#19:    Reserved
#20:    SMEP(Supervisor Mode Execution Prevention)
#21:    SMAP(Supervisor Mode Access Prevention)
#22-31: Reserved
x64 only:
#31-63: Reserved
</code></pre>
<p><em>CR1</em> and <em>CR5</em> to <em>CR7</em>:<br>
Marked as <em>reserved</em>, accessing them would result in raising the <em>Undefined Behavior</em>(<strong><span class="hashtag">#UD</span></strong>) exception.<br>
<strong>x86_64 only</strong>:<br>
<em>CR8</em>:<br>
Only the first 4 bits are used in this one, while the other 60 bits are <em>reserved(0)</em>.<br>
Also called <em>TPR</em>(<em>Task Priority Register</em>). Those 4 bits are used when servicing interrupts, checking if the task should really be interrupted. It may or may not, depending on the interrupt’s priority: (<em>IP &lt;= TP</em> ? <em>PASS</em>:<strong>SERVICE</strong>).</p>
<p>They differ from architecture to another, while the previous example reviewed two CISC(<em>x86_32</em>, <em>x86_64</em>). <em>Windows</em> itself does have much similiarities at this level:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/3/37a721645c4241ca5904b248c8286667964e4790.png" alt="image" data-base62-sha1="7WkkQivFFlX8xG0EJLmv4WqrKjS" width="690" height="402"><br>
The thing is a little bit more different in RISC(<em>ARM</em> for this example):<br>
Instead of <em>Control Registers</em>, they are named <em>Coprocessors</em>(<em>P0</em> to <em>P15</em>), each <em>Coprocessor</em> holds <em>16</em> registers(<em>C0</em> to <em>C15</em>). Note however, that only <em>CP14</em> and <em>CP15</em> are very important to the <em>system</em>.<br>
<em>MCR</em> and <em>MRC</em> Instructions are available to deal with data transfer(<em>read/write</em>).<br>
An example for the <em>TTBR</em>(<em>Translation Table Base Register</em>) is as follows:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/8/89757594c8563a79137d4348a67bb1e3314a8615.png" alt="image" data-base62-sha1="jC14gDx9ZZjKpaK1oDzZksdxIrj" width="690" height="29"></p>
<h5>SMAP:</h5>
<p>Stands for <em>Supervisor Mode Access Prevention</em>, as it’s name suggests, prevents access to user-space from a more <em>privileged context</em>, that is, <em>ring zero</em>. However, since access may still be necessary in certain occasions, a flag is dedicated(<em>AC in EFLAGS</em>) to this purpose, along with two instructions to <em>set</em> or <em>clear</em> it:<br>
<em>CLAC</em>:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/d/df2e144659b9f0c2b3382f8c7759087babd5d221.png" alt="image" data-base62-sha1="vQlcpknsX4fWsGJk9ji6eNFA21b" width="690" height="83"><br>
<em>STAC</em>:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/7/7ece672cd78f62bfc62465ad510be6e73c16696a.png" alt="image" data-base62-sha1="i5MnpYkZnj3JOkcaRNCAL5HCOFk" width="690" height="86"></p>
<pre><code class="lang-auto">static __init int setup_disable_smap(char *arg)
{
	setup_clear_cpu_cap(X86_FEATURE_SMAP);
	return 1;
}
__setup("nosmap", setup_disable_smap);
</code></pre>
<p>It can be disabled with <em>nosmap</em> boot flag, which would clear the CPU’s <em>SMAP</em> capability, or by unsetting the <em>SMAP bit</em>(<em><span class="hashtag">#21</span></em>) on <em>CR4</em>.</p>
<h5>SMEP:</h5>
<p>An abbreviation for <em>Supervisor Mode Execution Prevention</em>, when running on <em>ring zero</em>, execution would not be allowed to be transmitted to <em>user-space</em>. So both <em>SMEP</em> and <em>SMAP</em> put a form of limitation on the <em>attacker’s</em> surface.</p>
<pre><code class="lang-auto">static __init int setup_disable_smep(char *arg)
{
	setup_clear_cpu_cap(X86_FEATURE_SMEP);
	
	check_mpx_erratum(&amp;boot_cpu_data);
	return 1;
}
__setup("nosmep", setup_disable_smep);
</code></pre>
<p>Knowing if it’s on is as simple as checking <em>/proc/cpuinfo</em>, and it’s the same for <em>SMAP</em>.<br>
This protection can be disabled with <em>nosmep</em> boot flag, it can also be disabled during runtime by unsetting <em>SMEP bit</em>(<em><span class="hashtag">#20</span></em>) on <em>CR4</em>.</p>
<h5>Write-Protect:</h5>
<p>Since code executing at the highest level of privilege should normally be capable of writting to all pages even those marked as <em>RO</em>(<em>Read Only</em>). However, a bit in <em>CR0</em>(<em>WP bit(16th)</em>) is supposed to stop that from happening, by providing additional checks.</p>
<h5>Paging(a bit of segmentation too):</h5>
<p>Linux does separate privileges. the processor can handle up to 4 different rings, starting from <strong>0</strong> which obviously is the <em>most privileged</em> and ending with <strong>3</strong> being the <em>least privileged</em> with limited access to <em>system resources</em>. However, most operating systems do work with only two rings, zero(also called <em>kernel-space</em>) and three(or <em>user-space</em>).<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/d/ddf4ff854f521b78e5c9f9b17c6dcd9defaa88a5.png" alt="image" data-base62-sha1="vFwqtwYnSRFoW3nUoUrVt2NT1UV" width="353" height="215"><br>
Each running process does have a <strong>struct mm_struct</strong> which fully describes it’s virtual memory space.<br>
But when it comes to <em>segmentation</em> and <em>paging</em>, we’re only interested in few objects in this struct: <em>context</em>, the single-linked list <em>mmap</em> and <em>pgd</em>.</p>
<pre><code class="lang-auto">typedef struct {

	u64 ctx_id;

	atomic64_t tlb_gen;

#ifdef CONFIG_MODIFY_LDT_SYSCALL
	struct rw_semaphore	ldt_usr_sem;
	struct ldt_struct	*ldt;
#endif

#ifdef CONFIG_X86_64
	unsigned short ia32_compat;
#endif

	struct mutex lock;
	void __user *vdso;
	const struct vdso_image *vdso_image;

	atomic_t perf_rdpmc_allowed;
#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS

	u16 pkey_allocation_map;
	s16 execute_only_pkey;
#endif
#ifdef CONFIG_X86_INTEL_MPX
	void __user *bd_addr;
#endif
} mm_context_t;
</code></pre>
<p>This struct holds many information on the context, including the <em>Local descriptor table(LDT)</em>, the VDSO image and base address(<em>residing in user-space <strong>__user</strong></em>), a <em>read/write semaphore</em> and a <em>mutual exclusion lock</em>(<em>it’s a semaphore too, remember?</em>).</p>
<pre><code class="lang-auto">struct ldt_struct {

	struct desc_struct	*entries;
	unsigned int		nr_entries;

	int			slot;
};
</code></pre>
<p>The first element in the LDT is a <em>desc_struct</em> pointer, referencing an array of entries, <em>nr_entries</em> of them.<br>
However, know that <em>LDT</em> isn’t usually set up, it would only use the <em>Global Descriptor Table</em>, it’s enough for <em>most</em> processes.</p>
<pre><code class="lang-auto">DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
#ifdef CONFIG_X86_64
	[GDT_ENTRY_KERNEL32_CS]		= GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc093, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER32_CS]	= GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),
#else
	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xc09a, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff),
	[GDT_ENTRY_PNPBIOS_CS32]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
	[GDT_ENTRY_PNPBIOS_CS16]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
	[GDT_ENTRY_PNPBIOS_DS]		= GDT_ENTRY_INIT(0x0092, 0, 0xffff),
	[GDT_ENTRY_PNPBIOS_TS1]		= GDT_ENTRY_INIT(0x0092, 0, 0),
	[GDT_ENTRY_PNPBIOS_TS2]		= GDT_ENTRY_INIT(0x0092, 0, 0),
	[GDT_ENTRY_APMBIOS_BASE]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
	[GDT_ENTRY_APMBIOS_BASE+1]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
	[GDT_ENTRY_APMBIOS_BASE+2]	= GDT_ENTRY_INIT(0x4092, 0, 0xffff),

	[GDT_ENTRY_ESPFIX_SS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
	[GDT_ENTRY_PERCPU]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
	GDT_STACK_CANARY_INIT
#endif
} };
EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
</code></pre>
<p>A per-cpu variable <em>gdt_page</em> is initialized using the <em>GDT_ENTRY_INIT</em> macro.</p>
<pre><code class="lang-auto">#define GDT_ENTRY_INIT(flags, base, limit)			\
	{							\
		.limit0		= (u16) (limit),		\
		.limit1		= ((limit) &gt;&gt; 16) &amp; 0x0F,	\
		.base0		= (u16) (base),			\
		.base1		= ((base) &gt;&gt; 16) &amp; 0xFF,	\
		.base2		= ((base) &gt;&gt; 24) &amp; 0xFF,	\
		.type		= (flags &amp; 0x0f),		\
		.s		= (flags &gt;&gt; 4) &amp; 0x01,		\
		.dpl		= (flags &gt;&gt; 5) &amp; 0x03,		\
		.p		= (flags &gt;&gt; 7) &amp; 0x01,		\
		.avl		= (flags &gt;&gt; 12) &amp; 0x01,		\
		.l		= (flags &gt;&gt; 13) &amp; 0x01,		\
		.d		= (flags &gt;&gt; 14) &amp; 0x01,		\
		.g		= (flags &gt;&gt; 15) &amp; 0x01,		\
	}
</code></pre>
<p>This macro simply takes <em>three arguments</em>, and <em>splits them</em> in order to <em>store</em> at each <em>field</em> a <em>valid value</em>.<br>
The <em>GDT</em> holds more entries on <em>32bit</em> than on <em>64bit</em>.</p>
<pre><code class="lang-auto">struct gdt_page {
	struct desc_struct gdt[GDT_ENTRIES];
} __attribute__((aligned(PAGE_SIZE)));
</code></pre>
<p>Says that <em>gdt_page</em> is an array of <em>GDT_ENTRIES</em>(<em>32</em> on <em>x86_32</em>, <em>16</em> on <em>x86_64</em>) much of <em>desc_struct</em> aligned to <em>PAGE_SIZE</em>(<em>usually 4KB(4096)</em>).</p>
<pre><code class="lang-auto">struct desc_struct {
	u16	limit0;
	u16	base0;
	u16	base1: 8, type: 4, s: 1, dpl: 2, p: 1;
	u16	limit1: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8;
} __attribute__((packed));
</code></pre>
<p>When an ELF is about to run, and is being loaded with <em>load_elf_binary()</em>, it does call <em>setup_new_exec()</em>, <em>install_exec_creds()</em> on <em>bprm</em> before it calls <em>setup_arg_pages()</em> which would pick a <em>random stack pointer</em>.<br>
Before returning <em>successfully</em>, it would call <em>finalize_exec()</em> and <em>start_thread()</em> which would <em>update the stack’s rlimit</em> and <em>begin execution</em> respectively:</p>
<pre><code class="lang-auto">void
start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
{
	start_thread_common(regs, new_ip, new_sp,
			    __USER_CS, __USER_DS, 0);
}
EXPORT_SYMBOL_GPL(start_thread);
</code></pre>
<p>As you are able to see, this function is just a wrapper around <em>start_thread_common()</em>:</p>
<pre><code class="lang-auto">static void
start_thread_common(struct pt_regs *regs, unsigned long new_ip,
		    unsigned long new_sp,
		    unsigned int _cs, unsigned int _ss, unsigned int _ds)
{
	WARN_ON_ONCE(regs != current_pt_regs());

	if (static_cpu_has(X86_BUG_NULL_SEG)) {

		loadsegment(fs, __USER_DS);
		load_gs_index(__USER_DS);
	}

	loadsegment(fs, 0);
	loadsegment(es, _ds);
	loadsegment(ds, _ds);
	load_gs_index(0);

	regs-&gt;ip		= new_ip;
	regs-&gt;sp		= new_sp;
	regs-&gt;cs		= _cs;
	regs-&gt;ss		= _ss;
	regs-&gt;flags		= X86_EFLAGS_IF;
	force_iret();
}
</code></pre>
<p>As a conclusion, every process starts with default segment registers, but different GPRs, stack and instruction pointer, and by looking at <strong>__USER_DS</strong> and <strong>__USER_CS</strong>:</p>
<pre><code class="lang-auto">#define GDT_ENTRY_DEFAULT_USER_DS	5
#define GDT_ENTRY_DEFAULT_USER_CS	6

#define __USER_DS			(GDT_ENTRY_DEFAULT_USER_DS*8 + 3)
#define __USER_CS			(GDT_ENTRY_DEFAULT_USER_CS*8 + 3)
</code></pre>
<p>We would find the segment registers and their values on <em>user-space</em>:</p>
<pre><code class="lang-auto">Initial state:
CS = 6*8+3 = 0x33
SS = 5*8+3 = 0x2b
DS = FS = ES = 0
</code></pre>
<p>These values can be checked using <em>GDB</em> and a <em>dummy binary</em>.</p>
<pre><code class="lang-bash">(gdb) b* main
Breakpoint 1 at 0x6b0
(gdb) r
Starting program: /root/mod/cs

Breakpoint 1, 0x00005555555546b0 in main ()
(gdb) info reg cs ss
cs		0x33	51
ss		0x2b	43
</code></pre>
<p>Also, you should know that, CS holds in it’s least 2 significant bits, the <em>Current Privilege Level(CPL)</em>, other segment selectors hold the <em>Requested Privilege Level(RPL)</em> instead of <em>CPL</em>.</p>
<pre><code class="lang-bash">(gdb) p/t $cs
$1 = 110011
(gdb) p/x $cs &amp; 0b11
$2 = 0x3
# (Privilege Level: User(3) SuperUser(0))
(gdb) p/d $cs &amp; ~0b1111
$3 = 48
# (Table Offset: 48)
(gdb) p/d $cs &amp; 0b100
$4 = 0
# (Table Indicator: GDT(0) LDT(1))
</code></pre>
<p><strong>3</strong> stands for the <em>third ring</em>, <em>least privileged</em>, that is, <em>user-space</em>.<br>
It doesn’t change, unless the execution is in <em>kernel-space</em>, so it’s similiar for both <em>root</em> and <em>any normal user</em>. So both <em>RPL</em> and <em>CPL</em> could be considered a form of limitation when accessing segments with lower(<em>more privileged</em>) DPL(<em>Descriptor Privilege Level</em>).</p>
<p>When it comes to <em>paging</em>, it’s <em>equivalent bit</em> in <em>CR0</em>(<em><span class="hashtag">#31</span></em>) is only set when the system is running in <em>protected mode</em>(<em>PE bit</em> in <em>CR0</em> is set), because in <em>real mode</em>, <strong>virtual address are equal to physical ones</strong>.<br>
Linux moved from four-level page tables to support five-level page tables by adding an additional layer(<em>P4D</em>), so the levels now are: <em>PGD</em> <em>P4D</em> <em>PUD</em> <em>PMD</em> <em>PTE</em>.<br>
<em>PGD</em> is the first level <em>Page Global Directory</em>, it is a pointer of type <em>pgd_t</em>, and it’s definition is:</p>
<pre><code class="lang-auto">typedef struct { pgdval_t pgd; } pgd_t;
</code></pre>
<p>It holds a <em>pgdval_t</em> inside, which is an unsigned long(<em>8 bytes on x86_64, 4 on x86_32</em>):</p>
<pre><code class="lang-auto">typedef unsigned long	pgdval_t;
</code></pre>
<p>To get to the next level, <em>pagetable_l5_enabled()</em> is called to check if the CPU has <em>X86_FEATURE_LA57</em> enabled.</p>
<pre><code class="lang-auto">#define pgtable_l5_enabled() cpu_feature_enabled(X86_FEATURE_LA57)
</code></pre>
<p>This can be seen in <em>p4d_offset()</em>:</p>
<pre><code class="lang-auto">static inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)
{
	if (!pgtable_l5_enabled())
		return (p4d_t *)pgd;
	return (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);
}
</code></pre>
<p>If it isn’t enabled, it simply casts the <em>pgd_t *</em> as <em>p4d_t *</em> and returns it, otherwise it returns the <em>P4D entry</em> within the <em>PGD</em> that links to the <em>specific address</em>.<br>
Then <em>P4D</em> itself can be used to find the next level, which is <em>PUD</em> of type <em>pud_t *</em>, <em>PUD</em> links to <em>PMD</em>(<em>Page Middle Directory</em>) and <em>PMD</em> to the <em>PTE</em>(<em>Page Table Entry</em>) which is the last level, and contains the <em>physical address</em> of the <em>page</em> with some <em>protection flags</em> and is of type <em>pte_t *</em>.</p>
<p>Each <em>process</em> has it’s own <em>virtual space</em>(<em>mm_struct</em>, <em>vm_area_struct</em> and <em>pgd_t</em>).</p>
<pre><code class="lang-auto">struct vm_area_struct {

	unsigned long vm_start;
	unsigned long vm_end;

	struct vm_area_struct *vm_next, *vm_prev;

	struct rb_node vm_rb;


	unsigned long rb_subtree_gap;


	struct mm_struct *vm_mm;
	pgprot_t vm_page_prot;
	unsigned long vm_flags;	

	struct {
		struct rb_node rb;
		unsigned long rb_subtree_last;
	} shared;

	struct list_head anon_vma_chain;
	struct anon_vma *anon_vma;

	const struct vm_operations_struct *vm_ops;

	unsigned long vm_pgoff;
	struct file * vm_file;
	void * vm_private_data;

	atomic_long_t swap_readahead_info;
#ifndef CONFIG_MMU
	struct vm_region *vm_region;
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *vm_policy;
#endif
	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
} __randomize_layout;
</code></pre>
<pre><code class="lang-auto">typedef struct { pgdval_t pgd; } pgd_t;
</code></pre>
<p>So creating a new process would be very expensive on performance. <em>Copy-on-Write</em>(<strong>COW</strong>) comes in helpful here, by making a clone out of the parent process and only copying when a write happens to the previously marked <em>read-only</em> pages.<br>
This happens on fork and more specifically in <em>copy_process()</em>, which duplicates the <em>task_struct</em> and does specific operations depending on flags passed to <em>clone()</em>, before copying all parent information which includes <em>credentials, filesystem, files, namespaces, IO, Thread Local Storage, signal, address space</em>.<br>
As an example, this walks <em>VMAs</em> in search of a user specified address, once found, it gets its <em>Physical address</em> and <em>Flags</em> by walking <em>page tables</em>.</p>
<pre><code class="lang-auto">#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/proc_fs.h&gt;
#include &lt;linux/sched.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;asm/pgtable.h&gt;
#include &lt;linux/highmem.h&gt;
#include &lt;linux/slab.h&gt;

#define device_name "useless"
#define SET_ADDRESS 0x00112233

char *us_buf;
unsigned long address = 0;

long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	switch(cmd){
		case SET_ADDRESS:
			address = arg;
			return 0;
		default:
			return -EINVAL;
	}
}

ssize_t do_read(struct file *filp, char *buf, size_t count, loff_t *offp){
	int res, phys, flags;
	struct vm_area_struct *cmap;
	pgd_t *pgd;
	p4d_t *p4d;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep;

	/* Find corresponding VMA */
	cmap    = current-&gt;mm-&gt;mmap;

	while(1){
		if(cmap-&gt;vm_start &lt;= address &amp;&amp; address &lt; cmap-&gt;vm_end){
			break;
		}
		
		cmap     = cmap-&gt;vm_next;
		if(cmap  == NULL){
			return -1;
		}
	};
	
	/* Walking Page-tables for fun */
	pgd     = pgd_offset(current-&gt;mm, address);
	p4d     = p4d_offset(pgd,         address);
	pud     = pud_offset(p4d,         address);
	pmd     = pmd_offset(pud,         address);
	ptep    = pte_offset_kernel(pmd,  address);
	phys    = *((int *) ptep);
	flags   = phys &amp; 0xfff;
	phys   &amp;= ~0xfff;
	
	snprintf(us_buf, 64, "PhysAddr(%x) VMAStart(%lx) Flags(%x)", phys, cmap-&gt;vm_start, flags);

	if(count &gt; 64)
		count = 64;
	res = copy_to_user(buf, us_buf, count);
	return res;
}

struct file_operations fileops = {
					.owner = THIS_MODULE,
					.read  = do_read,
					.unlocked_ioctl = do_ioctl,
				 };

static int us_init(void){
	struct proc_dir_entry *res;

	us_buf = kmalloc(64, GFP_KERNEL);
	if(us_buf == NULL){
		printk(KERN_ERR "Couldn't reserve memory.");
		return -ENOMEM;
	}
	
	res = proc_create(device_name, 0, NULL, &amp;fileops);
	if(res == NULL){
		printk(KERN_ERR "Failed allocating a proc entry.");
		return -ENOMEM;
	}
	
	return 0;
}

static void us_exit(void){
	remove_proc_entry(device_name, NULL);
	kfree(us_buf);
}
MODULE_LICENSE("GPU");

module_init(us_init);
module_exit(us_exit);
</code></pre>
<p>To communicate with this <em>proc entry</em>, the following was written:</p>
<pre><code class="lang-auto">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/ioctl.h&gt;

#define device_path "/proc/useless"
#define SET_ADDRESS 0x00112233


void main(void){
	int fd;
	char *ok;
	char c[64];

	fd = open(device_path, O_RDONLY);
	
	ok = malloc(512);
	memcpy(ok, "Welp", sizeof(int ));
	
	ioctl(fd, SET_ADDRESS, ok);

	read(fd, c, sizeof( c));
	printf("%s\n", &amp;c);
}
</code></pre>
<p>This gives:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/b/b9e3fe8913d8af36cf6f2efc72a61fb13064ef73.png" alt="image" data-base62-sha1="qwsO47pCKiLfvekFOpRV0YnwdjR" width="419" height="98"><br>
<em>0x867 in binary is: 100001100111.<br>
Present: 1 (The page is present)<br>
R/W: 1 (The page have both read and write permissions)<br>
U/S: 1 (The page can be accessed by the user and supervisor)<br>
00<br>
Accessed: 1 (Set if the page had been accessed)<br>
Dirty: 1 (Set if the page was written to since last writeback)<br>
0000</em></p>
<p>Note that necessary checks on validity of return values was ignored in this example, these could be performed with <em>p??_none()</em> and <em>p??_present()</em>, and multiple other things could have been done, such as playing with the <em>PFN</em> or <em>page</em> or reading from the <em>Physical Address</em> with <em>void __iomem *</em>, <em>ioremap()</em> and <em>memcpy_fromio()</em> or <em>struct page *</em> and <em>kmap()</em>.</p>
<p>Translating address from virtual to physical takes time, so caching is implemented using the <em>TLB</em>(<em>Translation Lookaside Buffer</em>) to improve the performance, hopefully that the next access is going to land a cache-hit and that’ll hand the <em>PTE</em> faster than a miss where a <em>memory access</em> is forced to happen to get it. The <em>TLB</em> flushes from time to another, an example would be after a <em>page fault</em> is raised and completed.</p>
<h5>Processes:</h5>
<p>The kernel sees each process as a <strong>struct task_struct</strong> which is a huge struct that contains many fields which we can’t cover entirely, some are used to guarantee the (almost) fair scheduling and some show the task’s state(if it’s either unrunnable, runnable or stopped), priority, the parent process, a linked list of children processes, the address space it holds, and many others.<br>
We are mainly interested in the <strong>const struct cred __rcu *cred;</strong> which holds the task’s credentials.</p>
<pre><code class="lang-auto">struct cred {
	atomic_t	usage;
#ifdef CONFIG_DEBUG_CREDENTIALS
	atomic_t	subscribers;
	void		*put_addr;
	unsigned	magic;
#define CRED_MAGIC	0x43736564
#define CRED_MAGIC_DEAD	0x44656144
#endif
	kuid_t		uid;
	kgid_t		gid;
	kuid_t		suid;
	kgid_t		sgid;
	kuid_t		euid;
	kgid_t		egid;
	kuid_t		fsuid;
	kgid_t		fsgid;
	unsigned	securebits;
	kernel_cap_t	cap_inheritable;
	kernel_cap_t	cap_permitted;
	kernel_cap_t	cap_effective;
	kernel_cap_t	cap_bset;
	kernel_cap_t	cap_ambient;
#ifdef CONFIG_KEYS
	unsigned char	jit_keyring;
	struct key __rcu *session_keyring;
	struct key	*process_keyring;
	struct key	*thread_keyring;
	struct key	*request_key_auth;
#endif
#ifdef CONFIG_SECURITY
	void		*security;
#endif
	struct user_struct *user;
	struct user_namespace *user_ns;
	struct group_info *group_info;
	struct rcu_head	rcu;
} __randomize_layout;
</code></pre>
<p>This struct holds Capabilities, ((effective) <em>user</em> and <em>group</em>) ID, keyrings, (for synchronization, <em>Read-Copy-Update</em>) RCU, (tracks the user’s <em>usage</em> of the system by keeping <strong>counts</strong>) user and (holds <em>U/G ID</em> and the <strong>privileges</strong> for them) user_ns.<br>
In order to better understand this structure, a simple <em>proc entry</em> was created which extracts the <em>task_struct</em> of the process that uses it(<strong>current</strong>) and reads the effective <em>UID</em> and <em>GID</em>.</p>
<pre><code class="lang-auto">#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/proc_fs.h&gt;
#include &lt;linux/sched.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;linux/cred.h&gt;
#include &lt;linux/uidgid.h&gt;

#define device_name "useless"
#define SD_PRIV     0x10071007

struct{
	kuid_t ceuid;
	kgid_t cegid;
	spinlock_t clock;
}us_cd;

long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	int res;

	switch(cmd){
		case SD_PRIV:
			spin_lock(&amp;us_cd.clock);
			current_euid_egid(&amp;us_cd.ceuid, &amp;us_cd.cegid);
			spin_unlock(&amp;us_cd.clock);
			res = copy_to_user((void *)arg, &amp;us_cd, 8);
			return res;
		default:
			return -EINVAL;
	}
}

struct file_operations fileops = {
					.owner = THIS_MODULE,
					.unlocked_ioctl = do_ioctl,
				 };

static int us_init(void){
	struct proc_dir_entry *res;

	spin_lock_init(&amp;us_cd.clock);
	res = proc_create(device_name, 0, NULL, &amp;fileops);
	if(res == NULL){
		printk(KERN_ERR "Failed allocating a proc entry.");
		return -ENOMEM;
	}

	return 0;
}

static void us_exit(void){
	remove_proc_entry(device_name, NULL);
}
MODULE_LICENSE("GPU");

module_init(us_init);
module_exit(us_exit);
</code></pre>
<p>The initialization process starts by preparing the spinlock and creating a proc entry with a specified name <em>“useless”</em> and a file_operations struct containing only necessary <strong>owner</strong> and <strong>unlocked_ioctl</strong> entries.<br>
While the ioctl handler simply checks if the command passed was <em>SD_PRIV</em> to extract the <em>UID</em> and <em>GID</em> with a call to the <strong>current_euid_egid()</strong> macro which in turn calls <strong>current_cred()</strong> to extract the <em>current-&gt;cred</em>:</p>
<pre><code class="lang-auto">#define current_euid_egid(_euid, _egid)		\
do {						\
	const struct cred *__cred;		\
	__cred = current_cred();		\
	*(_euid) = __cred-&gt;euid;		\
	*(_egid) = __cred-&gt;egid;		\
} while(0)
</code></pre>
<pre><code class="lang-auto">#define current_cred() \
	rcu_dereference_protected(current-&gt;cred, 1)
</code></pre>
<p>Then, we create a <strong>tasktry.c</strong> to interract with the <em>/proc/useless</em>.</p>
<pre><code class="lang-auto">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/ioctl.h&gt;

#define device_path "/proc/useless"
#define SD_PRIV     0x10071007

struct{
	unsigned int uid;
	unsigned int gid;
}data;

void main(void){
	int fd;

	fd = open(device_path, O_RDONLY);
	ioctl(fd, SD_PRIV, &amp;data);

	printf("UID: %d GID: %d\n", data.uid, data.gid);
}
</code></pre>
<p>Two binaries are then created in <strong>/tmp</strong> directory, one which is compiled by root(<em>setuid</em> bit set) tasktry_root and the other by a normal user called tasktry_user.</p>
<pre><code class="lang-auto">root@Nwwz:~# cd /tmp
root@Nwwz:/tmp# gcc tasktry.c -o tasktry_root; chmod u+s tasktry_root
root@Nwwz:/tmp# cd /root/mod
root@Nwwz:~/mod# make
make -c /lib/modules/4.17.0/build M=/root/mod modules
make[1]: Entering directory '/usr/src/linux-4.17.2'
	CC [M]	/root/mod/task.o
	Building modules, stage 2.
	MODPOST 1 modules
	CC	/root/mod/task.mod.o
	LD [M] /root/mod/task.ko
make[1]: Leaving directory '/usr/src/linux-4.17.2'
root@Nwwz:~/mod# insmod task.ko
root@Nwwz:~/mod# su - user
user@Nwwz:~$ cd /tmp
user@Nwwz:/tmp$ gcc tasktry.c -o tasktry_user
user@Nwwz:/tmp$ ls
tasktry_user tasktry_root tasktry.c
user@Nwwz:/tmp$ ./tasktry_root
UID: 0 GID: 1000
user@Nwwz:/tmp$ ./tasktry_user
UID: 1000 GID: 1000
</code></pre>
<p>As you can see, the effective UID of <em>tasktry_root</em> is <em>0</em> making it own high privileges, so overwritting <em>effective creds</em> is one way to <em>privilege escalation</em>(<em>prepare_kernel_creds()</em> and <em>commit_creds()</em> are used for this <em>purpose in most exploits</em>, instead of getting the <em>stack base</em> and <em>overwritting it directly</em>.), another is to <em>change capabilities</em>.<br>
On <em>Windows</em>, one way to <em>escalate privileges</em> would be to steal the token of <em>System process</em>(<em>ID 4</em>) and assign it to the newly spawned <em>cmd.exe</em> after changing the <em>reference count</em>:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/7/70b355b66ad1e4811c79244b98fe125e52c57375.png" alt="image" data-base62-sha1="g4ZHnS7f1r9sa9TBv5Hted3xpmR" width="690" height="269"></p>
<h5>Syscalls:</h5>
<p>Processes running in userspace can still communicate with the kernel, thanks to <em>syscalls</em>.<br>
Each syscall is defined as follows:</p>
<pre><code class="lang-auto">SYSCALL_DEFINE0(getpid)
{
	return task_tgid_vnr(current);
}
</code></pre>
<p>With multiple arguments:</p>
<pre><code class="lang-auto">SYSCALL_DEFINE3(lseek, unsigned int, fd, off_t, offset, unsigned int, whence)
{
	return ksys_lseek(fd, offset, whence);
}
</code></pre>
<p>So, in general:</p>
<pre><code class="lang-auto">SYSCALL_DEFINE[ARG_COUNT]([SYSCALL_NAME], [ARG_TYPE], [ARG_NAME]){
	/* Passing the argument to another function, for processing. */
	return call_me([ARG_NAME]);
}
</code></pre>
<p>Few tries aaand <img src="https://0x00sec.org/images/emoji/twitter/slight_smile.png?v=9" title=":slight_smile:" class="emoji" alt=":slight_smile:">:</p>
<pre><code class="lang-auto">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;

int main(void){
	printf("ID: %d\n", getuid());
	
	return 0;
}
</code></pre>
<p>Running this sample with GDB and putting breakpoint on the x64 libc, we can see that it does set <em>EAX</em> register to <em>0x66</em>(<strong>syscall number on x64</strong>) before the <em>syscall</em> instruction.</p>
<pre><code class="lang-auto">(gdb) x/i $rip
=&gt; 0x555555554704 &lt;main+4&gt;:		callq 0x5555555545a0 &lt;getuid@plt&gt;
(gdb) x/x getuid
0x7ffff7af2f30 &lt;getuid&gt;: 		0x000066b8
(gdb) b* getuid
Breakpoint 2 at 0x7ffff7af2f30: file ../sysdeps/unix/syscall-template.S, line 65.
(gdb) c
Continuing.

Breakpoint 2, getuid () at ../sysdeps/unix/syscall-template.S:65
65		../sysdeps/unix/syscall-template.S: No such file or directory.
(gdb) disas $rip
Dump of assembler code for function getuid:
=&gt; 0x00007ffff7af2f30 &lt;+0&gt;:		mov		$0x66,%eax
   0x00007ffff7af2f35 &lt;+5&gt;:		syscall
   0x00007ffff7af2f37 &lt;+7&gt;:		retq
 End of assembler dump.
(gdb) shell
root@Nwwz:~# echo "g" &gt; /proc/sysrq-trigger
</code></pre>
<p>We can invoke a shell from <em>GDB</em> to force <em>SysRQ</em>, and see what this <em>offset</em> in the <em>kernel</em> links for:</p>
<pre><code class="lang-bash">[New Thread 756]
[New Thread 883]
[New Thread 885]

Thread 103 received signal SIGTRAP, Trace/breakpoint trap.
[Switching to Thread 889]
kgdb_breakpoint () at kernel/debug/debug_core.c:1073
10733			wmb(); /* Sync point after breakpoint */
(gdb) p &amp;sys_call_table
$1 = (const sys_call_ptr_t (*)[]) 0xffffffff81c00160 &lt;sys_call_table&gt;
(gdb) x/gx (void *)$1 + 0x66*8
0xffffffff81c00490 &lt;sys_call_table+816&gt;:	0xffffffff8108ec60
(gdb) x/i 0xffffffff8108ec60
0xffffffff8108ec60 &lt;__x64_sys_getuid&gt;:		nopl	0x0(%rax,%rax,1)
</code></pre>
<p>So, it’s the global <strong>sys_call_table</strong>, indexing the <em>__x64_sys_getuid</em> there.</p>
<pre><code class="lang-auto">"The __x64_sys_*() stubs are created on-the-fly for sys_*() system calls"
is written in syscall_64.tbl that contains all the syscalls
available to the kernel.
</code></pre>
<p>This is similiar to the <em>nt!KiServiceTable</em> on Windows.</p>
<pre><code class="lang-auto">kd&gt; dps nt!KeServiceDescriptorTable
82b759c0  82a89d9c nt!KiServiceTable
82b759c4  00000000
82b759c8  00000191
82b759cc  82a8a3e4 nt!KiArgumentTable
82b759d0  00000000
82b759d4  00000000
kd&gt; dd nt!KiServiceTable
82a89d9c  82c85c28 82acc40d 82c15b68 82a3088a
82a89dac  82c874ff 82b093fa 82cf7b05 82cf7b4e
82a89dbc  82c0a3bd 82d11368 82d125c1 82c00b95
kd&gt; ln 82c85c28
(82c85c28)   nt!NtAcceptConnectPort   |  (82c85ca5)   nt!EtwpRundownNotifications
Exact matches:
    nt!NtAcceptConnectPort = &lt;no type information&gt;
kd&gt; ln 82acc40d 
(82acc40d)   nt!NtAccessCheck   |  (82acc43e)   nt!PsGetThreadId
Exact matches:
    nt!NtAccessCheck = &lt;no type information&gt;
kd&gt; ln 82d125c1
(82d125c1)   nt!NtAddDriverEntry   |  (82d125f3)   nt!NtDeleteDriverEntry
Exact matches:
    nt!NtAddDriverEntry = &lt;no type information&gt;
</code></pre>
<p>Dissasembling it gives us:</p>
<pre><code class="lang-auto">(gdb) disas __x64_sys_getuid
Dump of assembler code for function __x64_sys_getuid:
	0xffffffff8108ec60 &lt;+0&gt;:	nopl	0x0(%rax,%rax,1)
	0xffffffff8108ec65 &lt;+5&gt;:	mov		%gs:0x15c00,%rax
	0xffffffff8108ec6e &lt;+14&gt;:	mov		0x668(%rax),%rax
	0xffffffff8108ec75 &lt;+21&gt;:	mov		0x4(%rax),%esi
	0xffffffff8108ec78 &lt;+24&gt;:	mov		0x88(%rax),%rdi
	0xffffffff8108ec7f &lt;+31&gt;:	callq	0xffffffff8112d4a0 &lt;from_kuid_munged&gt;
	0xffffffff8108ec84 &lt;+36&gt;:	mov		%eax,%eax
	0xffffffff8108ec86 &lt;+38&gt;:	retq
</code></pre>
<p>With a basic understanding of ASM and a very limited knowledge of the kernel <em>(AT&amp;T haha, too lazy to switch the syntax <img src="https://0x00sec.org/images/emoji/twitter/wink.png?v=9" title=":wink:" class="emoji" alt=":wink:">.)</em>, one can know that it does first search for the <em>current</em> task, store some pointer it holds at offset <em>0x668</em> at RAX before dereferencing it again and using content at +0x88(<em>RDI</em>) and +0x4(<em>RSI</em>) as arguments to the <strong>from_kuid_munged</strong> call before it nops and returns(<em>q there stands for qword</em>).<br>
We can verify this either by <em>looking at the source</em>:</p>
<pre><code class="lang-auto">SYSCALL_DEFINE0(getuid)
{
	return from_kuid_munged(current_user_ns(), current_uid());
}
</code></pre>
<pre><code class="lang-auto">uid_t from_kuid_munged(struct user_namespace *targ, kuid_t kuid)
{
	uid_t uid;
	uid = from_kuid(targ, kuid);

	if (uid == (uid_t) -1)
		uid = overflowuid;
	return uid;
}
EXPORT_SYMBOL(from_kuid_munged);
</code></pre>
<p>Or checking in <em>GDB</em>(<em>maybe both?</em>):</p>
<pre><code class="lang-auto">(gdb) b* __x64_sys_getuid
Breakpoint 1 at 0xffffffff8108ec60: file kernel/sys.c, line 920.
(gdb) c
[New Thread 938]
[Switching to Thread 938]

Thread 122 hit Breakpoint 1, __x64_sys_getuid () at kernel/sys.c:920
920		{
(gdb) ni
get_current () at ./arch/x86/include/asm/current.h:15
15		return this_cpu_read_stable(current_task);
(gdb) x/i $rip
=&gt; 0xffffffff8108ec65 &lt;__x64_sys_getuid+5&gt;:		mov		%gs:0x15c00,%rax
(gdb) p ((struct task_struct *)0)-&gt;cred
Cannot access memory at address 0x668
(gdb) p ((struct cred *)0)-&gt;uid
Cannot access memory at address 0x4
(gdb) p ((struct cred *)0)-&gt;user_ns
Cannot access memory at address 0x88
</code></pre>
<p>The sys_call_table is residing in a RO(<em>read only</em>) memory space:</p>
<pre><code class="lang-bash">(gdb) x/x sys_call_table
0xffffffff81c00160 &lt;sys_call_table&gt;:	0xffffffff81247310
(gdb) maintenance info sections
...
 [3]	0xffffffff81c00000-&gt;0xffffffff81ec1a42 at 0x00e00000: .rodata ALLOC LOAD RELOC DATA HAS_CONTENTS
...
(gdb) 
</code></pre>
<p>But a kernel module can overcome this protection and place a hook at any <em>systemcall</em>.<br>
For that, two example modules will be given:<br>
<strong>=] Disabling the previously discussed WP(<em>write-protect</em>) bit in the CR0(<em>control register <span class="hashtag">#0</span></em>), using <em>read_cr0</em> and <em>write_cr0</em> to acheive that.</strong></p>
<pre><code class="lang-auto">#include &lt;linux/fs.h&gt;
#include &lt;asm/pgtable.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;linux/kallsyms.h&gt;
#include &lt;linux/miscdevice.h&gt;
#include &lt;asm/special_insns.h&gt;

#define device_name "hookcontrol"
#define ioctl_base    0x005ec
#define ioctl_enable  ioctl_base+1
#define ioctl_disable ioctl_base+2

int    res;
int  (*real_getuid)(void);
void **sys_call_table;
unsigned long const *address;

static int hooked_getuid(void){
	printk(KERN_INFO "Received getuid call from %s!", current-&gt;comm);
	if(real_getuid != NULL){
		return real_getuid();
	}
	
	return 0;
}

long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	unsigned long cr0 = read_cr0();

	switch(cmd){
		case ioctl_enable:
		printk(KERN_INFO "Enabling hook!");
		write_cr0(cr0 &amp; ~0x10000);
		sys_call_table[__NR_getuid] = hooked_getuid;
		write_cr0(cr0 |  0x10000);
		printk(KERN_INFO "Successfully changed!");

		return 0;
		case ioctl_disable:
		printk(KERN_INFO "Disabling hook!");
		write_cr0(cr0 &amp; ~0x10000);
		sys_call_table[__NR_getuid] = real_getuid;
		write_cr0(cr0 |  0x10000);
		printk(KERN_INFO "Successfully restored!");
		
		return 0;
		default:
		return -EINVAL;
	}
}

struct file_operations file_ops = {
									.owner          = THIS_MODULE,
									.unlocked_ioctl = do_ioctl
								  };

struct miscdevice hk_dev = {
							MISC_DYNAMIC_MINOR,
							device_name,
							&amp;file_ops
						   };

static int us_init(void){
	res = misc_register(&amp;hk_dev);
	if(res){
		printk(KERN_ERR "Couldn't load module!");
		return -1;
	}
	
	sys_call_table = (void *) kallsyms_lookup_name("sys_call_table");
	real_getuid    = sys_call_table[__NR_getuid];
	address        = (unsigned long *) &amp;sys_call_table;
	printk(KERN_INFO "Module successfully loaded with minor: %d!", hk_dev.minor);
	return 0;
}

static void us_exit(void){
	misc_deregister(&amp;hk_dev);
}
MODULE_LICENSE("GPL");

module_init(us_init);
module_exit(us_exit);
</code></pre>
<p><strong>=] <em>Orr</em>’ing the protection mask of the page at which it resides(<em>__pgprot(_PAGE_RW)</em>)( <em>set_memory_rw()</em> &amp; <em>set_memory_rw()</em>), or directly modifying the <em>PTE</em>.</strong></p>
<pre><code class="lang-auto">static inline pte_t pte_mkwrite(pte_t pte)
{
	return pte_set_flags(pte, _PAGE_RW);
}

static inline pte_t pte_wrprotect(pte_t pte)
{
	return pte_clear_flags(pte, _PAGE_RW);
}
</code></pre>
<p>Looking at these functions, one can safely assume that manipulation can be acheived with simple OR and AND(<em>_PAGE_RW</em>) operations on the <em>pte_t</em>.</p>
<pre><code class="lang-auto">pte_t *lookup_address(unsigned long address, unsigned int *level)
{
	return lookup_address_in_pgd(pgd_offset_k(address), address, level);
}
</code></pre>
<p>Since it’s a kernel address, <em>pgd_offset_k()</em> is called, which makes use of <strong>&amp;init_mm</strong>, instead of a mm_struct belonging to some process of <em>one’s choice</em>.</p>
<pre><code class="lang-auto">pte_t *lookup_address_in_pgd(pgd_t *pgd, unsigned long address,
			     unsigned int *level)
{
	p4d_t *p4d;
	pud_t *pud;
	pmd_t *pmd;

	*level = PG_LEVEL_NONE;

	if (pgd_none(*pgd))
		return NULL;

	p4d = p4d_offset(pgd, address);
	if (p4d_none(*p4d))
		return NULL;

	*level = PG_LEVEL_512G;
	if (p4d_large(*p4d) || !p4d_present(*p4d))
		return (pte_t *)p4d;

	pud = pud_offset(p4d, address);
	if (pud_none(*pud))
		return NULL;

	*level = PG_LEVEL_1G;
	if (pud_large(*pud) || !pud_present(*pud))
		return (pte_t *)pud;

	pmd = pmd_offset(pud, address);
	if (pmd_none(*pmd))
		return NULL;

	*level = PG_LEVEL_2M;
	if (pmd_large(*pmd) || !pmd_present(*pmd))
		return (pte_t *)pmd;

	*level = PG_LEVEL_4K;

	return pte_offset_kernel(pmd, address);
}
</code></pre>
<p>so, the <em>ioctl</em> handler looks like this:</p>
<pre><code class="lang-auto">long do_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	unsigned int level;
	pte_t *pte = lookup_address(*address, &amp;level);;

	switch(cmd){
		case ioctl_enable:
		printk(KERN_INFO "Enabling hook!");
		pte-&gt;pte |= _PAGE_RW;
		sys_call_table[__NR_getuid] = hooked_getuid;
		pte-&gt;pte &amp;= ~_PAGE_RW;
		printk(KERN_INFO "Successfully changed!");

		return 0;
		case ioctl_disable:
		printk(KERN_INFO "Disabling hook!");
		pte-&gt;pte |= _PAGE_RW;
		sys_call_table[__NR_getuid] = real_getuid;
		pte-&gt;pte &amp;= ~_PAGE_RW;
		printk(KERN_INFO "Successfully restored!");
		
		return 0;
		default:
		return -EINVAL;
	}
}
</code></pre>
<p>(Know that these are only <em>examples</em>, <strong>usually</strong>, <em>replacing</em> should take place at <em>init</em> and <em>restoring the original</em> at <em>exit</em>, plus the definition of both the <em>hook</em> and <em>original</em> handlers, should hold <em>asmlinkage</em>(passing <em>arguments</em> in <em>stack</em>, unlike <em>fastcall</em>(<em>default</em>) in <em>registers</em>), however, since the syscall here holds no <em>arguments</em>, this was <em>ignored</em>.)<br>
By running an application from user-space to interact with <strong>/dev/hookcontrol</strong>: (<em>enabling and disabling after a while</em>) and taking a look at <em>dmesg</em>:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/1/18a67ac77e14a1f3d4f58460bd0ad0593364eaa4.png" alt="image" data-base62-sha1="3w48GWeMaYiIqdgRYJRrBDKcYCw" width="377" height="307"><br>
This can be used to provide a layer on the syscall, prevent or manipulate the return value, like <em>kill</em> to prevent a process from being <em>killed</em>, <em>getdents</em> to <em>hide</em> some files, <em>unlink</em> to prevent a file from being <em>deleted</em>, et cetera…<br>
And it doesn’t stop here, even without <em>syscall hooking</em>, one can play with processes(<em>hide them as an example…</em>) with <em>task_struct elements</em> and <em>per-task flags</em>, or change the <em>file_operations</em> in some <em>specific struct</em>, and <strong>many</strong> other possibilities.</p>
<h5>IDT(Interrupt Descriptor Table):</h5>
<p>In order to handle <em>exceptions</em>, this <em>table</em> exists, by linking a <em>specific handler</em> to each exception, it helps deal with those raised from <em>userspace</em>(<em>a translation to ring zero is required first</em>) and <em>kernelspace</em>.<br>
It first is initialized during early setup, and this can be seen in <em>setup_arch()</em> which calls multiple functions, some to setup the <em>IDT</em>, most important to us is <em>idt_setup_traps()</em>:</p>
<pre><code class="lang-auto">void __init idt_setup_traps(void)
{
	idt_setup_from_table(idt_table, def_idts, ARRAY_SIZE(def_idts), true);
}
</code></pre>
<p>It makes use of the <em>default IDT</em>s array(<em>def_idts</em>).</p>
<pre><code class="lang-auto">static const __initconst struct idt_data def_idts[] = {
	INTG(X86_TRAP_DE,		divide_error),
	INTG(X86_TRAP_NMI,		nmi),
	INTG(X86_TRAP_BR,		bounds),
	INTG(X86_TRAP_UD,		invalid_op),
	INTG(X86_TRAP_NM,		device_not_available),
	INTG(X86_TRAP_OLD_MF,		coprocessor_segment_overrun),
	INTG(X86_TRAP_TS,		invalid_TSS),
	INTG(X86_TRAP_NP,		segment_not_present),
	INTG(X86_TRAP_SS,		stack_segment),
	INTG(X86_TRAP_GP,		general_protection),
	INTG(X86_TRAP_SPURIOUS,		spurious_interrupt_bug),
	INTG(X86_TRAP_MF,		coprocessor_error),
	INTG(X86_TRAP_AC,		alignment_check),
	INTG(X86_TRAP_XF,		simd_coprocessor_error),

#ifdef CONFIG_X86_32
	TSKG(X86_TRAP_DF,		GDT_ENTRY_DOUBLEFAULT_TSS),
#else
	INTG(X86_TRAP_DF,		double_fault),
#endif
	INTG(X86_TRAP_DB,		debug),

#ifdef CONFIG_X86_MCE
	INTG(X86_TRAP_MC,		&amp;machine_check),
#endif

	SYSG(X86_TRAP_OF,		overflow),
#if defined(CONFIG_IA32_EMULATION)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_compat),
#elif defined(CONFIG_X86_32)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
#endif
};
</code></pre>
<p>On <em>x86_32</em> as an example, when an <em>int 0x80</em> is raised. the following happens:</p>
<pre><code class="lang-auto">static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
{
	struct thread_info *ti = current_thread_info();
	unsigned int nr = (unsigned int)regs-&gt;orig_ax;

#ifdef CONFIG_IA32_EMULATION
	ti-&gt;status |= TS_COMPAT;
#endif

	if (READ_ONCE(ti-&gt;flags) &amp; _TIF_WORK_SYSCALL_ENTRY) {
	
		nr = syscall_trace_enter(regs);
	}

	if (likely(nr &lt; IA32_NR_syscalls)) {
		nr = array_index_nospec(nr, IA32_NR_syscalls);
#ifdef CONFIG_IA32_EMULATION
		regs-&gt;ax = ia32_sys_call_table[nr](regs);
#else

		regs-&gt;ax = ia32_sys_call_table[nr](
			(unsigned int)regs-&gt;bx, (unsigned int)regs-&gt;cx,
			(unsigned int)regs-&gt;dx, (unsigned int)regs-&gt;si,
			(unsigned int)regs-&gt;di, (unsigned int)regs-&gt;bp);
#endif
	}

	syscall_return_slowpath(regs);
}

__visible void do_int80_syscall_32(struct pt_regs *regs)
{
	enter_from_user_mode();
	local_irq_enable();
	do_syscall_32_irqs_on(regs);
}
</code></pre>
<p>It would call <em>enter_from_user_mod()</em> to , then enable Interrupt Requests(<em>IRQs</em>) on the <em>current CPU</em>.<br>
Push the saved registers to find the syscall number(<em>EAX</em>), use it as an index in the <em>ia32_sys_call_table</em> array.<br>
Arguments are passed to the handler in <em>registers</em> with the <em>following order</em>: <em>EBX</em>, <em>ECX</em>, <em>EDX</em>, <em>ESI</em>, <em>EDI</em>, <em>EBP</em>.<br>
However, the first object as seen in the <em>idt_table</em> is the <strong>X86_TRAP_DE</strong>(<em>divide error</em>).<br>
This can be seen from <em>GDB</em>, that the first gate within <em>idt_table</em> holds the <em>offset_high</em>, <em>offset_middle</em> and <em>offset_low</em> referencing <em>divide_error</em>. Which would deal with <em>division by 0</em> exceptions.</p>
<pre><code class="lang-auto">(gdb) p idt_table
$1 = 0xffffffff82598000 &lt;idt_table&gt;
(gdb) p/x *(idt_table + 0x10*0)
$2 = {offset_low = 0xb90, segment = 0x10,
      bits = {ist = 0x0, zero = 0, type = 14, dpl = 0, p = 1},
	  offset_middle = 0x8180, offset_high = 0xffffffff, reserved = 0x0}
(gdb) x/8i 0xffffffff81800b90
	0xffffffff81800b90 &lt;divide_error&gt;:		nopl	(%rax)
	0xffffffff81800b93 &lt;divide_error+3&gt;:	pushq	$0xffffffffffffffff
	0xffffffff81800b95 &lt;divide_error+5&gt;:	callq	0xffffffff81801210 &lt;error_entry&gt;
	0xffffffff81800b9a &lt;divide_error+10&gt;:	mov		%rsp,%rdi
	0xffffffff81800b9d &lt;divide_error+13&gt;:	xor		%esi,%esi
	0xffffffff81800b9f &lt;divide_error+15&gt;:	callq	0xffffffff81025d60 &lt;do_devide_error&gt;
	0xffffffff81800ba4 &lt;divide_error+20&gt;:	jmpq	0xffffffff81801310 &lt;error_exit&gt;
</code></pre>
<p>You can see that it’s <em>DPL</em> is zero, that is, an <em>int $0x00</em> from a userland process wouldn’t help reaching it(unlike <em>int $0x03</em>, <em>int $0x04</em> or <em>int $0x80</em>). <em>Gate descriptors</em> are initialized in <em>idt_setup_from_table</em> which calls <em>idt_init_desc</em>:</p>
<pre><code class="lang-auto">idt_setup_from_table(gate_desc *idt, const struct idt_data *t, int size, bool sys)
{
	gate_desc desc;

	for (; size &gt; 0; t++, size--) {
		idt_init_desc(&amp;desc, t);
		write_idt_entry(idt, t-&gt;vector, &amp;desc);
		if (sys)
			set_bit(t-&gt;vector, system_vectors);
	}
}
</code></pre>
<p>And here it is.</p>
<pre><code class="lang-auto">static inline void idt_init_desc(gate_desc *gate, const struct idt_data *d)
{
	unsigned long addr = (unsigned long) d-&gt;addr;

	gate-&gt;offset_low	= (u16) addr;
	gate-&gt;segment		= (u16) d-&gt;segment;
	gate-&gt;bits		= d-&gt;bits;
	gate-&gt;offset_middle	= (u16) (addr &gt;&gt; 16);
#ifdef CONFIG_X86_64
	gate-&gt;offset_high	= (u32) (addr &gt;&gt; 32);
	gate-&gt;reserved		= 0;
#endif
}
</code></pre>
<p>This could be used by the attacker, such as by getting the IDT address using the SIDT instruction, and looking for a specific handler in the list, incrementing <em>offset_high</em> would set it to <em>0</em>.</p>
<pre><code class="lang-bash">As we said above, we're going to use the IDT and overwrite one of its
entries (more precisely a Trap Gate, so that we're able to hijack an
exception handler and redirect the code-flow towards userspace).
Each IDT entry is 64-bit (8-bytes) long and we want to overflow the
'base_offset' value of it, to be able to modify the MSB of the exception
handler routine address and thus redirect it below PAGE_OFFSET
(0xc0000000) value.
</code></pre>
<p>~ <a href="http://phrack.org/issues/64/6.html" rel="noopener nofollow ugc">Phrack</a></p>
<h5>KSPP:</h5>
<p>This is a protection that appeared starting from <em>4.8</em>, it’s name is a short for: <em>“Kernel self-protection project”</em>, It does provide additional checks on <strong>copy_to_user()</strong> and <strong>copy_from_user()</strong> to prevent classic buffer-overflows bugs from happening, by checking the saved compile-time buffer size and making sure it fits. if not, abort and prevent any possible exploitation from happening.</p>
<pre><code class="lang-bash">root@Nwwz:~/mod# cd /usr/src
root@Nwwz:/usr/src# cd linux-4.17.2
root@Nwwz:/usr/src/linux-4.17.2# cd include
root@Nwwz:/usr/src/linux-4.17.2/include# nano uaccess.h
</code></pre>
<p>We can directly see a check that’s <em>likely to be 1</em>, before proceeding to the <em>copy operation</em>:</p>
<pre><code class="lang-auto">static __always_inline unsigned long __must_check
copy_from_user(void *to, const void __user *from, unsigned long n)
{
	if (likely(check_copy_size(to, n, false)))
		n = _copy_from_user(to, from, n);
	return n;
}

static __always_inline unsigned long __must_check
copy_to_user(void __user *to, const void *from, unsigned long n)
{
	if (likely(check_copy_size(from, n, true)))
		n = _copy_to_user(to, from, n);
	return n;
}
</code></pre>
<p>The check function is as follows, it does first check the compile-time size against the requested size, and calls <em>__bad_copy_from()</em> or <em>__bad_copy_to()</em> depending on the boolean <em>is_source</em> if it seems like an overflow is possible, which is <em>unlikely</em> of course(<em>or not?</em>), it then returns false.<br>
If not, it does call <em>check_object_size()</em> and returns true.</p>
<pre><code class="lang-auto">extern void __compiletime_error("copy source size is too small")
__bad_copy_from(void);
extern void __compiletime_error("copy destination size is too small")
__bad_copy_to(void);

static inline void copy_overflow(int size, unsigned long count)
{
	WARN(1, "Buffer overflow detected (%d &lt; %lu)!\n", size, count);
}

static __always_inline bool
check_copy_size(const void *addr, size_t bytes, bool is_source)
{
	int sz = __compiletime_object_size(addr);
	if (unlikely(sz &gt;= 0 &amp;&amp; sz &lt; bytes)) {
		if (!__builtin_constant_p(bytes))
			copy_overflow(sz, bytes);
		else if (is_source)
			__bad_copy_from();
		else
			__bad_copy_to();
		return false;
	}
	check_object_size(addr, bytes, is_source);
	return true;
}
</code></pre>
<p>This function is simply just a wrapper around <em>__check_object_size()</em>.</p>
<pre><code class="lang-auto">#ifdef CONFIG_HARDENED_USERCOPY
extern void __check_object_size(const void *ptr, unsigned long n,
					bool to_user);

static __always_inline void check_object_size(const void *ptr, unsigned long n,
					      bool to_user)
{
	if (!__builtin_constant_p(n))
		__check_object_size(ptr, n, to_user);
}
#else
static inline void check_object_size(const void *ptr, unsigned long n,
				     bool to_user)
{ }
#endif
</code></pre>
<p>Additional checks are provided here in <em>__check_object_size()</em>, and as the comment says, not a <em>kernel .text</em> address, not a <em>bogus</em> address and is a safe <em>heap or stack object</em>.</p>
<pre><code class="lang-auto">void __check_object_size(const void *ptr, unsigned long n, bool to_user)
{
	if (static_branch_unlikely(&amp;bypass_usercopy_checks))
		return;

	if (!n)
		return;

	check_bogus_address((const unsigned long)ptr, n, to_user);

	check_heap_object(ptr, n, to_user);

	switch (check_stack_object(ptr, n)) {
	case NOT_STACK:
		break;
	case GOOD_FRAME:
	case GOOD_STACK:
		return;
	default:
		usercopy_abort("process stack", NULL, to_user, 0, n);
	}

	check_kernel_text_object((const unsigned long)ptr, n, to_user);
}
EXPORT_SYMBOL(__check_object_size);
</code></pre>
<p>With this, it does provide enough to block and kill classic buffer-overflow bugs, this can be disabled by <em>commenting the check</em> and <em>recompiling a module</em>.</p>
<h5>KASLR:</h5>
<p>Stands for <em>Kernel Address Space Layout Randomization</em>.<br>
It’s similiar to the ASLR on <em>userspace</em> which protects the <em>stack</em> and <em>heap addresses</em> from being at the <em>same location</em> in two different runs(<em>unless the attacker gets lucky <img src="https://0x00sec.org/images/emoji/twitter/stuck_out_tongue.png?v=9" title=":stuck_out_tongue:" class="emoji" alt=":stuck_out_tongue:"></em>). PIE too since it does target the <em>main binary segments</em> which are <em>text</em>, <em>data</em> and <em>bss</em>.</p>
<p>This protection randomizes the kernel segments(<em>Exception table</em>, <em>text</em>, <em>data</em>…) at each <em>restart</em>(<em>boot</em>), we’ve previously disabled it by using the <em>nokaslr</em> at the <em>kernel command line</em>.<br>
In order to experiment on it, this was removed and specific symbols in <em>/proc/kallsyms</em> were then fetched on two different runs.<br>
First run:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/7/79bda735ca6a331460d68ebc005b50152ff41588.png" alt="image" data-base62-sha1="hmY6rBts39KAyi3lwdELezIKy8w" width="575" height="81"><br>
Second run:<br>
<img src="https://0x00sec.s3.amazonaws.com/original/2X/9/973fcf58caa0e7f80b97352743743d59c9fd10f6.png" alt="image" data-base62-sha1="lA0OeuD5CE6XEqPtGRHGlLhto8u" width="574" height="82"><br>
This shows that addresses are randomly assigned on boottime to <em>_stext</em> and <em>_sdata</em>, whereas their end is just the <em>start address</em> plus a <em>size</em> which doesn’t change in this case(<em>0x21dc0</em> for <em>.data</em>, <em>0x6184d1</em> for <em>.text</em>), note that <em>.data</em> is on a constant distance from <em>.text</em>.<br>
So if the attacker gets the <em>.text base address</em>(<em>which is the result of a <strong>leak</strong></em>), he can know the location of all the <em>kernel symbols</em> even with <em>no access</em> to <em>kallsyms</em> using <em>RVAs</em>(<em>or offsets</em>), but he’ll have to compile the <em>target kernel</em> in his box to get them.<br>
This is for example used when <em>SMEP</em> is on and one has to go for <em>ROP</em> to disable it first, and then redirect execution to a shellcode placed in <em>userspace</em>(<em>&lt; TASK_SIZE</em>).</p>
<h5>kptr_restrict:</h5>
<p>This protection prevents <em>kernel addresses</em> from being exposed to the attacker. It does stop <em>%pK</em> format from dumping an address, and it’s work depends on the <em>kptr_restrict</em> value(0, 1 or 2).</p>
<pre><code class="lang-auto">Kernel Pointers:

	%pK	0x01234567 or 0x0123456789abcdef

	For printing kernel pointers which should be hidden from unprivileged
	users. The behaviour of %pK depends on the kptr_restrict sysctl - see
	Documentation/sysctl/kernel.txt for more details.
</code></pre>
<p>This can be seen in <em>kprobe_blacklist_seq_show()</em> which performs a <em>check</em> with a call to <em>kallsyms_show_value()</em>, depending on it, it <em>would</em> or <em>would not</em> print the <em>start</em> and <em>end</em> addresses.</p>
<pre><code class="lang-auto">static int kprobe_blacklist_seq_show(struct seq_file *m, void *v)
{
	struct kprobe_blacklist_entry *ent =
		list_entry(v, struct kprobe_blacklist_entry, list);

	if (!kallsyms_show_value())
		seq_printf(m, "0x%px-0x%px\t%ps\n", NULL, NULL,
			   (void *)ent-&gt;start_addr);
	else
		seq_printf(m, "0x%px-0x%px\t%ps\n", (void *)ent-&gt;start_addr,
			   (void *)ent-&gt;end_addr, (void *)ent-&gt;start_addr);
	return 0;
}
</code></pre>
<p>What <em>kallsyms_show_value()</em> does is shown here:</p>
<pre><code class="lang-auto">int kallsyms_show_value(void)
{
	switch (kptr_restrict) {
	case 0:
		if (kallsyms_for_perf())
			return 1;
	case 1:
		if (has_capability_noaudit(current, CAP_SYSLOG))
			return 1;
	default:
		return 0;
	}
}
</code></pre>
<p>If <em>kptr_restrict</em> value is 0, it does call <em>kallsyms_for_perf()</em> to check if <em>sysctl_perf_event_paranoid</em> value is smaller or equal to 1, returns 1 if true.<br>
If it’s 1, it checks if <em>CAP_SYSLOG</em> is within the user’s capabilities, if true, it returns 1.<br>
Otherwise, it returns 0.</p>
<p>Disabling this protection can be done by setting <em>/proc/sys/kernel/kptr_restrict</em> content to <strong>0</strong>.<br>
Or using <em>sysctl</em> to do that:</p>
<pre><code class="lang-auto">sysctl -w kernel.kptr_restrict=0
</code></pre>
<p>But watchout for <em>perf_event_paranoid</em> too, if it’s <em>&gt; 1</em>, then it needs to be adjusted.<br>
This is an example on the default kernel run by my <em>Debian VM</em>:</p>
<pre><code class="lang-bash">user@Nwwz:~$ cd /proc/self
user@Nwwz:/proc/self$ cat stack
[&lt;ffffffff81e7c869&gt;] do_wait+0x1c9/0x240
[&lt;ffffffff81e7d9ab&gt;] SyS_wait4+0x7b/0xf0
[&lt;ffffffff81e7b550&gt;] task_stopped_code+0x50/0x50
[&lt;ffffffff81e03b7d&gt;] do_syscall_64+0x8d/0xf0
[&lt;ffffffff8241244e&gt;] entry_SYSCALL_64_after_swapgs+0x58/0xc6
[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff
</code></pre>
<p>However, in the <em>4.17</em> kernel, we get this, because of <em>perf_event_paranoid</em>:</p>
<pre><code class="lang-bash">root@Nwwz:~# cd /proc/self
root@Nwwz:/proc/self# cat stack
[&lt;0&gt;] do_wait+0x1c9/0x240
[&lt;0&gt;] kernel_wait4+0x8d/0x140
[&lt;0&gt;] __do_sys_wait4+0x95/0xa0
[&lt;0&gt;] do_syscall_64+0x55/0x100
[&lt;0&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9
[&lt;0&gt;] 0xffffffffffffffff
root@Nwwz:/proc/self# cat /proc/sys/kernel/kptr_restrict
0
root@Nwwz:/proc/self# cat /proc/sys/kernel/perf_event_paranoid
2
</code></pre>
<h5>mmap_min_addr:</h5>
<p>The <em>mm_struct</em> within <em>task_struct</em> holds an operation function called <em>get_unmapped_area</em>.</p>
<pre><code class="lang-auto">struct mm_struct {
...
#ifdef CONFIG_MMU
		unsigned long (*get_unmapped_area) (struct file *filp,
				unsigned long addr, unsigned long len,
				unsigned long pgoff, unsigned long flags);
#endif
...
}
</code></pre>
<p>It is then extracted in <em>get_unmapped_area()</em>, which tries to get it from the <em>mm</em>(<em>mm_struct</em>), before checking it’s <em>file</em> and it’s <em>file_operations</em> or if it has the <em>MAP_SHARED</em> flag and assign <em>shmem_get_unmapped_area()</em> to it.<br>
However, within the <em>mm_struct</em>, the default value of <em>get_unmapped_area</em> is the <em>arch specific</em> function.<br>
This function does search for a <em>large enough memory block</em> to satisfy the <em>request</em>, but before returning the <em>addr</em>, it does check if it’s <strong>bigger or equal</strong> to <em>mmap_min_addr</em>, which means that any address <em>below it</em> will <strong>not be given</strong>, this prevents <em>NULL pointer dereference</em> attack from happening(no mmaping <em>NULL address</em>, <em>nothing</em> will be stored there(<em>shellcode, pointers…</em>)).</p>
<p>Disabling this protection can be done by setting <em>/proc/sys/vm/mmap_min_addr</em> content to <strong>0</strong>, or using <em>sysctl</em> like before.</p>
<pre><code class="lang-auto">sysctl -w vm.mmap_min_addr=0
</code></pre>
<h5>addr_limit:</h5>
<p>The <strong>thread</strong>(<em>thread_struct</em>) within the <em>task_struct</em> contains some important fields, amongst them, is the <em>addr_limit</em>.</p>
<pre><code class="lang-auto">typedef struct {
	unsigned long		seg;
} mm_segment_t;

struct thread_struct {
	...
	mm_segment_t		addr_limit;

	unsigned int		sig_on_uaccess_err:1;
	unsigned int		uaccess_err:1;
	...
};
</code></pre>
<p>This can be read with a call to <em>get_fs()</em>, changed with <em>set_fs()</em>:</p>
<pre><code class="lang-auto">#define MAKE_MM_SEG(s)	((mm_segment_t) { (s) })

#define KERNEL_DS	MAKE_MM_SEG(-1UL)
#define USER_DS 	MAKE_MM_SEG(TASK_SIZE_MAX)

#define get_ds()	(KERNEL_DS)
#define get_fs()	(current-&gt;thread.addr_limit)
static inline void set_fs(mm_segment_t fs)
{
	current-&gt;thread.addr_limit = fs;
	
	set_thread_flag(TIF_FSCHECK);
}
</code></pre>
<p>When <em>userspace</em> likes to reach an address, it is checked against this first, so overwritting it with -1UL(<em>KERNEL_DS</em>) would let you access(<em>read or write</em>) to <em>kernelspace</em>.</p>
<p>This was the introduction, I’ve noticed that it has grown bigger than I expected, so I stopped, and removed parts about <a href="https://en.wikipedia.org/wiki/Kernel_page-table_isolation" rel="noopener nofollow ugc">protections</a>, <a href="https://meltdownattack.com/meltdown.pdf" rel="noopener nofollow ugc">side-channel</a> <a href="https://arxiv.org/pdf/1901.01161.pdf" rel="noopener nofollow ugc">attacks</a> and others.</p>
<p>Starting this was possible, thanks to: <a class="mention" href="https://0x00sec.org/u/_py">@_py</a>(DA BEST), <a class="mention" href="https://0x00sec.org/u/pry0cc">@pry0cc</a>, <a class="mention" href="https://0x00sec.org/u/anon79434934">@anon79434934</a>, <a class="mention" href="https://0x00sec.org/u/4w1il">@4w1il</a>, <a class="mention" href="https://0x00sec.org/u/ricksanchez">@ricksanchez</a> and <a class="mention" href="https://0x00sec.org/u/leeky">@Leeky</a>.<br>
See y’all in <em>part 1</em>, peace.</p>
<p>“<em>nothing is enough</em>, <em>search more to learn more</em>”.<br>
~ exploit</p>
          <p><a href="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585/1</link>
        <pubDate>Thu, 14 Feb 2019 17:42:37 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-11585-1</guid>
        <source url="https://0x00sec.org/t/point-of-no-c3-linux-kernel-exploitation-part-0/11585.rss">Point of no C3 | Linux Kernel Exploitation - Part 0</source>
      </item>
  </channel>
</rss>
