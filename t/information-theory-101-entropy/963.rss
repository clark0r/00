<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Information Theory 101. Entropy</title>
    <link>https://0x00sec.org/t/information-theory-101-entropy/963</link>
    <description>Continuing the discussion from [CAPTCHA - Randomness applicability](https://0x00sec.org/t/captcha-randomness-applicability/954/9):

When I first discovered [Information Theory](https://en.wikipedia.org/wiki/Information_theory) many years ago I was amazed. The fact of being able to quantify the amount of information in a sequence of bits just looked awesome. So I would try to explain this as simpler as I can. There will be a bit of Math, but despite of those logarithms all around the place, the idea behind is pretty simple and elegant.

The [Entropy]( https://en.wikipedia.org/wiki/Entropy_(information_theory)) is a magnitude that measures the unpredictability of the information contained by a set of possible values. The idea is that, if we can predict the information in the message, then we can _reconstruct_ the message, and, if we can predict what comes next in a message, we can do some interesting stuff: compress data or having a chance to find out the next number in a sequence.

The Entropy measures, in a sense, our chances to predict the next symbol in a message.


# Amount of Information
So... How can we measure the amount of information contained in a message?. Let&#39;s figure this out using one classic example:

Suppose we have the following two symbols:

* It is not raining at Atacama Desert
* It is raining at Atacama Desert

So, which one of those symbols gives you more information?. In case you do not know, the [Atacama desert](https://en.wikipedia.org/wiki/Atacama_Desert#Climate) at South America is one of the driest places in the work. It does not rain very often there. 

Yes, you are completely right. If somebody tells you that it is raining at Atacama desert it is giving you a lot of information.... because... the probability of raining at that place is very, very low. If somebody says that it is not raining at the Atacama desert, you are not getting much information, because that is happening all the time. 

Therefore, the amount of information of a given symbol/message is inversely proportional to the probability of occurrence of that symbol/message. The lower the probability, the higher the information we are received.

# Entropy 
Back to the original definition, the entropy should give us a number that will provide a measurement of the predictability/unpredictability of the messages we may compose with a given set of symbols.

The expression to calculate the Entropy for a given set of symbols is:

&gt; H = SUM {p&lt;sub&gt;i&lt;/sub&gt; * log (1 / p&lt;sub&gt;i&lt;/sub&gt;)} = - SUM {(p&lt;sub&gt;i&lt;/sub&gt;) * log (p&lt;sub&gt;i&lt;/sub&gt;)}

_You know 1/x = x&lt;sup&gt; -1&lt;/sup&gt; and exponents get out of the log multiplying_

where p&lt;sub&gt;i&lt;/sub&gt; is the probability of the i-th symbol in our set.

&gt; __Note__

&gt; In case you do not know, the function log&lt;sub&gt; b &lt;/sub&gt;(x) = y such as b&lt;sup&gt; y &lt;/sup&gt; = x
&gt; For instance: log&lt;sub&gt; 2&lt;/sub&gt; (8) = 3 because 2&lt;sup&gt; 3 &lt;/sup&gt; = 8  log&lt;sub&gt; 10 &lt;/sub&gt; (1000) = 4 because 10&lt;sup&gt; 4 &lt;/sup&gt;=1000


So, what we have is a weighted average (we are multiplying each value in the average by its probability) of the logarithm of the inverse of the probability.

As we said, the amount of information in a symbol is inversely proportional to its predictability (remember the rain at Atacama?). As probabilities can only take values between 0 and 1, using the log function allows us to scale those values conveniently, providing a higher value for lower probabilities. You know... the log (1) equals 0 (because b &lt;sup&gt; 0&lt;/sup&gt; = 1 for all b) and the log (0) equals -infinite.

Let&#39;s do some numbering to make more sense out of that expression works.

# Messages
So, let&#39;s assume we have an alphabet composed of 4 symbols: A, B, C, D (yes I took this example from the wiki page :).

Now, let&#39;s suppose that all symbols have the same probability (1/4 it should be). Let&#39;s calculate the entropy for this set of symbols:

&gt; H = - (1/4 * log (1/4) + 1/4 * log (1/4) +...) 
&gt; H = - 4 * 1/4 * log(1/4) 
&gt; H = log (1/4)

Now, let&#39;s choose a base for our logarithm. Depending on the base we chose, the result will be bits (or shannon if we use base 2), nats (if we use base e) or hartleys (if we use base 10). Let&#39;s go for base 2 and get some bits!

&gt; H = - log&lt;sub&gt;2&lt;/sub&gt; (1/4) = log&lt;sub&gt;2&lt;/sub&gt; (4) = 2 bits

_Note the `-` is gone in the second step_ 

So, this is the maximum entropy, or in other words the amount of bits we have to use to encode this four symbols in binary. As all of them are equally probable, we cannot _compress_ them. In other words, any sequence of those values is random and the entropy is maximal. We cannot predict the next value, it may be any of the four symbols.

Now let&#39;s chose a different set of probabilities:

&gt; p&lt;sub&gt; a&lt;/sub&gt; = 1/2
&gt; p&lt;sub&gt; b&lt;/sub&gt; = 1/4
&gt; p&lt;sub&gt; c&lt;/sub&gt; = 1/8
&gt; p&lt;sub&gt; d&lt;/sub&gt; = 1/8

&gt; H = - (1/2 * log&lt;sub&gt;2&lt;/sub&gt;(1/2) + 1/4 * log&lt;sub&gt;2&lt;/sub&gt;(1/4) + 1/8 * log&lt;sub&gt;2&lt;/sub&gt;(1/8) + 1/8 * log&lt;sub&gt;2&lt;/sub&gt;(1/8)) 
&gt; H = - (1/2 * (-1) + 1/4 * (-2) + 1/8 * (-3) + 1/8 * (-3)) 
&gt; H = - (-1/2 - 1/2 - 2*3/8) = - ( -1 - 3/4) = - (-7/4) = 1.75 bits

So, we need 1.75 bits to encode a message using this alphabet... but how?

# Prefix Codes and Huffman Compression
Now, you may be wondering how do you can code 4 symbols using less than 2 bits. Well the answer is: [variable-length codes](https://en.wikipedia.org/wiki/Variable-length_code)  We will assign longest codes to the less probable symbols. The simplest way to build such codes is using a [prefix code](https://en.wikipedia.org/wiki/Prefix_code)

A prefix code assigns a code to each symbol in a way that there is no whole codeword that is a prefix of any other code word. The Huffman code is a prefix code that is optimal. Well, let&#39;s forget about this for now.

For our previous example, let&#39;s build a prefix code so we can encode our messages using  (in average) 1.75 bits instead of 2.

    Symbol  |A   |B   |C     |D
    --------+----+----+------+-----
    Prob    |.5  |.25 |0.125 |0.125
    Code 1  |0   |1   |1     |1
    Code 2  |0   |10  |11    |11
    Code 3  |0   |10  |110   |111

`Code1` to `Code3` are different iterations were we add a bit to the symbols with higher probability. The last line represents our final code.

At `Code 1` we assign `0` to `A` (most probable symbol) and `1` as first bit for all other symbols, in order to fullfil our prefix condition. We are done with `A` but we have to continue with the rest. Now `A`, the symbol that will appear more often, has the sorted codeword... just one bit

At `Code 2` we assign an extra `0` to `B` (the second more probable symbol). So the symbol `B` (the second most common symbol) will have a 2 bits codeword. We add a `1` to the rest of the symbols that have lower probabilities.

Finally, at `Code 3` we assign extra `0` to symbol `C` and an extra `1` to symbol `D`. Now, the two less frequent symbols will use 3 bits instead of 2, but as they appear a lot less than the symbols `A` and `B`, in average the length of the message should be smaller.

Also note that, none of the code words is a prefix of any other. This will allow us to always unambiguously decode a sequence composed of those symbols. For instance

Given the sequence:
`000101101010011110000010101000`

We can decode it:


    Variable-Len 0  0  0  10 110 10 10 0  111 10 0  0  0  0  10 10 10 0  0
                 A  A  A  B  C   B  B  A   D   B A  A  A  A  B  B  B  A  A
    Fixed-Len    00 00 00 01 02  01 01 00 11  01 00 00 00 00 01 01 01 00 00

Our sequence has 19 symbols. Using the normal 2 bits per symbol, the whole sequence will require 38 bits to be encoded , but we just used 31 bits. This gave us 31/19 = 1.63 bits per symbol, that is pretty close to our theoretical 1.75 bits per symbol.

If you produce a longer sequence with proper symbol probabilities you should get a value even closer to the theoretical entropy value.

# Nice, but what does all this mean?
Well, roughly speaking the entropy measures the randomness of a message. For completely random messages entropy will be maximal. Otherwise, the sequence is not random and therefore it can be predicted at some extend.

The direct application of this predictability is compression. If we can predict the next symbol in a sequence, then we do not need to store it or transmitted it... we are making the sequence shorter... we are compressing it.

The other direct application of predictability is actually predicting the next value in the sequence. This is directly related to [the great post from Nitrax](https://0x00sec.org/t/captcha-randomness-applicability/954) that  you should read. 

However, the entropy itself, does not help to predict the values. The entropy just let us know how much redundancy (predictability) is in our symbol sequence and therefore how far we can go predicting values in the sequence. Nitrax post is great because it takes advantage of this property in an unexpected way... As he said... &quot;Think out of the box&quot;.

By the way, Huffman codes (prefix codes build using the Huffman algorithm) are used all around the world. As an example, the JPEG standard uses Huffman codes internally.

Well, not sure if this has clarified something or actually add more noise to your brain! :P</description>
    
    <lastBuildDate>Wed, 24 Aug 2016 20:31:21 +0000</lastBuildDate>
    <category>Cryptology</category>
    <atom:link href="https://0x00sec.org/t/information-theory-101-entropy/963.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[system]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed after 30 days. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/9">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/9</link>
        <pubDate>Sun, 21 Jan 2018 00:41:40 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-9</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[Donnette]]></dc:creator>
        <description><![CDATA[
            <p>You are wicked smart! Thank you!!! <img src="https://0x00sec.org/images/emoji/twitter/innocent.png?v=9" title=":innocent:" class="emoji" alt=":innocent:"><br>
I love your ability to explain!</p>
<p>“If you can’t explain it simply, you don’t understand it well enough.” Albert Einstein</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/8">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/8</link>
        <pubDate>Wed, 24 Aug 2016 20:31:21 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-8</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[0x00pf]]></dc:creator>
        <description><![CDATA[
            <p>That’s completely right <a class="mention" href="https://0x00sec.org/u/oaktree">@oaktree</a> thanks for the correction</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/7">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/7</link>
        <pubDate>Wed, 24 Aug 2016 05:24:31 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-7</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[oaktree]]></dc:creator>
        <description><![CDATA[
            <p>OK Just one tiny correction:</p>
<p>The <code>log</code> function only approaches <code>0</code> <em>asymptotically</em>. Thus, <code>logx(0)</code> does not exist for any <code>x</code>, but can be noted as <code>-infinity</code>.</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/6">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/6</link>
        <pubDate>Tue, 23 Aug 2016 23:37:00 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-6</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[oaktree]]></dc:creator>
        <description><![CDATA[
            <p><a class="mention" href="https://0x00sec.org/u/0x00pf">@0x00pf</a>, I’ve honestly started “liking” your articles before even reading them. I just expect them to be good.</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/5">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/5</link>
        <pubDate>Tue, 23 Aug 2016 21:57:48 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-5</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[_py]]></dc:creator>
        <description><![CDATA[
            <p>Fantastic explanation! Right when I was having my refresher in Information Theory for my uni exam, you drop this bomb article. It’d be great to see some compression algos in the future (i.e Huffman, Shannon.)</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/4">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/4</link>
        <pubDate>Tue, 23 Aug 2016 17:45:26 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-4</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[0x00pf]]></dc:creator>
        <description><![CDATA[
            <p>that sounds great… looking forward to that malware stuff!</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/3">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/3</link>
        <pubDate>Tue, 23 Aug 2016 16:35:46 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-3</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[Nitrax]]></dc:creator>
        <description><![CDATA[
            <p>Love it mate ! I appreciate that you took the time to clarify the concept of entropy ! Very well explained with some self-explaining comparisons, simplifying the approach of such notion !</p>
<p>By the way, the Huffman algorithm is widely used in the area of malware classification too. It is a reliable alternative to <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow noopener">the Kolmogorov complexity</a>. I think that I will further develop this concept in my next article.</p>
<p>Once again, well done and great initiative !</p>
<p>Best,<br>
Nitrax</p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/2</link>
        <pubDate>Tue, 23 Aug 2016 16:26:12 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-2</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
      <item>
        <title>Information Theory 101. Entropy</title>
        <dc:creator><![CDATA[0x00pf]]></dc:creator>
        <description><![CDATA[
            <p>Continuing the discussion from <a href="https://0x00sec.org/t/captcha-randomness-applicability/954/9">CAPTCHA - Randomness applicability</a>:</p>
<p>When I first discovered <a href="https://en.wikipedia.org/wiki/Information_theory" rel="nofollow noopener">Information Theory</a> many years ago I was amazed. The fact of being able to quantify the amount of information in a sequence of bits just looked awesome. So I would try to explain this as simpler as I can. There will be a bit of Math, but despite of those logarithms all around the place, the idea behind is pretty simple and elegant.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="nofollow noopener">Entropy</a> is a magnitude that measures the unpredictability of the information contained by a set of possible values. The idea is that, if we can predict the information in the message, then we can <em>reconstruct</em> the message, and, if we can predict what comes next in a message, we can do some interesting stuff: compress data or having a chance to find out the next number in a sequence.</p>
<p>The Entropy measures, in a sense, our chances to predict the next symbol in a message.</p>
<h1>Amount of Information</h1>
<p>So… How can we measure the amount of information contained in a message?. Let’s figure this out using one classic example:</p>
<p>Suppose we have the following two symbols:</p>
<ul>
<li>It is not raining at Atacama Desert</li>
<li>It is raining at Atacama Desert</li>
</ul>
<p>So, which one of those symbols gives you more information?. In case you do not know, the <a href="https://en.wikipedia.org/wiki/Atacama_Desert#Climate" rel="nofollow noopener">Atacama desert</a> at South America is one of the driest places in the work. It does not rain very often there.</p>
<p>Yes, you are completely right. If somebody tells you that it is raining at Atacama desert it is giving you a lot of information… because… the probability of raining at that place is very, very low. If somebody says that it is not raining at the Atacama desert, you are not getting much information, because that is happening all the time.</p>
<p>Therefore, the amount of information of a given symbol/message is inversely proportional to the probability of occurrence of that symbol/message. The lower the probability, the higher the information we are received.</p>
<h1>Entropy</h1>
<p>Back to the original definition, the entropy should give us a number that will provide a measurement of the predictability/unpredictability of the messages we may compose with a given set of symbols.</p>
<p>The expression to calculate the Entropy for a given set of symbols is:</p>
<blockquote>
<p>H = SUM {p<sub>i</sub> * log (1 / p<sub>i</sub>)} = - SUM {(p<sub>i</sub>) * log (p<sub>i</sub>)}</p>
</blockquote>
<p><em>You know 1/x = x<sup> -1</sup> and exponents get out of the log multiplying</em></p>
<p>where p<sub>i</sub> is the probability of the i-th symbol in our set.</p>
<blockquote>
<p><strong>Note</strong></p>
</blockquote>
<blockquote>
<p>In case you do not know, the function log<sub> b </sub>(x) = y such as b<sup> y </sup> = x<br>
For instance: log<sub> 2</sub> (8) = 3 because 2<sup> 3 </sup> = 8  log<sub> 10 </sub> (1000) = 4 because 10<sup> 4 </sup>=1000</p>
</blockquote>
<p>So, what we have is a weighted average (we are multiplying each value in the average by its probability) of the logarithm of the inverse of the probability.</p>
<p>As we said, the amount of information in a symbol is inversely proportional to its predictability (remember the rain at Atacama?). As probabilities can only take values between 0 and 1, using the log function allows us to scale those values conveniently, providing a higher value for lower probabilities. You know… the log (1) equals 0 (because b <sup> 0</sup> = 1 for all b) and the log (0) equals -infinite.</p>
<p>Let’s do some numbering to make more sense out of that expression works.</p>
<h1>Messages</h1>
<p>So, let’s assume we have an alphabet composed of 4 symbols: A, B, C, D (yes I took this example from the wiki page :).</p>
<p>Now, let’s suppose that all symbols have the same probability (1/4 it should be). Let’s calculate the entropy for this set of symbols:</p>
<blockquote>
<p>H = - (1/4 * log (1/4) + 1/4 * log (1/4) +…)<br>
H = - 4 * 1/4 * log(1/4)<br>
H = log (1/4)</p>
</blockquote>
<p>Now, let’s choose a base for our logarithm. Depending on the base we chose, the result will be bits (or shannon if we use base 2), nats (if we use base e) or hartleys (if we use base 10). Let’s go for base 2 and get some bits!</p>
<blockquote>
<p>H = - log<sub>2</sub> (1/4) = log<sub>2</sub> (4) = 2 bits</p>
</blockquote>
<p><em>Note the <code>-</code> is gone in the second step</em></p>
<p>So, this is the maximum entropy, or in other words the amount of bits we have to use to encode this four symbols in binary. As all of them are equally probable, we cannot <em>compress</em> them. In other words, any sequence of those values is random and the entropy is maximal. We cannot predict the next value, it may be any of the four symbols.</p>
<p>Now let’s chose a different set of probabilities:</p>
<blockquote>
<p>p<sub> a</sub> = 1/2<br>
p<sub> b</sub> = 1/4<br>
p<sub> c</sub> = 1/8<br>
p<sub> d</sub> = 1/8</p>
</blockquote>
<blockquote>
<p>H = - (1/2 * log<sub>2</sub>(1/2) + 1/4 * log<sub>2</sub>(1/4) + 1/8 * log<sub>2</sub>(1/8) + 1/8 * log<sub>2</sub>(1/8))<br>
H = - (1/2 * (-1) + 1/4 * (-2) + 1/8 * (-3) + 1/8 * (-3))<br>
H = - (-1/2 - 1/2 - 2*3/8) = - ( -1 - 3/4) = - (-7/4) = 1.75 bits</p>
</blockquote>
<p>So, we need 1.75 bits to encode a message using this alphabet… but how?</p>
<h1>Prefix Codes and Huffman Compression</h1>
<p>Now, you may be wondering how do you can code 4 symbols using less than 2 bits. Well the answer is: <a href="https://en.wikipedia.org/wiki/Variable-length_code" rel="nofollow noopener">variable-length codes</a>  We will assign longest codes to the less probable symbols. The simplest way to build such codes is using a <a href="https://en.wikipedia.org/wiki/Prefix_code" rel="nofollow noopener">prefix code</a></p>
<p>A prefix code assigns a code to each symbol in a way that there is no whole codeword that is a prefix of any other code word. The Huffman code is a prefix code that is optimal. Well, let’s forget about this for now.</p>
<p>For our previous example, let’s build a prefix code so we can encode our messages using  (in average) 1.75 bits instead of 2.</p>
<pre><code>Symbol  |A   |B   |C     |D
--------+----+----+------+-----
Prob    |.5  |.25 |0.125 |0.125
Code 1  |0   |1   |1     |1
Code 2  |0   |10  |11    |11
Code 3  |0   |10  |110   |111
</code></pre>
<p><code>Code1</code> to <code>Code3</code> are different iterations were we add a bit to the symbols with higher probability. The last line represents our final code.</p>
<p>At <code>Code 1</code> we assign <code>0</code> to <code>A</code> (most probable symbol) and <code>1</code> as first bit for all other symbols, in order to fullfil our prefix condition. We are done with <code>A</code> but we have to continue with the rest. Now <code>A</code>, the symbol that will appear more often, has the sorted codeword… just one bit</p>
<p>At <code>Code 2</code> we assign an extra <code>0</code> to <code>B</code> (the second more probable symbol). So the symbol <code>B</code> (the second most common symbol) will have a 2 bits codeword. We add a <code>1</code> to the rest of the symbols that have lower probabilities.</p>
<p>Finally, at <code>Code 3</code> we assign extra <code>0</code> to symbol <code>C</code> and an extra <code>1</code> to symbol <code>D</code>. Now, the two less frequent symbols will use 3 bits instead of 2, but as they appear a lot less than the symbols <code>A</code> and <code>B</code>, in average the length of the message should be smaller.</p>
<p>Also note that, none of the code words is a prefix of any other. This will allow us to always unambiguously decode a sequence composed of those symbols. For instance</p>
<p>Given the sequence:<br>
<code>000101101010011110000010101000</code></p>
<p>We can decode it:</p>
<pre><code>Variable-Len 0  0  0  10 110 10 10 0  111 10 0  0  0  0  10 10 10 0  0
             A  A  A  B  C   B  B  A   D   B A  A  A  A  B  B  B  A  A
Fixed-Len    00 00 00 01 02  01 01 00 11  01 00 00 00 00 01 01 01 00 00
</code></pre>
<p>Our sequence has 19 symbols. Using the normal 2 bits per symbol, the whole sequence will require 38 bits to be encoded , but we just used 31 bits. This gave us 31/19 = 1.63 bits per symbol, that is pretty close to our theoretical 1.75 bits per symbol.</p>
<p>If you produce a longer sequence with proper symbol probabilities you should get a value even closer to the theoretical entropy value.</p>
<h1>Nice, but what does all this mean?</h1>
<p>Well, roughly speaking the entropy measures the randomness of a message. For completely random messages entropy will be maximal. Otherwise, the sequence is not random and therefore it can be predicted at some extend.</p>
<p>The direct application of this predictability is compression. If we can predict the next symbol in a sequence, then we do not need to store it or transmitted it… we are making the sequence shorter… we are compressing it.</p>
<p>The other direct application of predictability is actually predicting the next value in the sequence. This is directly related to <a href="https://0x00sec.org/t/captcha-randomness-applicability/954">the great post from Nitrax</a> that  you should read.</p>
<p>However, the entropy itself, does not help to predict the values. The entropy just let us know how much redundancy (predictability) is in our symbol sequence and therefore how far we can go predicting values in the sequence. Nitrax post is great because it takes advantage of this property in an unexpected way… As he said… “Think out of the box”.</p>
<p>By the way, Huffman codes (prefix codes build using the Huffman algorithm) are used all around the world. As an example, the JPEG standard uses Huffman codes internally.</p>
<p>Well, not sure if this has clarified something or actually add more noise to your brain! <img src="https://0x00sec.org/images/emoji/twitter/stuck_out_tongue.png?v=9" title=":stuck_out_tongue:" class="emoji" alt=":stuck_out_tongue:"></p>
          <p><a href="https://0x00sec.org/t/information-theory-101-entropy/963/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/information-theory-101-entropy/963/1</link>
        <pubDate>Tue, 23 Aug 2016 16:07:57 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-963-1</guid>
        <source url="https://0x00sec.org/t/information-theory-101-entropy/963.rss">Information Theory 101. Entropy</source>
      </item>
  </channel>
</rss>
