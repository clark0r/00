<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Compression for P2P Networks</title>
    <link>https://0x00sec.org/t/compression-for-p2p-networks/32119</link>
    <description>I know there are algorithms like gzip to compress files, but I am wondering if there are alternatives to compressing files for my use case. I have created a P2P browser network that stores data in LocalStorage (which has a limit of 5mb of storage). Here is the process:
- Turn file into an ArrayBuffer
- Split bytes into chunks of 50kb
- Upload chunks to the network (peers get the data and store it in their LocalStorage)

The problem is that I had to increase the quota/limit for LocalStorage for file uploads to even work.

Currently I am thinking of an algorithm that looks at the byte array and then examines the array by chunks. Assume the array is of length N, the algo would start off by examining chunks with length N, then N-1, then N-2 and so on. If there was a large chunk of 0s of length L, for example, it would store this chunk as 0xL instead of the L number of 0s. If there was a chunk of 1s of length 43, for example, it would do the same, making it 1x43. Putting these compressed chunks in order would allow for lots of compression with varied chunk sizes. I don&#39;t know how well this would work though since bytes tend to have different values and patterns may repeat themselves with more complexity than just repeating the same value over and over.

Any pointers to simple compression algorithms would be great! I would also like to discuss ways a new algorithm could be invented (because it&#39;s fun!).</description>
    
    <lastBuildDate>Mon, 14 Nov 2022 16:31:04 +0000</lastBuildDate>
    <category>Algorithms</category>
    <atom:link href="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[system]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed after 121 days. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/11">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/11</link>
        <pubDate>Wed, 15 Mar 2023 14:00:34 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-11</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[prometheus]]></dc:creator>
        <description><![CDATA[
            <p>I found this useful API for compressing and decompressing streams. Turning whatever byte array you have into a ReadableStream will allow you to use this native browser API to compress data! May be helpful to those who read this post: <a href="https://developer.mozilla.org/en-US/docs/Web/API/Compression_Streams_API" class="inline-onebox" rel="noopener nofollow ugc">Compression Streams API - Web APIs | MDN</a></p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/10">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/10</link>
        <pubDate>Mon, 14 Nov 2022 16:31:04 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-10</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[prometheus]]></dc:creator>
        <description><![CDATA[
            <p>dang I’m getting ahead of myself <img src="https://0x00sec.org/images/emoji/twitter/skull.png?v=12" title=":skull:" class="emoji" alt=":skull:" loading="lazy" width="20" height="20"></p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/9">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/9</link>
        <pubDate>Mon, 14 Nov 2022 15:20:09 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-9</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[0x00pf]]></dc:creator>
        <description><![CDATA[
            <aside class="quote no-group" data-username="prometheus" data-post="7" data-topic="32119">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://0x00sec.org/user_avatar/0x00sec.org/prometheus/40/45342_2.png" class="avatar"> prometheus:</div>
<blockquote>
<p>is to look for patterns that are more complex than just a repeated single-byte sequence.</p>
</blockquote>
</aside>
<p>Sure… that is the idea behind LZ <img src="https://0x00sec.org/images/emoji/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> … but follow your path and you will learn way more</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/8">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/8</link>
        <pubDate>Mon, 14 Nov 2022 15:18:53 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-8</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[prometheus]]></dc:creator>
        <description><![CDATA[
            <p>One of the things I am thinking about for the run-length encoding algorithm is to look for patterns that are more complex than just a repeated single-byte sequence. Think of how humans recognize 102030 or 123 or 2468 as patterns. You only need to remember the first number in the sequence and the pattern that follows from that.</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/7">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/7</link>
        <pubDate>Mon, 14 Nov 2022 15:01:27 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-7</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[c0z]]></dc:creator>
        <description><![CDATA[
            <p>Was going to recommend LZMA but you already got it. Nice</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/6">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/6</link>
        <pubDate>Mon, 14 Nov 2022 15:00:14 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-6</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[prometheus]]></dc:creator>
        <description><![CDATA[
            <p>Yay thanks for the advice, it’s reassuring to come up with ideas that already exist because it means I am on the right track/way of thinking. I’ll take a look into LZ77!</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/5">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/5</link>
        <pubDate>Mon, 14 Nov 2022 14:58:40 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-5</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[0x00pf]]></dc:creator>
        <description><![CDATA[
            <p>What you described is known as <a href="https://en.wikipedia.org/wiki/Run-length_encoding" rel="noopener nofollow ugc">RLE</a> (Run-Lenght Encoding). However it is not the most efficient compression algorithm out there. Worked well with simple images when using 16 coulors palettes <img src="https://0x00sec.org/images/emoji/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> … zip and family are based on LZ family of algorithms (the initials of the creators). Take a look to <a href="https://en.wikipedia.org/wiki/LZ77_and_LZ78" rel="noopener nofollow ugc">LZ77</a>.</p>
<p>Then just follow the links <img src="https://0x00sec.org/images/emoji/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"></p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/4">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/4</link>
        <pubDate>Mon, 14 Nov 2022 14:56:26 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-4</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[prometheus]]></dc:creator>
        <description><![CDATA[
            <p>true, compressing text is relatively easy compared to videos, images and the like.</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/3">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/3</link>
        <pubDate>Mon, 14 Nov 2022 13:59:50 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-3</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[messede]]></dc:creator>
        <description><![CDATA[
            <p>You also might want to take into consideration  the computational overhead of compressing and decompressing each chunk instead of the entire file.</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/2</link>
        <pubDate>Mon, 14 Nov 2022 13:07:32 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-2</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
      <item>
        <title>Compression for P2P Networks</title>
        <dc:creator><![CDATA[prometheus]]></dc:creator>
        <description><![CDATA[
            <p>I know there are algorithms like gzip to compress files, but I am wondering if there are alternatives to compressing files for my use case. I have created a P2P browser network that stores data in LocalStorage (which has a limit of 5mb of storage). Here is the process:</p>
<ul>
<li>Turn file into an ArrayBuffer</li>
<li>Split bytes into chunks of 50kb</li>
<li>Upload chunks to the network (peers get the data and store it in their LocalStorage)</li>
</ul>
<p>The problem is that I had to increase the quota/limit for LocalStorage for file uploads to even work.</p>
<p>Currently I am thinking of an algorithm that looks at the byte array and then examines the array by chunks. Assume the array is of length N, the algo would start off by examining chunks with length N, then N-1, then N-2 and so on. If there was a large chunk of 0s of length L, for example, it would store this chunk as 0xL instead of the L number of 0s. If there was a chunk of 1s of length 43, for example, it would do the same, making it 1x43. Putting these compressed chunks in order would allow for lots of compression with varied chunk sizes. I don’t know how well this would work though since bytes tend to have different values and patterns may repeat themselves with more complexity than just repeating the same value over and over.</p>
<p>Any pointers to simple compression algorithms would be great! I would also like to discuss ways a new algorithm could be invented (because it’s fun!).</p>
          <p><a href="https://0x00sec.org/t/compression-for-p2p-networks/32119/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/compression-for-p2p-networks/32119/1</link>
        <pubDate>Sun, 13 Nov 2022 22:00:14 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-32119-1</guid>
        <source url="https://0x00sec.org/t/compression-for-p2p-networks/32119.rss">Compression for P2P Networks</source>
      </item>
  </channel>
</rss>
