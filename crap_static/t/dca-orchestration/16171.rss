<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>[DCA] Orchestration</title>
    <link>https://0x00sec.org/t/dca-orchestration/16171</link>
    <description>**Disclaimer: I&#39;m not the author of the content below. I just aggregated public contents to create a knowledge base.**
____
Hi fellas, 

Here we go, the first article about the prior mentioned DCA series. Today, we will cover the points approached in the orchestration section of the exam.

### Complete the setup of swarm mode cluster with managers and worker nodes

#### Create a Docker Swarm cluster

Make sure the Docker Engine daemon is started on the host machines.

1.  Open a terminal and ssh into the machine where you want to run your manager node. This tutorial uses a machine named `manager1`. If you use Docker Machine, you can connect to it via SSH using the following command:

  ```bash
  $ docker-machine ssh manager1
  ```

2.  Run the following command to create a new swarm:

  ```bash
  $ docker swarm init --advertise-addr &lt;MANAGER-IP&gt;
  ```

&gt;**Note**: If you are using Docker Desktop for Mac or Docker Desktop for Windows to test single-node swarm, simply run `docker swarm init` with no arguments. There is no need to specify **--advertise-addr** in this case. To learn more, see the topic on how to [Use Docker Desktop or Mac or Docker Desktop for Windows](/engine/swarm/swarm-tutorial/index.md#use-docker-for-mac-or-docker-for-windows) with Swarm.

In the tutorial, the following command creates a swarm on the `manager1` machine:

  ```bash
  $ docker swarm init --advertise-addr 192.168.99.100
  Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.

  To add a worker to this swarm, run the following command:

      docker swarm join \
      --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
      192.168.99.100:2377

  To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions.
  ```

The **--advertise-addr** flag configures the manager node to publish its address as **192.168.99.100**. The other nodes in the swarm must be able to access the manager at the IP address.

The output includes the commands to join new nodes to the swarm. Nodes will join as managers or workers depending on the value for the **--token** flag.

2.  Run **docker info** to view the current state of the swarm:

  ```bash
  $ docker info

  Containers: 2
  Running: 0
  Paused: 0
  Stopped: 2
    ...snip...
  Swarm: active
    NodeID: dxn1zf6l61qsb1josjja83ngz
    Is Manager: true
    Managers: 1
    Nodes: 1
    ...snip...
  ```

3.  Run the **docker node ls** command to view information about nodes:

  ```bash
  $ docker node ls

  ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
  dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader

  ```

The * next to the node ID indicates that you&#39;re currently connected on this node.

Docker Engine swarm mode automatically names the node for the machine hostname. The tutorial covers other columns in later steps.

#### Add nodes to the swarm

Once you&#39;ve [created a swarm](#Create-a-Docker-Swarm-cluster) with a manager node, you&#39;re ready to add worker nodes.

1.  Open a terminal and ssh into the machine where you want to run a worker node. This tutorial uses the name **worker1**.

2.  Run the command produced by the **docker swarm init** output from the [Create a swarm](#Create-a-Docker-Swarm-cluster) tutorial step to create a worker node joined to the existing swarm:

  ```bash
  $ docker swarm join \
    --token  SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
    192.168.99.100:2377

  This node joined a swarm as a worker.
  ```

If you don&#39;t have the command available, you can run the following command on a manager node to retrieve the join command for a worker:

  ```bash
  $ docker swarm join-token worker

  To add a worker to this swarm, run the following command:

      docker swarm join \
      --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
      192.168.99.100:2377
  ```

3.  Open a terminal and ssh into the machine where you want to run a second worker node. This tutorial uses the name `worker2`.

4.  Run the command produced by the **docker swarm init** output from the [Create a swarm](#Create-a-Docker-Swarm-cluster) tutorial step to create a second worker node joined to the existing swarm:

  ```bash
  $ docker swarm join \
    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
    192.168.99.100:2377

  This node joined a swarm as a worker.
  ```

5.  Open a terminal and ssh into the machine where the manager node runs and run the **docker node ls** command to see the worker nodes:

  ```bash
  ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
  03g1y59jwfg7cf99w4lt0f662    worker2   Ready   Active
  9j68exjopxe7wfl6yuxml7a7j    worker1   Ready   Active
  dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader
  ```

The **MANAGER** column identifies the manager nodes in the swarm. The empty status in this column for **worker1** and **worker2** identifies them as worker nodes.

Swarm management commands like **docker node ls** only work on manager nodes.

#### Demo

[![asciicast](upload://h4o8hqyaEGzyOFWqe8BZgu93oAH.png)](https://asciinema.org/a/sxZFWFSL2B70Uzb6jyAmy9tKF)

### State the differences between running a container vs running a service

#### Docker run

The **docker run** command first **creates** a writeable container layer over the specified image, and then **starts** it using the specified command. That is, **docker run** is equivalent to the API **/containers/create** then **/containers/(id)/start**. A stopped container can be restarted with all its previous changes intact using **docker start**. See **docker ps -a** to view a list of all containers.

The **docker run** command can be used in combination with **docker commit** to change the command that a container runs. There is additional detailed information about docker run in the [Docker run reference](https://docs.docker.com/engine/reference/run/).

#### Docker services

When you deploy the service to the swarm, the swarm manager accepts your service definition as the desired state for the service. Then it schedules the service on nodes in the swarm as one or more replica tasks. The tasks run independently of each other on nodes in the swarm.

For example, imagine you want to load balance between three instances of an HTTP listener. The diagram below shows an HTTP listener service with three replicas. Each of the three instances of the listener is a task in the swarm.

![Services diagram](upload://9X6NdrdJaAcWBTk8kyScc3Tfnqu.png)

A container is an isolated process. In the swarm mode model, each task invokes exactly one container. A task is analogous to a “slot” where the scheduler places a container. Once the container is live, the scheduler recognizes that the task is in a running state. If the container fails health checks or terminates, the task terminates.

#### Demo

[![asciicast](upload://xbplYloYJi7dgYjujfuYkeItWjE.png)](https://asciinema.org/a/TlMJZDNKZCeUYSfRbmyiKXK46)

### Demonstrate steps to lock a swarm cluster

In Docker 1.13 and higher, the Raft logs used by swarm managers are encrypted on disk by default. This at-rest encryption protects your service&#39;s configuration and data from attackers who gain access to the encrypted Raft logs. One of the reasons this feature was introduced was in support of the new [Docker
secrets](https://docs.docker.com/engine/swarm/secrets/) feature.

When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node&#39;s memory. Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called _autolock_.

When Docker restarts, you must [unlock the swarm](#Unlock-a-swarm) first, using a _key encryption key_ generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.

&gt; **Note**: You don&#39;t need to unlock the swarm when a new node joins the swarm,
&gt; because the key is propagated to it over mutual TLS.

#### Initialize a swarm with autolocking enabled

When you initialize a new swarm, you can use the **--autolock** flag to enable autolocking of swarm manager nodes when Docker restarts.

```bash
$ docker swarm init --autolock

Swarm initialized: current node (k1q27tfyx9rncpixhk69sa61v) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-0j52ln6hxjpxk2wgk917abcnxywj3xed0y8vi1e5m9t3uttrtu-7bnxvvlz2mrcpfonjuztmtts9 \
    172.31.46.109:2377

To add a manager to this swarm, run **docker swarm join-token manager** and follow the instructions.

To unlock a swarm manager after it restarts, run the **docker swarm unlock** command and provide the following key:

    SWMKEY-1-WuYH/IX284+lRcXuoVf38viIDK3HJEKY13MIHX+tTt8
```

Store the key in a safe place, such as in a password manager.

When Docker restarts, you need to [unlock the swarm](#Unlock-a-swarm). A locked swarm causes an error like the following when you try to start or restart a service:

```bash
$ sudo service docker restart

$ docker service ls

Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Use &quot;docker swarm unlock&quot; to unlock it.
```

#### Enable or disable autolock on an existing swarm

To enable autolock on an existing swarm, set the **autolock** flag to **true**.

```bash
$ docker swarm update --autolock=true

Swarm updated.
To unlock a swarm manager after it restarts, run the ***docker swarm unlock*** command and provide the following key:

    SWMKEY-1-+MrE8NgAyKj5r3NcR4FiQMdgu+7W72urH0EZeSmP/0Y

Please remember to store this key in a password manager, since without it you will not be able to restart the manager.
```

To disable autolock, set **--autolock** to **false**. The mutual TLS key and theencryption key used to read and write Raft logs are stored unencrypted on disk. There is a trade-off between the risk of storing the encryption key unencrypted at rest and the convenience of restarting a swarm without needing to unlock each manager.

```bash
$ docker swarm update --autolock=false
```

Keep the unlock key around for a short time after disabling autolocking, in case a manager goes down while it is still configured to lock using the old key.

#### Unlock a swarm

To unlock a locked swarm, use **docker swarm unlock**.

```bash
$ docker swarm unlock

Please enter unlock key:
```

Enter the encryption key that was generated and shown in the command output when you locked the swarm or rotated the key, and the swarm unlocks.

#### View the current unlock key for a running swarm

Consider a situation where your swarm is running as expected, then a manager node becomes unavailable. You troubleshoot the problem and bring the physical node back online, but you need to unlock the manager by providing the unlock key to read the encrypted credentials and Raft logs.

If the key has not been rotated since the node left the swarm, and you have a quorum of functional manager nodes in the swarm, you can view the current unlock key using **docker swarm unlock-key** without any arguments.

```bash
$ docker swarm unlock-key

To unlock a swarm manager after it restarts, run the **docker swarm unlock** command and provide the following key:

    SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA

Please remember to store this key in a password manager, since without it you will not be able to restart the manager.
```

If the key was rotated after the swarm node became unavailable and you do not have a record of the previous key, you may need to force the manager to leave the swarm and join it back to the swarm as a new manager.

#### Rotate the unlock key

You should rotate the locked swarm&#39;s unlock key on a regular schedule.

```bash
$ docker swarm unlock-key --rotate

Successfully rotated manager unlock key.

To unlock a swarm manager after it restarts, run the **docker swarm unlock** command and provide the following key:

    SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA

Please remember to store this key in a password manager, since without it you will not be able to restart the manager.
```

&gt; **Warning**:
&gt; When you rotate the unlock key, keep a record of the old key
&gt; around for a few minutes, so that if a manager goes down before it gets the new
&gt; key, it may still be unlocked with the old one.

#### Demo

[![asciicast](upload://iPUFyRwSyUYtb8GU5PMQIk3PlXn.png)](https://asciinema.org/a/taLgo9ebq1Ut1KOjF1iyvXUEb)

### Extend the instructions to run individual containers into running services under swarm

After you [create a swarm](#Create-a-Docker-Swarm-cluster), you can deploy a service to the swarm. For this tutorial, you also [added worker nodes](#Add-nodes-to-the-swarm), but that is not a requirement to deploy a service.

1.  Open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named **manager1**.

2.  Run the following command:

```bash
$ docker service create --replicas 1 --name helloworld alpine ping docker.com

9uk4639qpg7npwf3fn2aasksr
```

* The **docker service create** command creates the service.
* The **--name** flag names the service **helloworld**.
* The **--replicas** flag specifies the desired state of 1 running instance.
* The arguments **alpine ping docker.com** define the service as an Alpine

Linux container that executes the command **ping docker.com**.

3.  Run **docker service ls** to see the list of running services:

```bash
$ docker service ls

ID            NAME        SCALE  IMAGE   COMMAND
9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com
```

### Interpret the output of docker inspect commands

&gt; Required read: [Docker inspect](https://docs.docker.com/engine/reference/commandline/inspect/)

#### Inspect a service on the swarm

When you have [deployed a service](#Extend-the-instructions-to-run-individual-containers-into-running-services-under-swarm) to your swarm, you can use the Docker CLI to see details about the service running in the swarm.

1.  If you haven&#39;t already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named **manager1**.

2.  Run **docker service inspect --pretty &lt;SERVICE-ID&gt;** to display the details about a service in an easily readable format.

To see the details on the **helloworld** service:

```bash
[manager1]$ docker service inspect --pretty helloworld

ID:		9uk4639qpg7npwf3fn2aasksr
Name:		helloworld
Service Mode:	REPLICATED
    Replicas:		1
Placement:
UpdateConfig:
    Parallelism:	1
ContainerSpec:
    Image:		alpine
    Args:	ping docker.com
Resources:
Endpoint Mode:  vip
```

&gt;**Tip**: To return the service details in json format, run the same command without the **--pretty** flag.

```bash
[manager1]$ docker service inspect helloworld
[
{
    &quot;ID&quot;: &quot;9uk4639qpg7npwf3fn2aasksr&quot;,
    &quot;Version&quot;: {
        &quot;Index&quot;: 418
    },
    &quot;CreatedAt&quot;: &quot;2016-06-16T21:57:11.622222327Z&quot;,
    &quot;UpdatedAt&quot;: &quot;2016-06-16T21:57:11.622222327Z&quot;,
    &quot;Spec&quot;: {
        &quot;Name&quot;: &quot;helloworld&quot;,
        &quot;TaskTemplate&quot;: {
            &quot;ContainerSpec&quot;: {
                &quot;Image&quot;: &quot;alpine&quot;,
                &quot;Args&quot;: [
                    &quot;ping&quot;,
                    &quot;docker.com&quot;
                ]
            },
            &quot;Resources&quot;: {
                &quot;Limits&quot;: {},
                &quot;Reservations&quot;: {}
            },
            &quot;RestartPolicy&quot;: {
                &quot;Condition&quot;: &quot;any&quot;,
                &quot;MaxAttempts&quot;: 0
            },
            &quot;Placement&quot;: {}
        },
        &quot;Mode&quot;: {
            &quot;Replicated&quot;: {
                &quot;Replicas&quot;: 1
            }
        },
        &quot;UpdateConfig&quot;: {
            &quot;Parallelism&quot;: 1
        },
        &quot;EndpointSpec&quot;: {
            &quot;Mode&quot;: &quot;vip&quot;
        }
    },
    &quot;Endpoint&quot;: {
        &quot;Spec&quot;: {}
    }
}
]
```

4.  Run **docker service ps &lt;SERVICE-ID&gt;** to see which nodes are running the service:

```bash
[manager1]$ docker service ps helloworld

NAME                                    IMAGE   NODE     DESIRED STATE  CURRENT STATE           ERROR               PORTS
helloworld.1.8p1vev3fq5zm0mi8g0as41w35  alpine  worker2  Running        Running 3 minutes
```

In this case, the one instance of the **helloworld** service is running on the **worker2** node. You may see the service running on your manager node. By default, manager nodes in a swarm can execute tasks just like worker nodes.

Swarm also shows you the **DESIRED STATE** and **CURRENT STATE** of the service task so you can see if tasks are running according to the service definition.

4.  Run **docker ps** on the node where the task is running to see details about the container for the task.

&gt; **Tip**: If **helloworld** is running on a node other than your manager node, you must ssh to that node.

```bash
[worker2]$docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
e609dde94e47        alpine:latest       &quot;ping docker.com&quot;   3 minutes ago       Up 3 minutes                            helloworld.1.8p1vev3fq5zm0mi8g0as41w35
```

### Convert an application deployment into a stack file using a YAML compose file with docker stack deploy

&gt; Required read [Docker Stack Deploy](https://docs.docker.com/engine/reference/commandline/stack_deploy/)
&gt; required read [Service Configuration Reference](https://docs.docker.com/compose/compose-file/#service-configuration-reference)

#### Demo

[![asciicast](upload://u7RxgYW3sFtvAoifQthQOiictM8.png)](https://asciinema.org/a/vtoxMgJiUMaay9IhNMporfpXV)

### Manipulate a running stack of services

&gt; Required read [Docker Stack Services](https://docs.docker.com/engine/reference/commandline/stack_services/)

#### Demo

[![asciicast](upload://zCkC7xu5ZWoutjvpuQTRIPGtXYx.png)](https://asciinema.org/a/4bqsdgv46EueyOPF48Fzd70dc)

### Increase number of replicas

The scale command enables you to scale one or more replicated services either up or down to the desired number of replicas. This command cannot be applied on services which are global mode. The command will return immediately, but the actual scaling of the service may take some time. To stop all replicas of a service while keeping the service active in the swarm you can set the scale to 0.

The following command scales the “frontend” service to 50 tasks.

```bash
$ docker service scale frontend=50

frontend scaled to 50
```

The following command tries to scale a global service to 10 tasks and returns an error.

```bash
$ docker service create --mode global --name backend backend:latest

b4g08uwuairexjub6ome6usqh

$ docker service scale backend=10

backend: scale can only be used with replicated mode
```

Directly afterwards, run docker service ls, to see the actual number of replicas.

```bash
$ docker service ls --filter name=frontend

ID            NAME      MODE        REPLICAS  IMAGE
3pr5mlvu3fh9  frontend  replicated  15/50     nginx:alpine
```

You can also scale a service using the docker service update command. The following commands are equivalent:

```bash
$ docker service scale frontend=50
$ docker service update --replicas=50 frontend
```

The docker service scale command allows you to set the desired number of tasks for multiple services at once. The following example scales both the backend and frontend services:

```bash
$ docker service scale backend=3 frontend=5

backend scaled to 3
frontend scaled to 5

$ docker service ls

ID            NAME      MODE        REPLICAS  IMAGE
3pr5mlvu3fh9  frontend  replicated  5/5       nginx:alpine
74nzcxxjv6fq  backend   replicated  3/3       redis:3.0.6
```

#### Demo

[![asciicast](upload://jzhVMcb9zCDQradYaJRRPHOvtwl.png)](https://asciinema.org/a/uwKaS8HHk35Aw4H8yCfD8XtG5)

### Add networks, publish ports

When you create a swarm service, you can publish that service&#39;s ports to hosts outside the swarm in two ways:

- [You can rely on the routing mesh](https://docs.docker.com/engine/swarm/services/#publish-a%20services-ports-using-the-routing-mesh). When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.

- [You can publish a service task&#39;s port directly on the swarm node](#Publish-a-service&#39;s-ports-using-the-routing-mesh) where that service is running. This feature is available in Docker 1.13 and higher. This bypasses the routing mesh and provides the maximum flexibility, including the ability for you to develop your own routing framework. However, you are responsible for keeping track of where each task is running and routing requests to the tasks, and load-balancing across the nodes.

Keep reading for more information and use cases for each of these methods.

#### Publish a service&#39;s ports using the routing mesh

To publish a service&#39;s ports externally to the swarm, use the **--publish &lt;PUBLISHED-PORT&gt;:&lt;SERVICE-PORT&gt;** flag. The swarm makes the service accessible at the published port **on every swarm node**. If an external host connects to that port on any swarm node, the routing mesh routes it to a task. The external host does not need to know the IP addresses or internally-used ports of the service tasks to interact with the service. When a user or process connects to a service, any worker node running a service task may respond. For more details about swarm service networking, see [Manage swarm service networks](https://docs.docker.com/network/overlay/).

##### Example: Run a three-task Nginx service on 10-node swarm

Imagine that you have a 10-node swarm, and you deploy an Nginx service running three tasks on a 10-node swarm:

```bash
$ docker service create --name my_web \
                        --replicas 3 \
                        --publish published=8080,target=80 \
                        nginx
```

Three tasks run on up to three nodes. You don&#39;t need to know which nodes are running the tasks; connecting to port 8080 on **any** of the 10 nodes connects you to one of the three **nginx** tasks. You can test this using **curl**. The following example assumes that **localhost** is one of the swarm nodes. If this is not the case, or **localhost** does not resolve to an IP address on your host, substitute the host&#39;s IP address or resolvable host name.

The HTML output is truncated:

```bash
$ curl localhost:8080

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...truncated...
&lt;/html&gt;
```

Subsequent connections may be routed to the same swarm node or a different one.

#### Publish a service&#39;s ports directly on the swarm node

Using the routing mesh may not be the right choice for your application if you need to make routing decisions based on application state or you need total control of the process for routing requests to your service&#39;s tasks. To publish a service&#39;s port directly on the node where it is running, use the **mode=host** option to the **--publish** flag.

&gt; **Note**: If you publish a service&#39;s ports directly on the swarm node using **mode=host** and also set **published=&lt;PORT&gt;** this creates an implicit limitation that you can only run one task for that service on a given swarm node. You can work around this by specifying **published** without a port definition, which causes Docker to assign a random port for each task.
&gt; In addition, if you use **mode=host** and you do not use the **--mode=global** flag on **docker service create**, it is difficult to know which nodes are running the service to route work to them.

##### Example: Run an `nginx` web server service on every swarm node

[nginx](https://hub.docker.com/_/nginx/) is an open source reverse proxy, load balancer, HTTP cache, and a web server. If you run nginx as a service using the routing mesh,connecting to the nginx port on any swarm node shows you the web page for (effectively) **a random swarm node** running the service.

The following example runs nginx as a service on each node in your swarm and exposes nginx port locally on each swarm node.

```bash
$ docker service create \
  --mode global \
  --publish mode=host,target=80,published=8080 \
  --name=nginx \
  nginx:latest
```

You can reach the nginx server on port 8080 of every swarm node. If you add a node to the swarm, a nginx task is started on it. You cannot start another service or container on any swarm node which binds to port 8080.

&gt; **Note**: This is a naive example. Creating an application-layer routing framework for a multi-tiered service is complex and out of scope for this topic.

#### Connect the service to an overlay network

You can use overlay networks to connect one or more services within the swarm.

First, create overlay network on a manager node using the **docker network create** command with the **--driver overlay** flag.

```bash
$ docker network create --driver overlay my-network
```

After you create an overlay network in swarm mode, all manager nodes have access to the network.

You can create a new service and pass the **--network** flag to attach the service to the overlay network:

```bash
$ docker service create \
  --replicas 3 \
  --network my-network \
  --name my-web \
  nginx
```

The swarm extends **my-network** to each node running the service.

You can also connect an existing service to an overlay network using the **--network-add** flag.

```bash
$ docker service update --network-add my-network my-web
```

To disconnect a running service from a network, use the **--network-rm** flag.

```bash
$ docker service update --network-rm my-network my-web
```

For more information on overlay networking and service discovery, refer to [Attach services to an overlay network](https://docs.docker.com/network/overlay/) and
[Docker swarm mode overlay network security model](https://docs.docker.com/network/overlay/).

#### Demo

[![asciicast](upload://3iVFep8UrttrobnssHt0F68PSqz.png)](https://asciinema.org/a/SARGFvegQA7H1B6pNouiKSNPZ)

### Mount volumes

For best performance and portability, you should avoid writing important data directly into a container&#39;s writable layer, instead using data volumes or bind mounts. This principle also applies to services.

You can create two types of mounts for services in a swarm, **volume** mounts or **bind** mounts. Regardless of which type of mount you use, configure it using the **--mount** flag when you create a service, or the **--mount-add** or **--mount-rm** flag when updating an existing service. The default is a data volume if you don&#39;t specify a type.

#### Data volumes

Data volumes are storage that exist independently of a container. The lifecycle of data volumes under swarm services is similar to that under containers. Volumes outlive tasks and services, so their removal must be managed separately. Volumes can be created before deploying a service, or if they don&#39;t exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service.

To use existing data volumes with a service use the **--mount** flag:

```bash
$ docker service create \
  --mount src=&lt;VOLUME-NAME&gt;,dst=&lt;CONTAINER-PATH&gt; \
  --name myservice \
  &lt;IMAGE&gt;
```

If a volume with the same **&lt;VOLUME-NAME&gt;** does not exist when a task is scheduled to a particular host, then one is created. The default volume driver is `local`.  To use a different volume driver with this create-on-demand pattern, specify the driver and its options with the **--mount** flag:

```bash
$ docker service create \
  --mount type=volume,src=&lt;VOLUME-NAME&gt;,dst=&lt;CONTAINER-PATH&gt;,volume-driver=&lt;DRIVER&gt;,volume-opt=&lt;KEY0&gt;=&lt;VALUE0&gt;,volume-opt=&lt;KEY1&gt;=&lt;VALUE1&gt;
  --name myservice \
  &lt;IMAGE&gt;
```

For more information on how to create data volumes and the use of volume drivers, see [Use volumes](https://docs.docker.com/storage/volumes/).


#### Bind mounts

Bind mounts are file system paths from the host where the scheduler deploys the container for the task. Docker mounts the path into the container. The file system path must exist before the swarm initializes the container for the task.

The following examples show bind mount syntax:

- To mount a read-write bind:

  ```bash
  $ docker service create \
    --mount type=bind,src=&lt;HOST-PATH&gt;,dst=&lt;CONTAINER-PATH&gt; \
    --name myservice \
    &lt;IMAGE&gt;
  ```

- To mount a read-only bind:

  ```bash
  $ docker service create \
    --mount type=bind,src=&lt;HOST-PATH&gt;,dst=&lt;CONTAINER-PATH&gt;,readonly \
    --name myservice \
    &lt;IMAGE&gt;
  ```

&gt; **Important**: Bind mounts can be useful but they can also cause problems. In most cases, it is recommended that you architect your application such that mounting paths from the host is unnecessary. The main risks include the following:
&gt;
&gt; - If you bind mount a host path into your service’s containers, the path must exist on every swarm node. The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify.
&gt;
&gt; - The Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.
&gt;
&gt; - Host bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.

#### Demo

[![asciicast](upload://dRmBWhgZdigIkcMAiuD6lTUc8Ff.png)](https://asciinema.org/a/sExTsGhFBPW9LgLwzYN8afuOP)

### Illustrate running a replicated vs global service

There are two types of service deployments, replicated and global.

For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.

A global service is a service that runs one task on every node. There is no pre-specified number of tasks. Each time you add a node to the swarm, the orchestrator creates a task and the scheduler assigns the task to the new node. Good candidates for global services are monitoring agents, an anti-virus scanners or other types of containers that you want to run on every node in the swarm.

The diagram below shows a three-service replica in yellow and a global service in gray.

![Diagram](upload://7H98P6mF3N0yoIfubZCupHGC1vp.png)

#### Demo

[![asciicast](upload://ojmsq4QtPcFwwYXKutl9MIE1lwJ.png)](https://asciinema.org/a/heYzOqTCEpJFNFZkwakkCv97z)

### Identify the steps needed to troubleshoot a service not deploying

A service may be configured in such a way that no node currently in the swarm can run its tasks. In this case, the service remains in state pending. Here are a few examples of when a service might remain in state pending.

&gt;Note: If your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in pending.

* If all nodes are paused or drained, and you create a service, it is pending until a node becomes available. In reality, the first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.
* You can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks. If you specify a very large value, such as 500 GB, the task stays pending forever, unless you really have a node which can satisfy it.
* You can impose placement constraints on the service, and the constraints may not be able to be honored at a given time.

This behavior illustrates that the requirements and configuration of your tasks are not tightly tied to the current state of the swarm. As the administrator of a swarm, you declare the desired state of your swarm, and the manager works with the nodes in the swarm to create that state. You do not need to micro-manage the tasks on the swarm.

#### Demo

[![asciicast](upload://vGjvets3uIUse7YBCsiDymHZZbL.png)](https://asciinema.org/a/jwoYTaAYr4LEqhYBuRkEI9ets)

### Apply node labels to demonstrate placement of tasks

&gt; Required read [Docker node update](https://docs.docker.com/engine/reference/commandline/node_update/)

#### Placement constraints

Use placement constraints to control the nodes a service can be assigned to. In the following example, the service only runs on nodes with the label **region** set to **east**. If no appropriately-labelled nodes are available, tasks will wait in **Pending** until they become available. The **--constraint** flag uses an equality operator **(== or !=)**. For replicated services, it is possible that all services run on the same node, or each node only runs one replica, or that some nodes don’t run any replicas. For global services, the service runs on every node that meets the placement constraint and any [resource requirements](https://docs.docker.com/engine/swarm/services/#reserve-memory-or-cpus-for-a-service).

```bash
$ docker service create \
  --name my-nginx \
  --replicas 5 \
  --constraint node.labels.region==east \
  nginx
```

You can also use the **constraint** service-level key in a **docker-compose.yml** file.

If you specify multiple placement constraints, the service only deploys onto nodes where they are all met. The following example limits the service to run on all nodes where **region** is set to **east** and **type** is not set to **devel**:

```bash
$ docker service create \
  --name my-nginx \
  --mode global \
  --constraint node.labels.region==east \
  --constraint node.labels.type!=devel \
  nginx
```

You can also use placement constraints in conjunction with placement preferences and CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.

For more information on constraints, refer to the [docker service create CLI reference](https://docs.docker.com/engine/reference/commandline/service_create/).

#### Placement preferences

While placement constraints limit the nodes a service can run on, placement preferences try to place tasks on appropriate nodes in an algorithmic way (currently, only spread evenly). For instance, if you assign each node a **rack** label, you can set a placement preference to spread the service evenly across nodes with the rack label, by value. This way, if you lose a **rack**, the service is still running on nodes on other racks.

Placement preferences are not strictly enforced. If no node has the label you specify in your preference, the service is deployed as though the preference were not set.

&gt; Placement preferences are ignored for global services.

The following example sets a preference to spread the deployment across nodes based on the value of the **datacenter** label. If some nodes have **datacenter=us-east** and others have **datacenter=us-west**, the service is deployed as evenly as possible across the two sets of nodes.

```bash
$ docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref &#39;spread=node.labels.datacenter&#39; \
  redis:3.0.6
```

&gt; Missing or null labels

&gt; Nodes which are missing the label used to spread still receive task assignments. As a group, these nodes receive tasks in equal proportion to any of the other groups identified by a specific label value. In a sense, a missing label is the same as having the label with a null value attached to it. If the service should only run on nodes with the label being used for the spread preference, the preference should be combined with a constraint.

You can specify multiple placement preferences, and they are processed in the order they are encountered. The following example sets up a service with multiple placement preferences. Tasks are spread first over the various datacenters, and then over racks (as indicated by the respective labels):

```bash
$ docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref &#39;spread=node.labels.datacenter&#39; \
  --placement-pref &#39;spread=node.labels.rack&#39; \
  redis:3.0.6
```

You can also use placement preferences in conjunction with placement constraints or CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.

This diagram illustrates how placement preferences work:

![Diagram](upload://aIVSZGLm9dGeyuJ91xRhC7t8Cfq.png)

When updating a service with **docker service update**, **--placement-pref-add** appends a new placement preference after all existing placement preferences. **--placement-pref-rm** removes an existing placement preference that matches the argument.

#### Demo

[![asciicast](upload://6VhOKZd2TncayFH5nUadGhnl4bu.png)](https://asciinema.org/a/fweACvdGhMzUFCEnFIXune226)

### Sketch how a Dockerized application communicates with legacy systems

&gt; Required read [Container networking](https://docs.docker.com/config/containers/container-networking/)

### Paraphrase the importance of quorum in a swarm cluster

#### Maintain the quorum of managers

If the swarm loses the quorum of managers, the swarm cannot perform management tasks. If your swarm has multiple managers, always have more than two. To maintain quorum, a majority of managers must be available. An odd number of managers is recommended, because the next even number does not make the quorum easier to keep. For instance, whether you have 3 or 4 managers, you can still only lose 1 manager and maintain the quorum. If you have 5 or 6 managers, you can still only lose two.

Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.

See [Recovering from losing the quorum](https://docs.docker.com/engine/swarm/admin_guide/#recover-from-losing-the-quorum) for troubleshooting steps if you do lose the quorum of managers.

&gt; Required read [Add manager nodes for fault tolerance](https://docs.docker.com/engine/swarm/admin_guide/#add-manager-nodes-for-fault-tolerance)

#### Recover from losing the quorum

Swarm is resilient to failures and the swarm can recover from any number of temporary node failures (machine reboots or crash with restart) or other transient errors. However, a swarm cannot automatically recover if it loses a quorum. Tasks on existing worker nodes continue to run, but administrative tasks are not possible, including scaling or updating services and joining or removing nodes from the swarm. The best way to recover is to bring the missing manager nodes back online. If that is not possible, continue reading for some options for recovering your swarm.

In a swarm of N managers, a quorum (a majority) of manager nodes must always be available. For example, in a swarm with 5 managers, a minimum of 3 must be operational and in communication with each other. In other words, the swarm can tolerate up to (N-1)/2 permanent failures beyond which requests involving swarm management cannot be processed. These types of failures include data corruption or hardware failures.

If you lose the quorum of managers, you cannot administer the swarm. If you have lost the quorum and you attempt to perform any management operation on the swarm, an error occurs:

```bash
Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
```

The best way to recover from losing the quorum is to bring the failed nodes back online. If you can’t do that, the only way to recover from this state is to use the **--force-new-cluster** action from a manager node. This removes all managers except the manager the command was run from. The quorum is achieved because there is now only one manager. Promote nodes to be managers until you have the desired number of managers.

```bash
# From the node to recover
docker swarm init --force-new-cluster --advertise-addr node01:2377
```

When you run the **docker swarm init** command with the **--force-new-cluster** flag, the Docker Engine where you run the command becomes the manager node of a single-node swarm which is capable of managing and running services. The manager has all the previous information about services and tasks, worker nodes are still part of the swarm, and services are still running. You need to add or re-add manager nodes to achieve your previous task distribution and ensure that you have enough managers to maintain high availability and prevent losing the quorum.

### Demonstrate the usage of templates with docker service create

&gt; Required read [Create services using templates](https://docs.docker.com/engine/reference/commandline/service_create/#create-services-using-templates)

#### Demo

[![asciicast](upload://1CF1e1v5l0r6trlMiqAEmktKYhj.png)](https://asciinema.org/a/zZnpejurCS7bE7F5BQUobLHVo)

## Sources

* [Docker Certification Associate preparation guide - a list of resources to help you prepare for a successful certification](https://github.com/DevOps-Academy-Org/dca-prep-guide)
* [Create a Docker Swarm cluster](https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/)
* [Add nodes to the Docker Swarm cluster](https://docs.docker.com/engine/swarm/swarm-tutorial/add-nodes/)
* [Docker run](https://docs.docker.com/engine/reference/commandline/run/#parent-command)
* [Docker services](https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#services-tasks-and-containers)
* [Lock you swarm to protect its encryption key](https://docs.docker.com/engine/swarm/swarm_manager_locking/)
* [Deploy a service to the swarm](https://docs.docker.com/engine/swarm/swarm-tutorial/deploy-service/)
* [Inspect a service on the swarm](https://docs.docker.com/engine/swarm/swarm-tutorial/inspect-service/)
* [Service scale](https://docs.docker.com/engine/reference/commandline/service_scale/)
* [Swarm publish ports](https://docs.docker.com/engine/swarm/services/#publish-ports)
* [Connect the service to an overlay network](https://docs.docker.com/engine/swarm/services/#connect-the-service-to-an-overlay-network)
* [Give a service access to volumes or bind mounts](https://docs.docker.com/engine/swarm/services/#give-a-service-access-to-volumes-or-bind-mounts)
* [Replicated and global services](https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#replicated-and-global-services)
* [Placement constraints](https://docs.docker.com/engine/swarm/services/#placement-constraints)
* [Maintain the quorum of managers](https://docs.docker.com/engine/swarm/admin_guide/#maintain-the-quorum-of-managers)
* [Recover from losing the quorum](https://docs.docker.com/engine/swarm/admin_guide/#recover-from-losing-the-quorum)</description>
    
    <lastBuildDate>Wed, 11 Sep 2019 14:41:40 +0000</lastBuildDate>
    <category>Operations</category>
    <atom:link href="https://0x00sec.org/t/dca-orchestration/16171.rss" rel="self" type="application/rss+xml" />
      <item>
        <title>[DCA] Orchestration</title>
        <dc:creator><![CDATA[system]]></dc:creator>
        <description><![CDATA[
            <p>This topic was automatically closed after 121 days. New replies are no longer allowed.</p>
          <p><a href="https://0x00sec.org/t/dca-orchestration/16171/2">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/dca-orchestration/16171/2</link>
        <pubDate>Sat, 11 Jan 2020 06:34:20 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-16171-2</guid>
        <source url="https://0x00sec.org/t/dca-orchestration/16171.rss">[DCA] Orchestration</source>
      </item>
      <item>
        <title>[DCA] Orchestration</title>
        <dc:creator><![CDATA[Nitrax]]></dc:creator>
        <description><![CDATA[
            <p><strong>Disclaimer: I’m not the author of the content below. I just aggregated public contents to create a knowledge base.</strong></p>
<hr>
<p>Hi fellas,</p>
<p>Here we go, the first article about the prior mentioned DCA series. Today, we will cover the points approached in the orchestration section of the exam.</p>
<h3>Complete the setup of swarm mode cluster with managers and worker nodes</h3>
<h4>Create a Docker Swarm cluster</h4>
<p>Make sure the Docker Engine daemon is started on the host machines.</p>
<ol>
<li>Open a terminal and ssh into the machine where you want to run your manager node. This tutorial uses a machine named <code>manager1</code>. If you use Docker Machine, you can connect to it via SSH using the following command:</li>
</ol>
<pre><code class="lang-bash">$ docker-machine ssh manager1
</code></pre>
<ol start="2">
<li>Run the following command to create a new swarm:</li>
</ol>
<pre><code class="lang-bash">$ docker swarm init --advertise-addr &lt;MANAGER-IP&gt;
</code></pre>
<blockquote>
<p><strong>Note</strong>: If you are using Docker Desktop for Mac or Docker Desktop for Windows to test single-node swarm, simply run <code>docker swarm init</code> with no arguments. There is no need to specify <strong>–advertise-addr</strong> in this case. To learn more, see the topic on how to <a href="https://0x00sec.org/engine/swarm/swarm-tutorial/index.md#use-docker-for-mac-or-docker-for-windows">Use Docker Desktop or Mac or Docker Desktop for Windows</a> with Swarm.</p>
</blockquote>
<p>In the tutorial, the following command creates a swarm on the <code>manager1</code> machine:</p>
<pre><code class="lang-bash">$ docker swarm init --advertise-addr 192.168.99.100
Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
    192.168.99.100:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>
<p>The <strong>–advertise-addr</strong> flag configures the manager node to publish its address as <strong>192.168.99.100</strong>. The other nodes in the swarm must be able to access the manager at the IP address.</p>
<p>The output includes the commands to join new nodes to the swarm. Nodes will join as managers or workers depending on the value for the <strong>–token</strong> flag.</p>
<ol start="2">
<li>Run <strong>docker info</strong> to view the current state of the swarm:</li>
</ol>
<pre><code class="lang-bash">$ docker info

Containers: 2
Running: 0
Paused: 0
Stopped: 2
  ...snip...
Swarm: active
  NodeID: dxn1zf6l61qsb1josjja83ngz
  Is Manager: true
  Managers: 1
  Nodes: 1
  ...snip...
</code></pre>
<ol start="3">
<li>Run the <strong>docker node ls</strong> command to view information about nodes:</li>
</ol>
<pre><code class="lang-bash">$ docker node ls

ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader

</code></pre>
<p>The * next to the node ID indicates that you’re currently connected on this node.</p>
<p>Docker Engine swarm mode automatically names the node for the machine hostname. The tutorial covers other columns in later steps.</p>
<h4>Add nodes to the swarm</h4>
<p>Once you’ve <a href="https://0x00sec.org#Create-a-Docker-Swarm-cluster">created a swarm</a> with a manager node, you’re ready to add worker nodes.</p>
<ol>
<li>
<p>Open a terminal and ssh into the machine where you want to run a worker node. This tutorial uses the name <strong>worker1</strong>.</p>
</li>
<li>
<p>Run the command produced by the <strong>docker swarm init</strong> output from the <a href="https://0x00sec.org#Create-a-Docker-Swarm-cluster">Create a swarm</a> tutorial step to create a worker node joined to the existing swarm:</p>
</li>
</ol>
<pre><code class="lang-bash">$ docker swarm join \
  --token  SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
  192.168.99.100:2377

This node joined a swarm as a worker.
</code></pre>
<p>If you don’t have the command available, you can run the following command on a manager node to retrieve the join command for a worker:</p>
<pre><code class="lang-bash">$ docker swarm join-token worker

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
    192.168.99.100:2377
</code></pre>
<ol start="3">
<li>
<p>Open a terminal and ssh into the machine where you want to run a second worker node. This tutorial uses the name <code>worker2</code>.</p>
</li>
<li>
<p>Run the command produced by the <strong>docker swarm init</strong> output from the <a href="https://0x00sec.org#Create-a-Docker-Swarm-cluster">Create a swarm</a> tutorial step to create a second worker node joined to the existing swarm:</p>
</li>
</ol>
<pre><code class="lang-bash">$ docker swarm join \
  --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
  192.168.99.100:2377

This node joined a swarm as a worker.
</code></pre>
<ol start="5">
<li>Open a terminal and ssh into the machine where the manager node runs and run the <strong>docker node ls</strong> command to see the worker nodes:</li>
</ol>
<pre><code class="lang-bash">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
03g1y59jwfg7cf99w4lt0f662    worker2   Ready   Active
9j68exjopxe7wfl6yuxml7a7j    worker1   Ready   Active
dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader
</code></pre>
<p>The <strong>MANAGER</strong> column identifies the manager nodes in the swarm. The empty status in this column for <strong>worker1</strong> and <strong>worker2</strong> identifies them as worker nodes.</p>
<p>Swarm management commands like <strong>docker node ls</strong> only work on manager nodes.</p>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/sxZFWFSL2B70Uzb6jyAmy9tKF" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/7/77a3f9926ce7ffcb0bbeb9fd3d7f30ec3e1cf97b.png" alt="asciicast" data-base62-sha1="h4o8hqyaEGzyOFWqe8BZgu93oAH" width="644" height="499"></a></p>
<h3>State the differences between running a container vs running a service</h3>
<h4>Docker run</h4>
<p>The <strong>docker run</strong> command first <strong>creates</strong> a writeable container layer over the specified image, and then <strong>starts</strong> it using the specified command. That is, <strong>docker run</strong> is equivalent to the API <strong>/containers/create</strong> then <strong>/containers/(id)/start</strong>. A stopped container can be restarted with all its previous changes intact using <strong>docker start</strong>. See <strong>docker ps -a</strong> to view a list of all containers.</p>
<p>The <strong>docker run</strong> command can be used in combination with <strong>docker commit</strong> to change the command that a container runs. There is additional detailed information about docker run in the <a href="https://docs.docker.com/engine/reference/run/" rel="noopener nofollow ugc">Docker run reference</a>.</p>
<h4>Docker services</h4>
<p>When you deploy the service to the swarm, the swarm manager accepts your service definition as the desired state for the service. Then it schedules the service on nodes in the swarm as one or more replica tasks. The tasks run independently of each other on nodes in the swarm.</p>
<p>For example, imagine you want to load balance between three instances of an HTTP listener. The diagram below shows an HTTP listener service with three replicas. Each of the three instances of the listener is a task in the swarm.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://0x00sec.s3.amazonaws.com/original/2X/4/45c2188715db770ebdbe4d1a19d27a00d8fa125e.png" data-download-href="/uploads/short-url/9X6NdrdJaAcWBTk8kyScc3Tfnqu.png?dl=1" title="Services diagram" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/optimized/2X/4/45c2188715db770ebdbe4d1a19d27a00d8fa125e_2_690x476.png" alt="Services diagram" data-base62-sha1="9X6NdrdJaAcWBTk8kyScc3Tfnqu" width="690" height="476" srcset="https://0x00sec.s3.amazonaws.com/optimized/2X/4/45c2188715db770ebdbe4d1a19d27a00d8fa125e_2_690x476.png, https://0x00sec.s3.amazonaws.com/original/2X/4/45c2188715db770ebdbe4d1a19d27a00d8fa125e.png 1.5x, https://0x00sec.s3.amazonaws.com/original/2X/4/45c2188715db770ebdbe4d1a19d27a00d8fa125e.png 2x" data-small-upload="https://0x00sec.s3.amazonaws.com/optimized/2X/4/45c2188715db770ebdbe4d1a19d27a00d8fa125e_2_10x10.png"></a></div><p></p>
<p>A container is an isolated process. In the swarm mode model, each task invokes exactly one container. A task is analogous to a “slot” where the scheduler places a container. Once the container is live, the scheduler recognizes that the task is in a running state. If the container fails health checks or terminates, the task terminates.</p>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/TlMJZDNKZCeUYSfRbmyiKXK46" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/e/e891e57055be1e617f12b5b4be42c605cc174dc2.png" alt="asciicast" data-base62-sha1="xbplYloYJi7dgYjujfuYkeItWjE" width="690" height="444"></a></p>
<h3>Demonstrate steps to lock a swarm cluster</h3>
<p>In Docker 1.13 and higher, the Raft logs used by swarm managers are encrypted on disk by default. This at-rest encryption protects your service’s configuration and data from attackers who gain access to the encrypted Raft logs. One of the reasons this feature was introduced was in support of the new <a href="https://docs.docker.com/engine/swarm/secrets/" rel="noopener nofollow ugc">Docker<br>
secrets</a> feature.</p>
<p>When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node’s memory. Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called <em>autolock</em>.</p>
<p>When Docker restarts, you must <a href="https://0x00sec.org#Unlock-a-swarm">unlock the swarm</a> first, using a <em>key encryption key</em> generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.</p>
<blockquote>
<p><strong>Note</strong>: You don’t need to unlock the swarm when a new node joins the swarm,<br>
because the key is propagated to it over mutual TLS.</p>
</blockquote>
<h4>Initialize a swarm with autolocking enabled</h4>
<p>When you initialize a new swarm, you can use the <strong>–autolock</strong> flag to enable autolocking of swarm manager nodes when Docker restarts.</p>
<pre><code class="lang-bash">$ docker swarm init --autolock

Swarm initialized: current node (k1q27tfyx9rncpixhk69sa61v) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-0j52ln6hxjpxk2wgk917abcnxywj3xed0y8vi1e5m9t3uttrtu-7bnxvvlz2mrcpfonjuztmtts9 \
    172.31.46.109:2377

To add a manager to this swarm, run **docker swarm join-token manager** and follow the instructions.

To unlock a swarm manager after it restarts, run the **docker swarm unlock** command and provide the following key:

    SWMKEY-1-WuYH/IX284+lRcXuoVf38viIDK3HJEKY13MIHX+tTt8
</code></pre>
<p>Store the key in a safe place, such as in a password manager.</p>
<p>When Docker restarts, you need to <a href="https://0x00sec.org#Unlock-a-swarm">unlock the swarm</a>. A locked swarm causes an error like the following when you try to start or restart a service:</p>
<pre><code class="lang-bash">$ sudo service docker restart

$ docker service ls

Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Use "docker swarm unlock" to unlock it.
</code></pre>
<h4>Enable or disable autolock on an existing swarm</h4>
<p>To enable autolock on an existing swarm, set the <strong>autolock</strong> flag to <strong>true</strong>.</p>
<pre><code class="lang-bash">$ docker swarm update --autolock=true

Swarm updated.
To unlock a swarm manager after it restarts, run the ***docker swarm unlock*** command and provide the following key:

    SWMKEY-1-+MrE8NgAyKj5r3NcR4FiQMdgu+7W72urH0EZeSmP/0Y

Please remember to store this key in a password manager, since without it you will not be able to restart the manager.
</code></pre>
<p>To disable autolock, set <strong>–autolock</strong> to <strong>false</strong>. The mutual TLS key and theencryption key used to read and write Raft logs are stored unencrypted on disk. There is a trade-off between the risk of storing the encryption key unencrypted at rest and the convenience of restarting a swarm without needing to unlock each manager.</p>
<pre><code class="lang-bash">$ docker swarm update --autolock=false
</code></pre>
<p>Keep the unlock key around for a short time after disabling autolocking, in case a manager goes down while it is still configured to lock using the old key.</p>
<h4>Unlock a swarm</h4>
<p>To unlock a locked swarm, use <strong>docker swarm unlock</strong>.</p>
<pre><code class="lang-bash">$ docker swarm unlock

Please enter unlock key:
</code></pre>
<p>Enter the encryption key that was generated and shown in the command output when you locked the swarm or rotated the key, and the swarm unlocks.</p>
<h4>View the current unlock key for a running swarm</h4>
<p>Consider a situation where your swarm is running as expected, then a manager node becomes unavailable. You troubleshoot the problem and bring the physical node back online, but you need to unlock the manager by providing the unlock key to read the encrypted credentials and Raft logs.</p>
<p>If the key has not been rotated since the node left the swarm, and you have a quorum of functional manager nodes in the swarm, you can view the current unlock key using <strong>docker swarm unlock-key</strong> without any arguments.</p>
<pre><code class="lang-bash">$ docker swarm unlock-key

To unlock a swarm manager after it restarts, run the **docker swarm unlock** command and provide the following key:

    SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA

Please remember to store this key in a password manager, since without it you will not be able to restart the manager.
</code></pre>
<p>If the key was rotated after the swarm node became unavailable and you do not have a record of the previous key, you may need to force the manager to leave the swarm and join it back to the swarm as a new manager.</p>
<h4>Rotate the unlock key</h4>
<p>You should rotate the locked swarm’s unlock key on a regular schedule.</p>
<pre><code class="lang-bash">$ docker swarm unlock-key --rotate

Successfully rotated manager unlock key.

To unlock a swarm manager after it restarts, run the **docker swarm unlock** command and provide the following key:

    SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA

Please remember to store this key in a password manager, since without it you will not be able to restart the manager.
</code></pre>
<blockquote>
<p><strong>Warning</strong>:<br>
When you rotate the unlock key, keep a record of the old key<br>
around for a few minutes, so that if a manager goes down before it gets the new<br>
key, it may still be unlocked with the old one.</p>
</blockquote>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/taLgo9ebq1Ut1KOjF1iyvXUEb" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/8/84056f56dcc6adf7f9fed9c477f1a83a73e0e5cd.png" alt="asciicast" data-base62-sha1="iPUFyRwSyUYtb8GU5PMQIk3PlXn" width="690" height="444"></a></p>
<h3>Extend the instructions to run individual containers into running services under swarm</h3>
<p>After you <a href="https://0x00sec.org#Create-a-Docker-Swarm-cluster">create a swarm</a>, you can deploy a service to the swarm. For this tutorial, you also <a href="https://0x00sec.org#Add-nodes-to-the-swarm">added worker nodes</a>, but that is not a requirement to deploy a service.</p>
<ol>
<li>
<p>Open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named <strong>manager1</strong>.</p>
</li>
<li>
<p>Run the following command:</p>
</li>
</ol>
<pre><code class="lang-bash">$ docker service create --replicas 1 --name helloworld alpine ping docker.com

9uk4639qpg7npwf3fn2aasksr
</code></pre>
<ul>
<li>The <strong>docker service create</strong> command creates the service.</li>
<li>The <strong>–name</strong> flag names the service <strong>helloworld</strong>.</li>
<li>The <strong>–replicas</strong> flag specifies the desired state of 1 running instance.</li>
<li>The arguments <strong>alpine ping <a href="http://docker.com" rel="noopener nofollow ugc">docker.com</a></strong> define the service as an Alpine</li>
</ul>
<p>Linux container that executes the command <strong>ping <a href="http://docker.com" rel="noopener nofollow ugc">docker.com</a></strong>.</p>
<ol start="3">
<li>Run <strong>docker service ls</strong> to see the list of running services:</li>
</ol>
<pre><code class="lang-bash">$ docker service ls

ID            NAME        SCALE  IMAGE   COMMAND
9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com
</code></pre>
<h3>Interpret the output of docker inspect commands</h3>
<blockquote>
<p>Required read: <a href="https://docs.docker.com/engine/reference/commandline/inspect/" rel="noopener nofollow ugc">Docker inspect</a></p>
</blockquote>
<h4>Inspect a service on the swarm</h4>
<p>When you have <a href="https://0x00sec.org#Extend-the-instructions-to-run-individual-containers-into-running-services-under-swarm">deployed a service</a> to your swarm, you can use the Docker CLI to see details about the service running in the swarm.</p>
<ol>
<li>
<p>If you haven’t already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named <strong>manager1</strong>.</p>
</li>
<li>
<p>Run <strong>docker service inspect --pretty </strong> to display the details about a service in an easily readable format.</p>
</li>
</ol>
<p>To see the details on the <strong>helloworld</strong> service:</p>
<pre><code class="lang-bash">[manager1]$ docker service inspect --pretty helloworld

ID:		9uk4639qpg7npwf3fn2aasksr
Name:		helloworld
Service Mode:	REPLICATED
    Replicas:		1
Placement:
UpdateConfig:
    Parallelism:	1
ContainerSpec:
    Image:		alpine
    Args:	ping docker.com
Resources:
Endpoint Mode:  vip
</code></pre>
<blockquote>
<p><strong>Tip</strong>: To return the service details in json format, run the same command without the <strong>–pretty</strong> flag.</p>
</blockquote>
<pre><code class="lang-bash">[manager1]$ docker service inspect helloworld
[
{
    "ID": "9uk4639qpg7npwf3fn2aasksr",
    "Version": {
        "Index": 418
    },
    "CreatedAt": "2016-06-16T21:57:11.622222327Z",
    "UpdatedAt": "2016-06-16T21:57:11.622222327Z",
    "Spec": {
        "Name": "helloworld",
        "TaskTemplate": {
            "ContainerSpec": {
                "Image": "alpine",
                "Args": [
                    "ping",
                    "docker.com"
                ]
            },
            "Resources": {
                "Limits": {},
                "Reservations": {}
            },
            "RestartPolicy": {
                "Condition": "any",
                "MaxAttempts": 0
            },
            "Placement": {}
        },
        "Mode": {
            "Replicated": {
                "Replicas": 1
            }
        },
        "UpdateConfig": {
            "Parallelism": 1
        },
        "EndpointSpec": {
            "Mode": "vip"
        }
    },
    "Endpoint": {
        "Spec": {}
    }
}
]
</code></pre>
<ol start="4">
<li>Run <strong>docker service ps </strong> to see which nodes are running the service:</li>
</ol>
<pre><code class="lang-bash">[manager1]$ docker service ps helloworld

NAME                                    IMAGE   NODE     DESIRED STATE  CURRENT STATE           ERROR               PORTS
helloworld.1.8p1vev3fq5zm0mi8g0as41w35  alpine  worker2  Running        Running 3 minutes
</code></pre>
<p>In this case, the one instance of the <strong>helloworld</strong> service is running on the <strong>worker2</strong> node. You may see the service running on your manager node. By default, manager nodes in a swarm can execute tasks just like worker nodes.</p>
<p>Swarm also shows you the <strong>DESIRED STATE</strong> and <strong>CURRENT STATE</strong> of the service task so you can see if tasks are running according to the service definition.</p>
<ol start="4">
<li>Run <strong>docker ps</strong> on the node where the task is running to see details about the container for the task.</li>
</ol>
<blockquote>
<p><strong>Tip</strong>: If <strong>helloworld</strong> is running on a node other than your manager node, you must ssh to that node.</p>
</blockquote>
<pre><code class="lang-bash">[worker2]$docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
e609dde94e47        alpine:latest       "ping docker.com"   3 minutes ago       Up 3 minutes                            helloworld.1.8p1vev3fq5zm0mi8g0as41w35
</code></pre>
<h3>Convert an application deployment into a stack file using a YAML compose file with docker stack deploy</h3>
<blockquote>
<p>Required read <a href="https://docs.docker.com/engine/reference/commandline/stack_deploy/" rel="noopener nofollow ugc">Docker Stack Deploy</a><br>
required read <a href="https://docs.docker.com/compose/compose-file/#service-configuration-reference" rel="noopener nofollow ugc">Service Configuration Reference</a></p>
</blockquote>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/vtoxMgJiUMaay9IhNMporfpXV" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/d/d324c670b6f259f430c755898a79d177889e9b1c.png" alt="asciicast" data-base62-sha1="u7RxgYW3sFtvAoifQthQOiictM8" width="690" height="394"></a></p>
<h3>Manipulate a running stack of services</h3>
<blockquote>
<p>Required read <a href="https://docs.docker.com/engine/reference/commandline/stack_services/" rel="noopener nofollow ugc">Docker Stack Services</a></p>
</blockquote>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/4bqsdgv46EueyOPF48Fzd70dc" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/f/f9a15da7d9681ee4eb66b375fa20caed4416d26d.png" alt="asciicast" data-base62-sha1="zCkC7xu5ZWoutjvpuQTRIPGtXYx" width="690" height="394"></a></p>
<h3>Increase number of replicas</h3>
<p>The scale command enables you to scale one or more replicated services either up or down to the desired number of replicas. This command cannot be applied on services which are global mode. The command will return immediately, but the actual scaling of the service may take some time. To stop all replicas of a service while keeping the service active in the swarm you can set the scale to 0.</p>
<p>The following command scales the “frontend” service to 50 tasks.</p>
<pre><code class="lang-bash">$ docker service scale frontend=50

frontend scaled to 50
</code></pre>
<p>The following command tries to scale a global service to 10 tasks and returns an error.</p>
<pre><code class="lang-bash">$ docker service create --mode global --name backend backend:latest

b4g08uwuairexjub6ome6usqh

$ docker service scale backend=10

backend: scale can only be used with replicated mode
</code></pre>
<p>Directly afterwards, run docker service ls, to see the actual number of replicas.</p>
<pre><code class="lang-bash">$ docker service ls --filter name=frontend

ID            NAME      MODE        REPLICAS  IMAGE
3pr5mlvu3fh9  frontend  replicated  15/50     nginx:alpine
</code></pre>
<p>You can also scale a service using the docker service update command. The following commands are equivalent:</p>
<pre><code class="lang-bash">$ docker service scale frontend=50
$ docker service update --replicas=50 frontend
</code></pre>
<p>The docker service scale command allows you to set the desired number of tasks for multiple services at once. The following example scales both the backend and frontend services:</p>
<pre><code class="lang-bash">$ docker service scale backend=3 frontend=5

backend scaled to 3
frontend scaled to 5

$ docker service ls

ID            NAME      MODE        REPLICAS  IMAGE
3pr5mlvu3fh9  frontend  replicated  5/5       nginx:alpine
74nzcxxjv6fq  backend   replicated  3/3       redis:3.0.6
</code></pre>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/uwKaS8HHk35Aw4H8yCfD8XtG5" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/8/892683e4549261b7f89e12d8741f3e34d65c8651.png" alt="asciicast" data-base62-sha1="jzhVMcb9zCDQradYaJRRPHOvtwl" width="690" height="394"></a></p>
<h3>Add networks, publish ports</h3>
<p>When you create a swarm service, you can publish that service’s ports to hosts outside the swarm in two ways:</p>
<ul>
<li>
<p><a href="https://docs.docker.com/engine/swarm/services/#publish-a%20services-ports-using-the-routing-mesh" rel="noopener nofollow ugc">You can rely on the routing mesh</a>. When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.</p>
</li>
<li>
<p><a href="https://0x00sec.org#Publish-a-service's-ports-using-the-routing-mesh">You can publish a service task’s port directly on the swarm node</a> where that service is running. This feature is available in Docker 1.13 and higher. This bypasses the routing mesh and provides the maximum flexibility, including the ability for you to develop your own routing framework. However, you are responsible for keeping track of where each task is running and routing requests to the tasks, and load-balancing across the nodes.</p>
</li>
</ul>
<p>Keep reading for more information and use cases for each of these methods.</p>
<h4>Publish a service’s ports using the routing mesh</h4>
<p>To publish a service’s ports externally to the swarm, use the <strong>–publish :</strong> flag. The swarm makes the service accessible at the published port <strong>on every swarm node</strong>. If an external host connects to that port on any swarm node, the routing mesh routes it to a task. The external host does not need to know the IP addresses or internally-used ports of the service tasks to interact with the service. When a user or process connects to a service, any worker node running a service task may respond. For more details about swarm service networking, see <a href="https://docs.docker.com/network/overlay/" rel="noopener nofollow ugc">Manage swarm service networks</a>.</p>
<h5>Example: Run a three-task Nginx service on 10-node swarm</h5>
<p>Imagine that you have a 10-node swarm, and you deploy an Nginx service running three tasks on a 10-node swarm:</p>
<pre><code class="lang-bash">$ docker service create --name my_web \
                        --replicas 3 \
                        --publish published=8080,target=80 \
                        nginx
</code></pre>
<p>Three tasks run on up to three nodes. You don’t need to know which nodes are running the tasks; connecting to port 8080 on <strong>any</strong> of the 10 nodes connects you to one of the three <strong>nginx</strong> tasks. You can test this using <strong>curl</strong>. The following example assumes that <strong>localhost</strong> is one of the swarm nodes. If this is not the case, or <strong>localhost</strong> does not resolve to an IP address on your host, substitute the host’s IP address or resolvable host name.</p>
<p>The HTML output is truncated:</p>
<pre><code class="lang-bash">$ curl localhost:8080

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...truncated...
&lt;/html&gt;
</code></pre>
<p>Subsequent connections may be routed to the same swarm node or a different one.</p>
<h4>Publish a service’s ports directly on the swarm node</h4>
<p>Using the routing mesh may not be the right choice for your application if you need to make routing decisions based on application state or you need total control of the process for routing requests to your service’s tasks. To publish a service’s port directly on the node where it is running, use the <strong>mode=host</strong> option to the <strong>–publish</strong> flag.</p>
<blockquote>
<p><strong>Note</strong>: If you publish a service’s ports directly on the swarm node using <strong>mode=host</strong> and also set <strong>published=</strong> this creates an implicit limitation that you can only run one task for that service on a given swarm node. You can work around this by specifying <strong>published</strong> without a port definition, which causes Docker to assign a random port for each task.<br>
In addition, if you use <strong>mode=host</strong> and you do not use the <strong>–mode=global</strong> flag on <strong>docker service create</strong>, it is difficult to know which nodes are running the service to route work to them.</p>
</blockquote>
<h5>Example: Run an <code>nginx</code> web server service on every swarm node</h5>
<p><a href="https://hub.docker.com/_/nginx/" rel="noopener nofollow ugc">nginx</a> is an open source reverse proxy, load balancer, HTTP cache, and a web server. If you run nginx as a service using the routing mesh,connecting to the nginx port on any swarm node shows you the web page for (effectively) <strong>a random swarm node</strong> running the service.</p>
<p>The following example runs nginx as a service on each node in your swarm and exposes nginx port locally on each swarm node.</p>
<pre><code class="lang-bash">$ docker service create \
  --mode global \
  --publish mode=host,target=80,published=8080 \
  --name=nginx \
  nginx:latest
</code></pre>
<p>You can reach the nginx server on port 8080 of every swarm node. If you add a node to the swarm, a nginx task is started on it. You cannot start another service or container on any swarm node which binds to port 8080.</p>
<blockquote>
<p><strong>Note</strong>: This is a naive example. Creating an application-layer routing framework for a multi-tiered service is complex and out of scope for this topic.</p>
</blockquote>
<h4>Connect the service to an overlay network</h4>
<p>You can use overlay networks to connect one or more services within the swarm.</p>
<p>First, create overlay network on a manager node using the <strong>docker network create</strong> command with the <strong>–driver overlay</strong> flag.</p>
<pre><code class="lang-bash">$ docker network create --driver overlay my-network
</code></pre>
<p>After you create an overlay network in swarm mode, all manager nodes have access to the network.</p>
<p>You can create a new service and pass the <strong>–network</strong> flag to attach the service to the overlay network:</p>
<pre><code class="lang-bash">$ docker service create \
  --replicas 3 \
  --network my-network \
  --name my-web \
  nginx
</code></pre>
<p>The swarm extends <strong>my-network</strong> to each node running the service.</p>
<p>You can also connect an existing service to an overlay network using the <strong>–network-add</strong> flag.</p>
<pre><code class="lang-bash">$ docker service update --network-add my-network my-web
</code></pre>
<p>To disconnect a running service from a network, use the <strong>–network-rm</strong> flag.</p>
<pre><code class="lang-bash">$ docker service update --network-rm my-network my-web
</code></pre>
<p>For more information on overlay networking and service discovery, refer to <a href="https://docs.docker.com/network/overlay/" rel="noopener nofollow ugc">Attach services to an overlay network</a> and<br>
<a href="https://docs.docker.com/network/overlay/" rel="noopener nofollow ugc">Docker swarm mode overlay network security model</a>.</p>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/SARGFvegQA7H1B6pNouiKSNPZ" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/1/172a537a43e53395aab2fe2161e99f8c0850e1af.png" alt="asciicast" data-base62-sha1="3iVFep8UrttrobnssHt0F68PSqz" width="690" height="394"></a></p>
<h3>Mount volumes</h3>
<p>For best performance and portability, you should avoid writing important data directly into a container’s writable layer, instead using data volumes or bind mounts. This principle also applies to services.</p>
<p>You can create two types of mounts for services in a swarm, <strong>volume</strong> mounts or <strong>bind</strong> mounts. Regardless of which type of mount you use, configure it using the <strong>–mount</strong> flag when you create a service, or the <strong>–mount-add</strong> or <strong>–mount-rm</strong> flag when updating an existing service. The default is a data volume if you don’t specify a type.</p>
<h4>Data volumes</h4>
<p>Data volumes are storage that exist independently of a container. The lifecycle of data volumes under swarm services is similar to that under containers. Volumes outlive tasks and services, so their removal must be managed separately. Volumes can be created before deploying a service, or if they don’t exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service.</p>
<p>To use existing data volumes with a service use the <strong>–mount</strong> flag:</p>
<pre><code class="lang-bash">$ docker service create \
  --mount src=&lt;VOLUME-NAME&gt;,dst=&lt;CONTAINER-PATH&gt; \
  --name myservice \
  &lt;IMAGE&gt;
</code></pre>
<p>If a volume with the same <strong></strong> does not exist when a task is scheduled to a particular host, then one is created. The default volume driver is <code>local</code>.  To use a different volume driver with this create-on-demand pattern, specify the driver and its options with the <strong>–mount</strong> flag:</p>
<pre><code class="lang-bash">$ docker service create \
  --mount type=volume,src=&lt;VOLUME-NAME&gt;,dst=&lt;CONTAINER-PATH&gt;,volume-driver=&lt;DRIVER&gt;,volume-opt=&lt;KEY0&gt;=&lt;VALUE0&gt;,volume-opt=&lt;KEY1&gt;=&lt;VALUE1&gt;
  --name myservice \
  &lt;IMAGE&gt;
</code></pre>
<p>For more information on how to create data volumes and the use of volume drivers, see <a href="https://docs.docker.com/storage/volumes/" rel="noopener nofollow ugc">Use volumes</a>.</p>
<h4>Bind mounts</h4>
<p>Bind mounts are file system paths from the host where the scheduler deploys the container for the task. Docker mounts the path into the container. The file system path must exist before the swarm initializes the container for the task.</p>
<p>The following examples show bind mount syntax:</p>
<ul>
<li>
<p>To mount a read-write bind:</p>
<pre><code class="lang-bash">$ docker service create \
  --mount type=bind,src=&lt;HOST-PATH&gt;,dst=&lt;CONTAINER-PATH&gt; \
  --name myservice \
  &lt;IMAGE&gt;
</code></pre>
</li>
<li>
<p>To mount a read-only bind:</p>
<pre><code class="lang-bash">$ docker service create \
  --mount type=bind,src=&lt;HOST-PATH&gt;,dst=&lt;CONTAINER-PATH&gt;,readonly \
  --name myservice \
  &lt;IMAGE&gt;
</code></pre>
</li>
</ul>
<blockquote>
<p><strong>Important</strong>: Bind mounts can be useful but they can also cause problems. In most cases, it is recommended that you architect your application such that mounting paths from the host is unnecessary. The main risks include the following:</p>
<ul>
<li>
<p>If you bind mount a host path into your service’s containers, the path must exist on every swarm node. The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify.</p>
</li>
<li>
<p>The Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.</p>
</li>
<li>
<p>Host bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.</p>
</li>
</ul>
</blockquote>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/sExTsGhFBPW9LgLwzYN8afuOP" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/6/61248b8520095cf4b30906fc99cea4b2aac3739d.png" alt="asciicast" data-base62-sha1="dRmBWhgZdigIkcMAiuD6lTUc8Ff" width="690" height="465"></a></p>
<h3>Illustrate running a replicated vs global service</h3>
<p>There are two types of service deployments, replicated and global.</p>
<p>For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.</p>
<p>A global service is a service that runs one task on every node. There is no pre-specified number of tasks. Each time you add a node to the swarm, the orchestrator creates a task and the scheduler assigns the task to the new node. Good candidates for global services are monitoring agents, an anti-virus scanners or other types of containers that you want to run on every node in the swarm.</p>
<p>The diagram below shows a three-service replica in yellow and a global service in gray.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://0x00sec.s3.amazonaws.com/original/2X/3/35efd4d64283c0e84edc613755d2893967a74fef.png" data-download-href="/uploads/short-url/7H98P6mF3N0yoIfubZCupHGC1vp.png?dl=1" title="Diagram" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/optimized/2X/3/35efd4d64283c0e84edc613755d2893967a74fef_2_690x492.png" alt="Diagram" data-base62-sha1="7H98P6mF3N0yoIfubZCupHGC1vp" width="690" height="492" srcset="https://0x00sec.s3.amazonaws.com/optimized/2X/3/35efd4d64283c0e84edc613755d2893967a74fef_2_690x492.png, https://0x00sec.s3.amazonaws.com/original/2X/3/35efd4d64283c0e84edc613755d2893967a74fef.png 1.5x, https://0x00sec.s3.amazonaws.com/original/2X/3/35efd4d64283c0e84edc613755d2893967a74fef.png 2x" data-small-upload="https://0x00sec.s3.amazonaws.com/optimized/2X/3/35efd4d64283c0e84edc613755d2893967a74fef_2_10x10.png"></a></div><p></p>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/heYzOqTCEpJFNFZkwakkCv97z" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/a/aa647c322b78d9c400788d1dd52a57eee0885db9.png" alt="asciicast" data-base62-sha1="ojmsq4QtPcFwwYXKutl9MIE1lwJ" width="664" height="500"></a></p>
<h3>Identify the steps needed to troubleshoot a service not deploying</h3>
<p>A service may be configured in such a way that no node currently in the swarm can run its tasks. In this case, the service remains in state pending. Here are a few examples of when a service might remain in state pending.</p>
<blockquote>
<p>Note: If your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in pending.</p>
</blockquote>
<ul>
<li>If all nodes are paused or drained, and you create a service, it is pending until a node becomes available. In reality, the first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.</li>
<li>You can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks. If you specify a very large value, such as 500 GB, the task stays pending forever, unless you really have a node which can satisfy it.</li>
<li>You can impose placement constraints on the service, and the constraints may not be able to be honored at a given time.</li>
</ul>
<p>This behavior illustrates that the requirements and configuration of your tasks are not tightly tied to the current state of the swarm. As the administrator of a swarm, you declare the desired state of your swarm, and the manager works with the nodes in the swarm to create that state. You do not need to micro-manage the tasks on the swarm.</p>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/jwoYTaAYr4LEqhYBuRkEI9ets" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/d/de0be78ee97c5cb8b61114c4bbb1ee05ac209ad5.png" alt="asciicast" data-base62-sha1="vGjvets3uIUse7YBCsiDymHZZbL" width="664" height="500"></a></p>
<h3>Apply node labels to demonstrate placement of tasks</h3>
<blockquote>
<p>Required read <a href="https://docs.docker.com/engine/reference/commandline/node_update/" rel="noopener nofollow ugc">Docker node update</a></p>
</blockquote>
<h4>Placement constraints</h4>
<p>Use placement constraints to control the nodes a service can be assigned to. In the following example, the service only runs on nodes with the label <strong>region</strong> set to <strong>east</strong>. If no appropriately-labelled nodes are available, tasks will wait in <strong>Pending</strong> until they become available. The <strong>–constraint</strong> flag uses an equality operator <strong>(== or !=)</strong>. For replicated services, it is possible that all services run on the same node, or each node only runs one replica, or that some nodes don’t run any replicas. For global services, the service runs on every node that meets the placement constraint and any <a href="https://docs.docker.com/engine/swarm/services/#reserve-memory-or-cpus-for-a-service" rel="noopener nofollow ugc">resource requirements</a>.</p>
<pre><code class="lang-bash">$ docker service create \
  --name my-nginx \
  --replicas 5 \
  --constraint node.labels.region==east \
  nginx
</code></pre>
<p>You can also use the <strong>constraint</strong> service-level key in a <strong>docker-compose.yml</strong> file.</p>
<p>If you specify multiple placement constraints, the service only deploys onto nodes where they are all met. The following example limits the service to run on all nodes where <strong>region</strong> is set to <strong>east</strong> and <strong>type</strong> is not set to <strong>devel</strong>:</p>
<pre><code class="lang-bash">$ docker service create \
  --name my-nginx \
  --mode global \
  --constraint node.labels.region==east \
  --constraint node.labels.type!=devel \
  nginx
</code></pre>
<p>You can also use placement constraints in conjunction with placement preferences and CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.</p>
<p>For more information on constraints, refer to the <a href="https://docs.docker.com/engine/reference/commandline/service_create/" rel="noopener nofollow ugc">docker service create CLI reference</a>.</p>
<h4>Placement preferences</h4>
<p>While placement constraints limit the nodes a service can run on, placement preferences try to place tasks on appropriate nodes in an algorithmic way (currently, only spread evenly). For instance, if you assign each node a <strong>rack</strong> label, you can set a placement preference to spread the service evenly across nodes with the rack label, by value. This way, if you lose a <strong>rack</strong>, the service is still running on nodes on other racks.</p>
<p>Placement preferences are not strictly enforced. If no node has the label you specify in your preference, the service is deployed as though the preference were not set.</p>
<blockquote>
<p>Placement preferences are ignored for global services.</p>
</blockquote>
<p>The following example sets a preference to spread the deployment across nodes based on the value of the <strong>datacenter</strong> label. If some nodes have <strong>datacenter=us-east</strong> and others have <strong>datacenter=us-west</strong>, the service is deployed as evenly as possible across the two sets of nodes.</p>
<pre><code class="lang-bash">$ docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  redis:3.0.6
</code></pre>
<blockquote>
<p>Missing or null labels</p>
</blockquote>
<blockquote>
<p>Nodes which are missing the label used to spread still receive task assignments. As a group, these nodes receive tasks in equal proportion to any of the other groups identified by a specific label value. In a sense, a missing label is the same as having the label with a null value attached to it. If the service should only run on nodes with the label being used for the spread preference, the preference should be combined with a constraint.</p>
</blockquote>
<p>You can specify multiple placement preferences, and they are processed in the order they are encountered. The following example sets up a service with multiple placement preferences. Tasks are spread first over the various datacenters, and then over racks (as indicated by the respective labels):</p>
<pre><code class="lang-bash">$ docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  --placement-pref 'spread=node.labels.rack' \
  redis:3.0.6
</code></pre>
<p>You can also use placement preferences in conjunction with placement constraints or CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.</p>
<p>This diagram illustrates how placement preferences work:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://0x00sec.s3.amazonaws.com/original/2X/4/4b2a0b02ab577e8c79aa783cbb2c27422ddf4b04.png" data-download-href="/uploads/short-url/aIVSZGLm9dGeyuJ91xRhC7t8Cfq.png?dl=1" title="Diagram" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/optimized/2X/4/4b2a0b02ab577e8c79aa783cbb2c27422ddf4b04_2_690x336.png" alt="Diagram" data-base62-sha1="aIVSZGLm9dGeyuJ91xRhC7t8Cfq" width="690" height="336" srcset="https://0x00sec.s3.amazonaws.com/optimized/2X/4/4b2a0b02ab577e8c79aa783cbb2c27422ddf4b04_2_690x336.png, https://0x00sec.s3.amazonaws.com/optimized/2X/4/4b2a0b02ab577e8c79aa783cbb2c27422ddf4b04_2_1035x504.png 1.5x, https://0x00sec.s3.amazonaws.com/optimized/2X/4/4b2a0b02ab577e8c79aa783cbb2c27422ddf4b04_2_1380x672.png 2x" data-small-upload="https://0x00sec.s3.amazonaws.com/optimized/2X/4/4b2a0b02ab577e8c79aa783cbb2c27422ddf4b04_2_10x10.png"></a></div><p></p>
<p>When updating a service with <strong>docker service update</strong>, <strong>–placement-pref-add</strong> appends a new placement preference after all existing placement preferences. <strong>–placement-pref-rm</strong> removes an existing placement preference that matches the argument.</p>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/fweACvdGhMzUFCEnFIXune226" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/3/3086d7d7fa6de44562e72de4b5b7031c553b6a80.png" alt="asciicast" data-base62-sha1="6VhOKZd2TncayFH5nUadGhnl4bu" width="664" height="500"></a></p>
<h3>Sketch how a Dockerized application communicates with legacy systems</h3>
<blockquote>
<p>Required read <a href="https://docs.docker.com/config/containers/container-networking/" rel="noopener nofollow ugc">Container networking</a></p>
</blockquote>
<h3>Paraphrase the importance of quorum in a swarm cluster</h3>
<h4>Maintain the quorum of managers</h4>
<p>If the swarm loses the quorum of managers, the swarm cannot perform management tasks. If your swarm has multiple managers, always have more than two. To maintain quorum, a majority of managers must be available. An odd number of managers is recommended, because the next even number does not make the quorum easier to keep. For instance, whether you have 3 or 4 managers, you can still only lose 1 manager and maintain the quorum. If you have 5 or 6 managers, you can still only lose two.</p>
<p>Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.</p>
<p>See <a href="https://docs.docker.com/engine/swarm/admin_guide/#recover-from-losing-the-quorum" rel="noopener nofollow ugc">Recovering from losing the quorum</a> for troubleshooting steps if you do lose the quorum of managers.</p>
<blockquote>
<p>Required read <a href="https://docs.docker.com/engine/swarm/admin_guide/#add-manager-nodes-for-fault-tolerance" rel="noopener nofollow ugc">Add manager nodes for fault tolerance</a></p>
</blockquote>
<h4>Recover from losing the quorum</h4>
<p>Swarm is resilient to failures and the swarm can recover from any number of temporary node failures (machine reboots or crash with restart) or other transient errors. However, a swarm cannot automatically recover if it loses a quorum. Tasks on existing worker nodes continue to run, but administrative tasks are not possible, including scaling or updating services and joining or removing nodes from the swarm. The best way to recover is to bring the missing manager nodes back online. If that is not possible, continue reading for some options for recovering your swarm.</p>
<p>In a swarm of N managers, a quorum (a majority) of manager nodes must always be available. For example, in a swarm with 5 managers, a minimum of 3 must be operational and in communication with each other. In other words, the swarm can tolerate up to (N-1)/2 permanent failures beyond which requests involving swarm management cannot be processed. These types of failures include data corruption or hardware failures.</p>
<p>If you lose the quorum of managers, you cannot administer the swarm. If you have lost the quorum and you attempt to perform any management operation on the swarm, an error occurs:</p>
<pre><code class="lang-bash">Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
</code></pre>
<p>The best way to recover from losing the quorum is to bring the failed nodes back online. If you can’t do that, the only way to recover from this state is to use the <strong>–force-new-cluster</strong> action from a manager node. This removes all managers except the manager the command was run from. The quorum is achieved because there is now only one manager. Promote nodes to be managers until you have the desired number of managers.</p>
<pre><code class="lang-bash"># From the node to recover
docker swarm init --force-new-cluster --advertise-addr node01:2377
</code></pre>
<p>When you run the <strong>docker swarm init</strong> command with the <strong>–force-new-cluster</strong> flag, the Docker Engine where you run the command becomes the manager node of a single-node swarm which is capable of managing and running services. The manager has all the previous information about services and tasks, worker nodes are still part of the swarm, and services are still running. You need to add or re-add manager nodes to achieve your previous task distribution and ensure that you have enough managers to maintain high availability and prevent losing the quorum.</p>
<h3>Demonstrate the usage of templates with docker service create</h3>
<blockquote>
<p>Required read <a href="https://docs.docker.com/engine/reference/commandline/service_create/#create-services-using-templates" rel="noopener nofollow ugc">Create services using templates</a></p>
</blockquote>
<h4>Demo</h4>
<p><a href="https://asciinema.org/a/zZnpejurCS7bE7F5BQUobLHVo" rel="noopener nofollow ugc"><img src="https://0x00sec.s3.amazonaws.com/original/2X/0/0b60f98e1293fdcb796a793ae27f60814812b281.png" alt="asciicast" data-base62-sha1="1CF1e1v5l0r6trlMiqAEmktKYhj" width="664" height="500"></a></p>
<h2>Sources</h2>
<ul>
<li><a href="https://github.com/DevOps-Academy-Org/dca-prep-guide" rel="noopener nofollow ugc">Docker Certification Associate preparation guide - a list of resources to help you prepare for a successful certification</a></li>
<li><a href="https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/" rel="noopener nofollow ugc">Create a Docker Swarm cluster</a></li>
<li><a href="https://docs.docker.com/engine/swarm/swarm-tutorial/add-nodes/" rel="noopener nofollow ugc">Add nodes to the Docker Swarm cluster</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/run/#parent-command" rel="noopener nofollow ugc">Docker run</a></li>
<li><a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#services-tasks-and-containers" rel="noopener nofollow ugc">Docker services</a></li>
<li><a href="https://docs.docker.com/engine/swarm/swarm_manager_locking/" rel="noopener nofollow ugc">Lock you swarm to protect its encryption key</a></li>
<li><a href="https://docs.docker.com/engine/swarm/swarm-tutorial/deploy-service/" rel="noopener nofollow ugc">Deploy a service to the swarm</a></li>
<li><a href="https://docs.docker.com/engine/swarm/swarm-tutorial/inspect-service/" rel="noopener nofollow ugc">Inspect a service on the swarm</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/service_scale/" rel="noopener nofollow ugc">Service scale</a></li>
<li><a href="https://docs.docker.com/engine/swarm/services/#publish-ports" rel="noopener nofollow ugc">Swarm publish ports</a></li>
<li><a href="https://docs.docker.com/engine/swarm/services/#connect-the-service-to-an-overlay-network" rel="noopener nofollow ugc">Connect the service to an overlay network</a></li>
<li><a href="https://docs.docker.com/engine/swarm/services/#give-a-service-access-to-volumes-or-bind-mounts" rel="noopener nofollow ugc">Give a service access to volumes or bind mounts</a></li>
<li><a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#replicated-and-global-services" rel="noopener nofollow ugc">Replicated and global services</a></li>
<li><a href="https://docs.docker.com/engine/swarm/services/#placement-constraints" rel="noopener nofollow ugc">Placement constraints</a></li>
<li><a href="https://docs.docker.com/engine/swarm/admin_guide/#maintain-the-quorum-of-managers" rel="noopener nofollow ugc">Maintain the quorum of managers</a></li>
<li><a href="https://docs.docker.com/engine/swarm/admin_guide/#recover-from-losing-the-quorum" rel="noopener nofollow ugc">Recover from losing the quorum</a></li>
</ul>
          <p><a href="https://0x00sec.org/t/dca-orchestration/16171/1">Read full topic</a></p>
        ]]></description>
        <link>https://0x00sec.org/t/dca-orchestration/16171/1</link>
        <pubDate>Wed, 11 Sep 2019 14:34:18 +0000</pubDate>
        <guid isPermaLink="false">0x00sec.org-post-16171-1</guid>
        <source url="https://0x00sec.org/t/dca-orchestration/16171.rss">[DCA] Orchestration</source>
      </item>
  </channel>
</rss>
